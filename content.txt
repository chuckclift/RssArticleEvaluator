<item>
<title>Australian agencies accessing metadata more than ever before</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
A total of 77 Australian state and federal agencies accessed the telecommunications data of citizens 334,658 times in the 2013-14 financial year, the government has revealed.
The details came in the annual report of the Telecommunications (Interception and Access) Act 1979 (PDF) that lists the interceptions and access to stored telecommunications data made by government agencies each financial year. 
Of the 334,658 authorisations, the government stated that 324,260 of these were made to enforce criminal law. It has painted this figure as an improvement on the total 330,798 authorisations made in the previous financial year, as it was only a growth of 1.2 percent, compared to the 9.8 percent growth between 2011-12 and 2012-13.NSW Police made the most requests, with 111,889 authorisations in 2013-14, followed by Victoria Police, at 63,325, and Queensland Police, at 35,663. The Australian Federal Police (AFP) had 21,358 authorisations, while the Australian Security Intelligence Organisation (ASIO) does not have to disclose its metadata access.For access requests made by Commonwealth enforcement agencies, Customs had the most, at 6,196 for the year -- a significant rise from 3,902 from the previous financial year.For state and territory organisations, the most requests came from Corrections Victoria, at 389. RSPCA Victoria, The Hills Shire Council, Racing NSW, and the WA Department of Commerce also accessed metadata in the last financial year.The Australian Federal Police also revealed in the report that it had handed over data 17 times to foreign law-enforcement agencies in Russia, France, Germany, Greece, Hong Kong, Hungary, India, Italy, Japan, Lithuania, Norway, Poland, Sri Lanka, and Singapore.Notable exclusions from this list are Australia's fellow members of the Five Eyes alliance: The United States, Canada, New Zealand, and the United Kingdom.
There was a total of 4,007 warrants issued for the interception of telecommunications, down from 4,232 in the previous financial year. The most were authorised to NSW Police, at 1,514 warrants issued. There were a total of 2,938 arrests, 4,008 prosecutions, and 2,210 convictions in the financial year based on intercepted telecommunications, the report stated.Interception for the 2013-14 financial year cost the AFP AU$10.5 million, while it cost NSW Police AU$6.8 million, and Victoria Police AU$6.6 million.In the 2013-14 financial year, there were 1,511 requests for the preservation of stored communications for the companies to retain this data while a warrant is obtained to access the data.The report is typically tabled by the attorney-general in November or early December; however, the government waited over seven months longer before tabling the report, coinciding it with the introduction and passage of new legislation that forces telecommunications companies to keep so-called metadata for two years for access by agencies without a warrant.Organisations like RSPCA Victoria cannot access data stored under the new scheme, but can apply to be added, with parliamentary approval.The Attorney-General's Department has previously denied that the report was late, stating that its tabling varies from year to year. The department had not responded to a request for comment on Thursday.
Dallas Buyers Club wants to know torrenters' wage and download history
Optus' three-part strategy to becoming a multimedia provider
TPG commits to preserving iiNet's customer focus
Cosmic communications: African telcos turn to satellites to bridge the connectivity gap
</article>
</item>
<item>
<title>Google, Microsoft, Mozilla team up for WebAssembly binary format for the web</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
Before much of the web could experience the benefits of asm.js, a group of engineers from Microsoft, Google, and Mozilla are already planning its successor, named WebAssembly.The reason for creating a binary format was to tackle the bottleneck in JavaScript execution, the parser.
"asm.js is great, but once engines optimize for it, the parser becomes the hot spot -- very hot on mobile devices. Transport compression is required and saves bandwidth, but decompression before parsing hurts," JavaScript creator and former Mozilla CEO Brendan Eich said in a blog post announcing WebAssembly.
"No, JS isn't going away in any foreseeable future. Yes, wasm [WebAssembly] should relieve JS from having to serve two masters. This is a win-win plan."WebAssembly claims in its FAQ that the binary format under consideration can be natively decoded more than 20 times faster than JavaScript can be parsed.Initially, WebAssembly will be converted to JavaScript via a polyfill in order to run, and be targeted only at C and C++. Once a minimal viable product is created, it is expected that support for other languages and compilers will be added, and features beyond those in JavaScript or asm.js can be added.WebAssembly has provided a list of use cases for the format, with its use beyond web browsers targeted in the future.Although it is a binary format, WebAssembly will have a text format to allow for wasm code to be readable when a user views a web page's source in their browser.
App Engine for PHP moves out of beta phase
​Databricks' Apache Spark cloud platform goes public
Apache Spark 1.4 adds R language and hardened machine-learning
Oculus teaming with Microsoft as Rift becomes a reality
</article>
</item>
<item>
<title>Dallas Buyers Club wants to know torrenters' wage and download history</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
The law firm chasing 4,726 Australians alleged to have illicitly downloaded Dallas Buyers Club has indicated that it will ask for the annual income and what other films the person has downloaded in letters to be sent out in the next few months.
In April, iiNet, Dodo, and four other internet service providers (ISPs) were ordered to hand over the details of account holders associated with 4,726 IP addresses alleged to have downloaded the film, but with a catch: Voltage will need to pay the costs for the ISPs, and the court must see a draft of the letters to be sent out to customers before any details will be handed over.
This was designed to avoid so-called "speculative invoicing", which Voltage has used in the US to demand from downloaders up to $9,000 in compensation or the threat of having to pay potentially hundreds of thousands of dollars in damages under court order.Drafts of the letter and a telephone script have been submitted to the court, but have not yet been made public.In an interlocutory hearing on Thursday, Counsel representing the ISPs, Richard Lancaster, said that the telephone script used to call alleged infringers and the letters to be sent are still very strongly worded, framing the allegations as proof that a household and the person sent the letter or contacted on the phone had personally downloaded the film."There's a problem with that, because among other things, there is the possibility there isn't an infringement," he said.He said that Voltage also comes on too strong in terms of the questions it asks customers in determining the damages to be paid by each individual infringer.The questions include what a person's annual income is, and how many titles the person has illicitly downloaded over peer-to-peer file-sharing services "now and in the past".
"There is no respondent out there to copyright infringement who is required before the identification for a potential course of action ... to be required to state or provide information about their other activity," he said.Counsel for Voltage, Ian Pike, said that the questions would be used to determine the financial hardship of particular alleged infringers, and whether it was just a one-off."We are entitled in the letter to assert in reasonably firm terms why we contend there has been copyright infringement. Nothing in the letter oversteps the mark. They are perfectly proper questions. We would be criticised if we didn't ask them," he said.Voltage is also seeking to exclude a dollar figure in the letter and telephone script for how much it will seek in damages for sharing the film online. Pike said there is "no one-size-fits-all" number, because it would depend on the financial hardship of the alleged infringer, the number of times the film was shared online, and whether it was a one-off or the infringer had a history of illicitly downloading films.It is not clear how Voltage intends to determine what other films an alleged infringer has downloaded, outside of asking them in the letter.Justice Nye Perram said he needs to be convinced that he isn't just  giving Voltage "a blank cheque" to then ask the alleged infringers for  much more compensation, and has asked the firm to provide a confidential  methodology on how it will be determining the compensation sought from  each of the 4,726 users.The methodology includes a licence fee, a  contribution to the court costs, and damages based on how many times  the film was shared online by a particular user.Pike also sought to make the letters and telephone script confidential, to "avoid confusion" should media publish the letters that the public should expect to receive. ZDNet has sought access to the letter and script in question.Perram on Thursday ordered the original securities paid by Voltage in November 2014 be released back to Voltage, and has not yet made a decision on costs to be paid to Voltage, or securities for ISPs to hand over customer details.The ISPs have argued that the bond paid should be high to ensure that Voltage, which resides outside of the Australian jurisdiction, does not just take customer details and ignore the court order around the form of the letter and script, and begin speculative invoicing.Voltage has argued that the bond should be in the range of AU$20,000.Perram indicated that he would make a final decision on the case by July 15, 2015.It comes as this week, the House of Representatives passed legislation -- supported by the Coalition and Labor -- that would force ISPs to block sites under court order that infringe on copyright. The site-blocking legislation is expected to be debated in the Senate next week. Greens Communications spokesperson Scott Ludlam said the Greens would seek to challenge the Bill's passage in the Senate."The government has proceeded with a punitive site-blocking regime and  completely  ignored more practical options for copyright reform that have been on  the table for years. At a bare minimum, we need to see the government's response to the  Australian Law Reform Commission's review into copyright law before  legislating for an internet filter directed by foreign rights holders," he said in a statement on Thursday. "This is a dangerous and unnecessary piece of legislation that potentially criminalises legitimate use of VPNs or other tools."
Optus' three-part strategy to becoming a multimedia provider
TPG commits to preserving iiNet's customer focus
Cosmic communications: African telcos turn to satellites to bridge the connectivity gap
Singapore to cease 2G services by April 2017
</article>
</item>
<item>
<title>Encrypting data at rest is vital, but it's just not happening</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
The Office of the Australian Information Commissioner (OAIC) has been clear about encrypting personal data, both in its guidelines and in recent data breach investigations. But according to Chris Gatford, director of penetration testing firm Hacklabs, very few organisations are living up to expectations."Encrypted file systems, especially encrypting data at rest, it just doesn't occur," Gatford told ZDNet. "Ninety nine percent of organisations do not encrypt anything other than the occasional laptop."The most common scenario Gatford encounters during pentests is where none of the target organisation's desktop workstations run any kind of encryption for end users whatsoever. That seems a long way from what the OAIC expects.The OAIC doesn't demand encryption outright. But its Guide to securing personal information reminds organisations that they need to take "reasonable steps" to secure that information. Encryption is "important in many circumstances", and organisations need to protect data, whether it's on servers, in databases, in backups, in third-party cloud services, on end-user devices including smartphones and tablets as well as laptops, or in portable storage devices."Encryption methods should be reviewed regularly to ensure they continue to be relevant and effective, and are used where necessary. This includes ensuring that the scope of encryption is wide enough so that attackers cannot access another unencrypted copy of your encrypted information," the guide says.What that can mean in practice is illustrated by the OAIC's recent report on its investigation into Adobe's massive 2013 data breach.        
Adobe had apparently encrypted all user passwords with the same key, rather than each being individually salted then hashed. Password hints weren't encrypted at all.
"Hashing and salting is a basic security step that Adobe could reasonably have implemented to better protect the passwords," the OAIC wrote.
While different laws applied at that time of Adobe's data breach -- Australia's privacy laws were updated on March 12, 2014 -- the "reasonable steps" test applied both then and now. The key difference is that now, the Privacy Commissioner can issue fines of up to AU$1.7 million to organisations that fail to take those reasonable steps.Businesses also need to protect their trade secrets, of course, and Gatford said that more mature enterprises have become used to encrypting laptops because of the obvious risk of theft.A username and password offer zero protection when a thief can simply remove the hard drive, install it in another computer, and copy the data. Encrypting laptops is essential, and the same goes for tablets and smartphones.The theft of a mobile device can often be part of an organised operation, according to Sven Radavics, Imation Mobile Security's general manager for the APAC region -- and it's not just about national security and defence information. Any organisation's intellectual property can be a target, from the design for a new car engine to a movie or video game."Particularly in China -- but not always in China, it's happened in other countries -- [there have been cases] where it's been clear my hotel safe has been opened, and my laptop has been moved," Radavics told ZDNet last week."It's fairly common that if some entity wants access to your data, that hotel safe provides no protection," he said.Radavics' own travel kit consists of his personal MacBook Air, hardened with a variety of security software, and one of Imation's own IronKey encrypted USB sticks.Many other companies have a similar process for travellers to higher-risk destinations, he said. Employees are supplied with a laptop with a freshly installed, limited operating system image, with all of the company's data kept on an encrypted device, or running everything off something like IronKey's Windows to Go USB stick-based secure mobile workspace. Upon return, the laptop is completely wiped. Radavics was keen to boast about IronKey's security features, of course, such as the layers of epoxy that make it difficult to get at the crypto chip without destroying it, or the self-destruct mechanism that trashes the keys if the chip is exposed to air. But he did make some valid points about evaluating the cost of defence against the risk of attack.  "You could theoretically put the chip under the [electron] microscope and extract the keys, and we consider that a $50,000 hack. But we actually shield the chip, so an electron microscope can't actually see what's going on inside the chip," Radavics said."If you have a hardware crypto device and the key is stored in flash, having somebody pull apart the device and put a couple of probes between the crypto chip and the flash to extract the keys that way, that's kind of a sub-$1,000 hack," he said.
"It's fairly common that if some entity wants access to your data, that hotel safe provides no protection."
"A lot of the conversation is around high-tech hacks ... but a lot of data loss can still be very mundane," Radavics said, like thumb drives or portable hard drives lost on trains, planes, and automobiles. "The encrypted device vendors have been talking about this sort of thing for years, and it's not new, and it's a little bit boring."The need to encrypt mobile devices is obvious, but data on servers can also be vulnerable to theft if it isn't encrypted -- and sometimes it's easy to get to the servers.One of the more notorious examples took place at the Australian Customs Service's national cargo intelligence centre at Sydney Airport on August 27, 2003. Thieves simply turned up, claimed to be technicians working for outsourced IT provider EDS, and walked out with two of the organisation's four servers -- along with the intelligence data they held."The burglars, described as men of Middle Eastern/Pakistani/Indian appearance, gave false EDS credentials and were given access to the mainframe room," the Sydney Morning Herald reported at the time."They spent two hours there that night before using trolleys to wheel the two servers past the third-floor security desk, into a lift and out of the building."Gatford told ZDNet that he "very rarely" sees encryption deployed on servers, and Hacklabs has a "reasonable" client base across "quite a few" industry verticals."You hear people talking about it. If you're talking about credit card environments, where you've got a requirement to encrypt the credit card information at rest, I think the most common method people use there is enabling encryption within the database," he said. "That's typically about as good as it gets in terms of host-based encryption."In fact, any kind of physical access to the organisation is usually enough."When you're physically in front of a workstation inside an organisation, it's game over, because it's trivial to boot it up of alternate media to gain access to the raw data, and from that, Bob's your uncle and you're away," Gatford said.
"When you're physically in front of a workstation inside an organisation, it's game over"
"Just about every pentest that we do, we see that the local admin workstation password is the same password for every local admin in the organisation," he said. That's either because the organisation has copied that password to the workstation as part of its standard operating environment (SOE) rollout, or simply because IT staff members need to be able to move from computer to computer efficiently."If you compromise one endpoint, and you get the local admin password, nine times out of 10 it's game over, and you reuse those credentials in the environment to go and find what you're after. You don't need the main admin access. You only need local admin workstation access, and you're good to go."It gets worse. Even physically penetrating the organisation usually isn't required."Most successful compromises of an organisation these days occur from a phishing email compromising an end user, and using the end user's workstation to attack the rest of the network," Gatford said."Looking from the outside in, you still see people making fundamentally flawed choices when designing their applications," he said. One common indicator of poor design is users being able to have their cleartext passwords emailed back to them -- something that Gatford called "an immediate fail"."The fact that they store unencrypted password values in the database, that occurs still on a regular basis. So immediately, you know that the whole way they're thinking about designing the authentication module, and presumably how that's protected, are immediate flags that they wouldn't have done anything in that space [before], and 95 percent of the time that's correct," Gatford said.But according to IBRS security analyst James Turner, all of this is actually an argument against putting too much effort into encrypting the data."The question that needs to be asked about full-disc encryption is 'What is the attack that it's actually preventing?' If the computer is on and functioning, and someone's actually using it, then full-disc encryption really isn't protecting against anything. A hacker can just go through a web vulnerability or whatever, and get access to all the plaintext stuff," Turner told ZDNet."I think encryption's incredibly important, but I don't think that this is an area that we need to be gnashing our teeth about. [Chris Gatford's] points are very valid. Authentication is where I see a lot of organisations having a lot of challenges."
Computex never fails to surprise its attendees with what can be done with technology, some items though, should never have left the drawing board.
Turner quoted a chief information security officer (CISO) who he'd approached in the last 12 months, seeking recommendations for an identity management vendor. "Show me an identity management project that's worked," that CISO replied."Therein lies the problem. Identity is actually really hard," Turner told ZDNet."Key management, there are solutions for that. Is it done as well as it could be? Probably not. Is it going to be one of the issues that we face as we start moving increasingly into the cloud? Absolutely. Is encryption of your company's data going to be increasingly important? Yes, it's going to be directly proportionate to the value of the data that you're putting there. And the cloud vendors are all desperately scrabbling to address these things."The recent massive data breach at the US Office of Personnel Management (OPM) would seem to support Turner's view. Encryption would "not have helped in this case", the Department of Homeland Security's assistant secretary for Cybersecurity Dr Andy Ozment reportedly testified to Congress, because the attackers had gained valid user credentials.Turner doesn't get a lot of enquiries about full-disc encryption."Now, that tells me one of two things. Either it's absolutely just not a priority, and not even on the radar of my clients, which I suspect is very unlikely. Or it's an area that they feel that they're handling sufficiently well that they don't need to go out there and find out what everyone else is doing," Turner said."So encryption, it's not unimportant, it's not the be-all and end-all, it's just one of the many pieces that we need to use... It's kinda like DLP [data loss prevention technology]. DLP isn't going to save you from the master assassin. DLP is going to stop something from going pear-shaped," he said."The real security that full-disc encryption offers is on a laptop that gets left behind at an airport."
Dropbox users can now issue 'file requests' to friends, colleagues
Facebook's SSD findings: Failure, fatigue and the data center
Storage company Tegile wants to make a splash with flash
HGST's new 10TB drive: Not for everyone
</article>
</item>
<item>
<title>Intel picks up Canadian wearables maker Recon Instruments</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
 	Intel has completed its acquisition of smart eyewear designer Recon Instruments.
 	 	In 2013, Intel Capital -- the investment and venture capital arm of Intel Corp -- invested $4 million in Recon Instruments for its product development, marketing, and global sales expansion.
 	 	"The growth of wearable technology is creating a new playing field for innovation, and we've made tremendous strides in developing products and technologies to capture this next wave of computing," Josh Walden, senior vice president and general manager of Intel's New Technology Group, said. 	 	Recon co-founder and CEO Dan Eisenhardt said the chip giant is an ideal partner for his company. 	 	 "This is a tremendous opportunity that will lead to amazing things, just as much for us as for our customers. 	 	 "As part of Intel, we'll have the resources to continue the mission we began with the creation of Recon in 2008, but with a level of efficacy and speed that's beyond the reach of a pioneer in a new market," Eisenhardt said in a blog post. 	 	Terms of the deal have not been disclosed by either party, but the smart eyewear CEO said that Recon will stay in Vancouver to retain its talent, brand, and "entrepreneurial spirit". 	 	The acquisition of Recon adds to Intel's growing portfolio of wearables, with the microprocessor giant purchasing smartwatch maker Basis Science early last year.
Twitter tapping into machine learning with Whetlab buy
Unleashed Software secures AU$4.5m in capital for expansion plans
LG opening cloud-hosted SaaS marketplace for startups
Pext app turns messages into pictures
</article>
</item>
<item>
<title>Groundhog Day for RBS as IT glitch leaves customers with payments delays</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
As many as 600,000 payments due to be made through RBS' banks  have been hit by delays after a fresh IT failure at the company.RBS' customer help Twitter feed broke the news on Wednesday morning. "Some customer payments are missing this morning - we are investigating this issue as a matter of urgency," RBS_Help tweeted.
RBS, whose brands include NatWest and Ulster Bank, said on Wednesday that a technology problem had held back the payments. While the problem has now been fixed, according to an RBS spokeswoman, customers may have a wait of several days before the payments are completed.
The company has set up a helpline on 03457 242424 for those affected, and said that customers whose payments have been delayed won't be left out of pocket as a result."We  are aware of an issue which has resulted in a delay to payments and  Direct Debits being applied to some customer accounts. We have fixed the  underlying issue, we apologise for the inconvenience caused and we are working flat out to get these payments updated for our customers no later than Saturday. To any customers concerned about the implications of this issue, we  advise them to come into a branch or get in touch with our call centres  where our staff will be ready to help," RBS said.The bank has been hit by a series of IT problems in recent years. In June 2012, RBS customers' balances were not properly updated due to problems with its batch processing, leaving some unable to access funds. The company set aside £175m to cover the cost to customers of the outage, and was subsequently fined £56m by regulators for the failures.The following month, some RBS customers found themselves unable to use online banking, or their debit cards.In early 2013, the bank experienced a "hardware fault" that once again left customers unable to access their bank accounts online and caused problems with its ATMs. Later that same year, another IT problem saw customers unable to make online payments or withdraw cash.
Asia-Pacific consumers are confident mobile shoppers: MasterCard
Westpac buys 11 percent stake in security firm QuintessenceLabs
MasterCard's MasterPass hits 38 percent growth in APAC in 2014
Apple Pay comes to the UK, but will your bank offer it?
</article>
</item>
<item>
<title>Cohesity comes out of stealth with $70 million in VC funding</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
On Wednesday, June 17, 2015, Cohesity emerged from stealth mode and announced a total of $70 million in venture capital funding.Cohesity was founded in June 2013 by former Nutanix founder Mohit Aron, who serves as the company's CEO. Aron previously worked on the Google File System at Google from 2003-2007.  The company's foundational product, the Cohesity Data Platform, folds storage, backup, development, and analytics onto its single platform. According to a Cohesity press release, the main value add is the lower overhead that comes from eliminating separate products and lowering data redundancy. Secondary storage is basically all the storage that isn't directly accessible by a computer system. Aron said it's kind of like an iceberg. "Think about the top of the iceberg, which is above the surface of water," Aron said. "In our minds, that's primary storage, that's where your production apps run. But everything else, stuff that's beneath the surface of water -- that's secondary storage."Existing secondary storage solutions are typically narrow in focus and created for a specific aspect of the data process such as backup or development. Additionally, people are spending tons of money on data protection, but only using it as an insurance policy, Aron said.          
Cohesity wants to offer a fuller approach to secondary storage that touches more parts of the process.  One of the key problems that Cohesity is looking to address is that of "dark data." The term dark data refers to data that is collected by an organization but isn't properly used. Usually the data is unstructured and untagged, so it is rarely accessed by the parent organization. 
The $70 million of capital is split between a $15 million Series A round and a $55 million Series B round. The Series A round was led by heavy hitters Sequoia Capital and Wing Venture Capital last year. And, the Series B was oversubscribed with investments from ARTIS Ventures, Qualcomm Ventures, Battery Ventures, Accel Partners, Trinity Ventures, and Google Ventures.The money will be used to grow Cohesity's engineering, sales, and marketing teams in order to better prepare for the release of the data platform. The company's existing teams includes former employees from Google, Netflix, and VMWare. "It's all about scaling the company at this point, and investing in the vision, investing in research and development," Aron said.  Cohesity is currently running a pilot program for the product, and working on availability through its Early Access Program. Pilot customers include Tribune Media and GS1 Canada.
OPM breach: We get exactly the IT security we're willing to pay for
Twitter tapping into machine learning with Whetlab buy
GM starts using older Chevy Volt batteries for power backup
After OPM breach, Snowden and Manning are just the beginning
</article>
</item>
<item>
<title>App Engine for PHP moves out of beta phase</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
 	Google has announced the removal of the beta label from App Engine for PHP, with the service now generally available to all customers.

 	 	 	 	The beta version of the  	App Engine for PHP has been active for over a year, launching as the fourth scripting language available on Google's App Engine, alongside Python, Java, and Go. 	Google App Engine is a platform-as-a-service (PaaS) offering from the  search giant that enables users to build and run their own applications  on Google's Cloud Platform. 	In a  	blog post announcing the change, Google said the platform handles over 800 million PHP queries each week. Last month, details emerged surrounding  	unpatched vulnerabilities in Google App Engine for Java.  	 	The flaws were found by Polish security and vulnerability research company Security Explorations, which had previously received a reward of $50,000 from Google in response to finding approximately 30 security vulnerabilities in 2014. The research firm said that at least five of these weaknesses remain, with a Google spokesperson telling ZDNet that it is a known issue and the company is working on mitigating it. 	Earlier this year, another flaw plagued Google Apps, with a fault in Google's Apps domain renewal system resulting in the  	 exposure of over 280,000 hidden WHOIS records.
Google, Microsoft, Mozilla team up for WebAssembly binary format for the web
Google updates Trends with real-time search data
Pre-orders begin for Asus Chromebook Flip, the closest yet to a Chrome tablet
Nest keeps smart home portfolio neat and tidy with latest upgrades
</article>
</item>
<item>
<title>Fronde poised to announce further revenue decline</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
Wellington-based IT services company Fronde has confirmed market rumours it is about to announce a further decline in sales.
Ian ClarkeChief executive Ian Clarke said trading for the year ended 31 March 2014 was complicated, with Fronde's Australian business doubling sales and Auckland trading well. However, sales in Wellington were subdued.Clarke said there had been an overall decline in sales for the year because the capital is Fronde's largest market."It's not a huge decline overall," he said.A big change in the way agencies procure, particularly for custom software development, was the root of the problem, he said. That change both reduced the overall volume of work and created longer sales cycles.Agencies, like enterprises, had also identified they can invest for digital transformation around the periphery without the risk of replacing core systems, he said.For the half-year to September 2014 Fronde announced a year-on-year decline in sales from NZ$32.8 million to NZ$31.6 million. Net profit declined from NZ$485,000 to a loss of NZ$1.1 million.
"In response to the poor performance and to remedy the much reduced consulting services revenue the sales team and sales leadership has been reestablished," he said."Overheads have been reduced and initiatives reduced to the minimum until the trading situation in Wellington becomes more stable. The company is still dominated by services supplied to government so the impact of a downturn in Fronde's Wellington consulting services is felt heavily on the bottom line."There had been no earlier announcement of the ongoing trading challenges in the second half because Fronde's shares are traded on the lightly regulated Unlisted platform."There is no continuous disclosure requirement on Unlisted," Clarke said.Fronde's shares are tightly held and very lightly traded. Clarke said Fronde had rules around when staff and directors were allowed to trade.Fronde acquired Australia-based Netsuite specialist OnlineOne in April 2013.
Government CTO claims NZ$60 million in savings
Wynyard Group gains NZ$40m funding ahead of ASX debut
Motorola Solutions teams with Wynyard Group for crime analytics
Vodafone New Zealand buys WorldxChange
</article>
</item>
<item>
<title>HTC to deploy advertisements to users' home screens</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
HTC users in the United States, the United Kingdom, Germany, Taiwan, and China will start to notice advertisements creeping into their BlinkFeed.  
 Announced on its blog on Tuesday, HTC said it will be introducing "native promotions" to its home-screen information feed, effective immediately.
According to the post, this rollout is a pilot to test and analyse the effectiveness of such advertisements. "It's too soon for us to say when or if these promotions will be rolled out to additional markets. Of course, we'll keep you updated as this pilot develops," the blog said."We hope that displaying native and limited promotions in this way will help you discover new and useful apps and/or products and services."These advertisements will appear as a typical BlinkFeed post rather than as labelled advertising. Users already have the option of turning the BlinkFeed function off, but HTC has said that it will be implementing a way for users to opt out of receiving the promotions. HTC boasted a net profit after tax of NT$360 million for the first quarter ended March 31, 2015, with quarterly revenue up 25 percent in comparison to last year.Earlier this week, HTC quashed speculation surrounding its potential acquisition by Asus. The computer hardware giant has been shunned by HTC, posting on its investor site that it will not consider being acquired.
Pre-orders begin for Asus Chromebook Flip, the closest yet to a Chrome tablet
Nest keeps smart home portfolio neat and tidy with latest upgrades
FCC fines AT&amp;T $100 million, says customers were misled about 'unlimited' data plan
Apple Watch now available in stores with online inventory checks
</article>
</item>
<item>
<title>Asia-Pacific consumers are confident mobile shoppers: MasterCard</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
The overall average comfort level of consumers in Asia Pacific shopping on their mobile devices has continued to remain solid, according to the latest results from the MasterCard Mobile Shopping Survey 2014 for Asia Pacific.
The survey showed that close to half of respondents in Asia Pacific said that they have made a purchase using their smartphone in the three months preceding the survey, which was conducted between October and December 2014. Those that led were China, at 70.1 percent; India, at 62.9 percent; and Taiwan, at 62.6 percent. Australia trailed in last place, at 19.6 percent, down from last year's results that saw 24.8 percent of Australian respondents claiming they had made a purchase on their smartphones. 
Raj Dhamodharan, MasterCard Asia Pacific group head of emerging payments, attributed the high levels of mobile shopping in the Asian countries to the soaring uptake of smartphone usage. In comparison to developed countries such as Australia, where consumers first shopped on PCs, he said that countries such as China and India had leapfrogged straight to using smartphones.When the respondents were asked why they were using their mobile phones for online shopping, almost half cited convenience as the top reason, followed by the fact that mobile shopping means they can do it on the go.Dhamodharan said the latest survey is indicative of the trend that spending is shifting to mobile devices. "Consumers' comfort to using mobile devices to complete payments has increased, but that's not necessarily news to those who have been tracking this closely. But what's interesting is there are signs that there are certain categories even growing faster than traditional e-commerce categories," he said. "There are certain spends that would otherwise be happening through cash or face-to-face is also shifting to mobile; that's something that we expected to see and now it is coming true."MasterCard also surveyed respondents as to what they are purchasing on their mobile phones, with clothing and other fashion accessories topping the list, making up 27.9 percent of total purchases in Asia Pacific. Products such as mobile phones, mobile gadget apps, coupons and deal sites, personal care, beauty brands, and movie tickets were also among the favourites for respondents to purchase on their mobiles.
"Previously, mobile purchases used to be in the digital goods category, like perhaps you downloaded music, and now it's going into classic retail and services," he said."There are more categories primarily because merchants are getting better at servicing folks who are online, and that's the big trend."Despite these findings, the survey also indicated that while it is on the decline, there were still 48.6 percent of consumers who said they prefer to shop in a physical retail store.On Monday, to further enhance its security capabilities, MasterCard announced that it will be extending its tokenisation support to store-branded credit cards and e-commerce merchants. This will mean that tokenisation will be available on all private-label credit cards, giving retailers the ability to accept mobile payments via Apple Pay, Android Pay, and just about any third-party payments app.
Groundhog Day for RBS as IT glitch leaves customers with payments delays
Westpac buys 11 percent stake in security firm QuintessenceLabs
MasterCard's MasterPass hits 38 percent growth in APAC in 2014
Apple Pay comes to the UK, but will your bank offer it?
</article>
</item>
<item>
<title>Piracy site-blocking Bill clears Australian House</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
A Bill that would require piracy sites to be blocked by Australian internet service providers (ISPs) following a Federal Court injunction cleared the Australian House of Representatives on Tuesday night.The passage of the Bill through parliament was assured after Labor decided to support the Bill last week. The amendments recommended by the Senate's legal and constitutional affairs committee were adopted by the government and included in the legislation.
Communications Minister Malcolm Turnbull said that while the site-blocking legislation is a good set of measures to tackle piracy, the timely availability of affordable content is key.
"When infringing sources of content are disrupted, this disruption will be most effective if Australian consumers have legitimate sources to turn to that provide content at competitive prices, and at the same time that it is available overseas," he said. "Furthermore, the industry code negotiated by internet service providers and rights holders is intended to provide a mechanism to inform Australian consumers of legitimate sources of content."Turnbull reiterated that the legislation is not intended to block virtual private network (VPN) operators."VPNs have a wide range of legitimate purposes, not least of which is the preservation of privacy -- something which every citizen is entitled to secure for themselves -- and they have no oversight, control, or influence over their customers' activities," Turnbull said.Due to the fact that the Bill sets out to block sites where the primary purpose, as determined by the Federal Court, is piracy, the minister said sites such as Netflix would not be blocked."Where someone is using a VPN to access, for example, Netflix from the United States to get content in respect of which Netflix does not have an Australian licence, this Bill would not deal with that," he said. "If Australian rights owners have got issues about American sites selling content to Australians in respect of which they do not have Australian rights, they should take it up with  them.
While supporting the passage of the Bill, Labor called on the government to respond to the recommendations of the IT pricing inquiry handed down in July 2013."We think it is unfortunate that this approach -- that is, this Bill -- favours a heavy-handed legislative approach ahead of market-based reforms such as those recommended by the House of Representatives inquiry into IT pricing, and also the recent final report of the federal government's competition policy review," said Shadow Assistant Minister for Communications Michelle Rowland. "I make the point that timely and affordable content is paramount in this case."The Bill was introduced to Australian parliament in March, and is expected to cost ISPs over AU$130,000 each year to implement.In an interview with ABC Radio National this morning, Turnbull said the cost for ISPs to implement the site blocks would be modest."The cost of compliance for them is relatively modest, and it's a cost of doing business -- assuming they don't contest the application in the court, they won't have to bear any of the legal costs," he said."The ISPs block sites now, they are asked to block sites by the AFP [Australian Federal Police], for example, if it is relating to child pornography or terrorism -- so the mechanism is there."
OPM breach: We get exactly the IT security we're willing to pay for
FCC fines AT&amp;T $100 million, says customers were misled about 'unlimited' data plan
Feds' cyber security woes can't all be blamed on legacy systems
Nokia and LG agree smartphone patent licensing deal
</article>
</item>
<item>
<title>Unleashed Software secures AU$4.5m in capital for expansion plans</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
 	Inventory management software-as-a-service (SaaS) developer Unleashed Software has received an additional AU$4.5 million in capital backing to help with its trans-Tasman and international expansion plans, as well as product development.
 	The AU$4.5 million is in addition to the AU$12 million that has been injected into the business since it was established in 2009. The majority of initial funding came from the company's two cornerstone investors, Milford Asset Management and Auckland-based firm Lewis Holdings. 
 	Founder of MYOB and now a stakeholder in Xero, Craig Winkler, is also among one of the backers for this funding round. He also recently put in money to support New Zealand-based crime and analytics software company Wynyard Group. 	Unleashed Software chairman Phil Norman said the company plans to use the extra capital to grow the company's footprint both locally and internationally, particularly its operations in sales and marketing. 	"Australia is our biggest market, and we've got an operation in Melbourne, and we also do good business in New Zealand. We're also going to use some of the funds to ramp our activities in America. We've got a small footprint already based in San Francisco," he said. 	Norman added that the company's approach to growing in the US market, mainly on the west coast, will be to target specific vertical markets, such as craft brewers and small distributors. 	In the last six months, Unleashed employed its first US staff members, appointing Lisa Miles-Heal as chief technology officer, and Anthony Mordech as chief marketing officer.
Intel picks up Canadian wearables maker Recon Instruments
Twitter tapping into machine learning with Whetlab buy
LG opening cloud-hosted SaaS marketplace for startups
Pext app turns messages into pictures
</article>
</item>
<item>
<title>Nokia and LG agree smartphone patent licensing deal</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
Another of the mobile industry's patent disputes has drawn to a close today with the announcement that Nokia and LG have signed a new licensing deal.Nokia said on Tuesday that LG had agreed to a "royalty-bearing smartphone patent licence" from the company, covering Nokia's 2G, 3G, and 4G patents.
The royalty rate that LG will pay is to be agreed through commercial arbitration, a process that's likely to conclude in one to two years, Nokia said.
"We are pleased to welcome LG Electronics to our licensing program. We've worked constructively with LG Electronics and agreed a mutually beneficial approach, including the use of independent arbitration to resolve any differences. This agreement sets the scene for further collaboration between our companies in future," Ramzi Haidamus, president of Nokia Technologies, said.LG is the first smartphone maker to become a licensee since Nokia sold off its device business to Microsoft last year, and joins 60 other companies that have patent agreements in place with Nokia.LG devices make up around 4.6 percent of all smartphones shipped globally, according to analysts Gartner.Nokia's deal with LG follows the conclusion of a similar long-running patent licensing wrangle with another South Korean smartphone giant: Samsung. In 2013, Samsung signed a five-year extension to a patent licensing agreement. That deal is also in arbitration to decide the royalty rate Samsung has to pay for using Nokia patents, the result of which is expected this year.Patents have become increasingly important to Nokia since the sale of its handsets business. Alongside its networking arm and its soon-to-be-sold Here mapping unit, patents and R&amp;D is one of three remaining business strands for Nokia.
Read more
Pre-orders begin for Asus Chromebook Flip, the closest yet to a Chrome tablet
Nest keeps smart home portfolio neat and tidy with latest upgrades
FCC fines AT&amp;T $100 million, says customers were misled about 'unlimited' data plan
Apple Watch now available in stores with online inventory checks
</article>
</item>
<item>
<title>Female CIOs winning bigger budget increases than male IT chiefs</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
For the second year in a row, Gartner's annual look at tech spending  reveals that female CIOs expect to have more success at increasing their  IT budgets this year than their male counterparts.While male  CIOs expect an increase of less than one percent (0.8 percent), female  tech chiefs expect to see their budgets go up by a healthier 2.4  percent.The worldwide survey included responses from 2,810 CIOs,  representing  more than $397bn in CIO IT budgets in 84 countries; 13.6  percent of respondents were  women.Analyst Gartner attributed this to the face that the female CIOs were more concerned about   underinvestment in risk  initiatives than male CIOs.
"The risk  data, combined with budget  numbers, may indicate that female CIOs are  more focused on the resource  side of the digital equation than their  male peers and are, therefore,  requesting and accumulating more IT  budget money," it said.
Existing technology and maintenance can crowd out innovation and impair IT's ability to be responsive to business needs.
Gartner added that this underlines the  fact that "a significant majority of CIOs of both genders believe that  the digital world is creating new and additional risks in their  environment".Risk is the big issue, the analyst believes. According to the research, female CIOs are more attuned to risk  than male IT chiefs.
The analyst in charge of the research, Gartner fellow Tina Nunno, said this is part of the reason that women are more successful than men at getting approval for large budgets. "It seems that women just tell a better story," said Nunno.  This is true regardless of whether the female CIO is reporting into a male or female boss, a CFO or a CEO, Nunno said. "The research took all those variables into account," said Nunno.According to the data, reporting structure impacts the budgets of male  CIOs more significantly  than female CIOs. When male CIOs  report to the CEO, they report a budget increase of 2.8  percent, but their budgets remain essentially flat in all other  reporting relationships with the exception of the COO, where a  slight negative budget trend appears.'The Wolf in CIO's Clothing' by Tina NunnoIn contrast, female CIOs expect to receive budget increases  regardless of reporting line.  Even when reporting to the CFO, generally  considered the toughest exec from whom to receive a budget rise, female  CIOs saw an average uptick of 3.2 percent. "For good or bad,  women and men view the top priorities virtually identically," said  Nunno. "Variations in top priorities by gender in past CIO surveys could  often be attributed to significant differences in the industries where  male and female CIOs worked. However, more recent data shows little  difference in the gender dispersion of CIOs across industries, which may  account for the consistency in prioritisation."When questioned about technology leadership, the survey found that  female CIOs are more in agreement that "analytics are increasing in  importance for their enterprises". Also, women IT chiefs are 10 percent  more likely to agree strongly that there is a shift from  backward-looking reporting to forward-looking analytics. The full results of the Gartner research are available on the company website.Read more
IoT, mobility, supply chain intelligence on GT Nexus roadmap
OPM breach: We get exactly the IT security we're willing to pay for
As IBM boards Spark bandwagon, is Databricks in danger of being eclipsed?
My open-source WordPress plug-in: Lessons learned from a release gone wrong
</article>
</item>
<item>
<title>40 percent of Australian jobs at risk of disappearing: CEDA</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
The Committee for Economic Development of Australia (CEDA) has published its major research report for the year, "Australia's future workforce?", and called for Australians to ensure that the nation is technologically ready for the workforce of the future.  
According to the think tank, more than 5 million jobs -- almost 40 percent of Australian jobs that exist today -- have a moderate to high likelihood of disappearing in the next 10 to 15 years, citing technological advancements as the reason.
CEDA's chief executive, professor Stephen Martin, released the findings on Tuesday in Melbourne. CEDA reported that while there will be new jobs and industries that emerge from the increasingly digital era, Australia will suffer if it does not plan for and invest in the right areas.   "Australia and the world is on the cusp of a new but very different industrial revolution, and it is important that we are planning now to ensure our economy does not get left behind."While we have seen automation replace some jobs in areas such as agriculture, mining, and manufacturing, other areas where we are likely to see change are, for example, the health sector, which to date has remained largely untouched by technological change," he said.  Speaking on ABC News 24, Martin used the example of a patient in a hospital bed, and said the role of a nurse checking a patient's status could be potentially nullified by machines -- a role that had previously been overlooked to be replaced by computers.  The professor said that given the unprecedented pace of technological advancement in the last 20 years, it is likely that pace will continue for the next 20 years. He urged for the government to lead the way for the rest of the country in preparing for the future."Creating a culture of innovation must be driven by the private sector, educational institutions, and government. However, government must lead the way with clear and detailed education, innovation, and technology policies that are funded adequately.  
Martin claimed that the current commitment is woefully underfunded compared to the country's global counterparts. The United Kingdom has allocated almost AU$3 billion towards technology growth centres, versus Australia's AU$190 million over the same four-year period. The report said the country needs to reconsider how it deals with reskilling workers as fields of employment disappear.
Nest keeps smart home portfolio neat and tidy with latest upgrades
My open-source WordPress plug-in: Lessons learned from a release gone wrong
Softbank leads $20 million investment round in warehouse robot maker Fetch Robotics
Influenster app brings free samples to influential product testers
</article>
</item>
<item>
<title>Optus' three-part strategy to becoming a multimedia provider</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
Optus may still be known as a telecommunications company, but maybe not for long, CEO Allen Lew has suggested.Speaking at an Australian-Israel Chamber of Commerce event in Sydney on Tuesday, Lew said the company is preparing to make inroads to transform into a multimedia solutions provider, a title that it can't give itself just yet. He said the plan will be to take an entrepreneurial approach to running the business by experimenting more, accepting failings, and moving on quicker.Lew said the company will be "borrowing a strategy from the Israeli defence force" that has been labelled as the "shield, expand, and capture" initiative."Increasing market share by merely acquiring new customers simply isn't the revenue engine it once was," he said. "For us at Singtel and Optus, with our eye on the horizon and what's happening with technology, consumer behaviour, we will focus on making sure we adopt the shield, expand, and capture imperative to capitalise on growth in the new era of multi-gigabit mobile internet."
As part of its shield imperative, Lew said the company will focus on the basics, with plans to invest close to AU$2 billion in an "integrated next-generation platform across fixed and mobile network", as well as spending on its customer care network and its staff in order to enhance customer experience for consumers, SMBs, and enterprise.As for how the company plans to expand, Lew said it will invest in areas where the company will be able to "concentrate on taking current business opportunities from the telecommunications market and enhance it with key capabilities and with digital services". 
At the same time, Lew said there are plans to further leverage partnerships with suitable global digital media platforms, drawing on the company's recent partnership with Netflix as one such example. Lew also did not rule out the possibilities of partnering with sport services, such as the Australian Football League. 
"This is a win-win relationship that generates value for both players, since telcos provide these brands with a go-to market channel, with a billing capability, and the ability to bundle with high-quality service data to provide a superior customer experience," he said.Finally, the capture initiative will see Optus look beyond its core business for opportunities. The company has already paved its way into digital advertising with Amobee; online security with the acquisition of Trustwave; data and analytics with DataSpark; and a premium mobile service called HOOQ, aimed at developing markets.Lew said the idea is to change the way that customers perceive the brand, not only as a "connection to the internet", but also as a service provider that "allows [customers] to push the boundaries of what the internet can do for them".Lew added that the company will also grow its venture capital investments. The company has already made 30 venture capital investments in the US, Israel, China, and Australia. To further extend its position in the startup space, Optus plans to set up collaborative hubs in Singapore, Sydney, Tel Aviv, and Silicon Valley to help incubate potential growth areas for the business.
Dallas Buyers Club wants to know torrenters' wage and download history
TPG commits to preserving iiNet's customer focus
Cosmic communications: African telcos turn to satellites to bridge the connectivity gap
Singapore to cease 2G services by April 2017
</article>
</item>
<item>
<title>First NBN satellite to be launched in September</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
Rural Australians could be just months away from faster internet, after the government confirmed that the first of NBN Co's new communications satellites will be launched in September.Communications Minister Malcolm Turnbull told a Coalition joint party room meeting on Tuesday that the service is expected to be commercially available in the first half of 2016.
Turnbull also said the rollout of the NBN had ramped up in June, with 436,000 fibre-to-the-node connections.
The launch will be go ahead despite the company having yet to finalise negotiations to co-ordinate frequencies with other nearby satellites.Matt Dawson, NBN program director for satellite, said the company had over 165 agreements in place with other international operators."This is recognised by experts as being well beyond industry norms," he said.  "There remain only a few network filings in the nearby arcs to conclude, these represent no or low technical risk.""NBN will shortly be notifying the ITU of its intention to bring its satellites into use in the filed orbital locations."Last month, the Australian Communications and Media Authority (ACMA) said NBN would be able to launch and operate the satellites without needing to have completed frequency negotiations. The satellites will be responsible for providing broadband services to the 3 percent of the Australian premises that are beyond the fixed-line and fixed-wireless footprints.
"If the orbital slot is an important assumption in the design of the satellite, and if the slot assumed in the design is not the one ultimately granted, there would presumably be additional costs to take into account the new slot and potential delays, depending on how long the grant of the new slot actually took," he said in 2012.Under changes proposed last month, NBN would charge satellite customers a AU$15 reconnection fee should a satellite service be disconnected in order to move house or suspend a service. Users connected to the NBN via fixed wireless, fibre-to-the-node/building/premises, and hybrid fibre-coaxial would not be charged the fee. With AAP
NBN Co under Labor was an unrealistic glee club: Turnbull
NBN switches to performance-based contracts
NBN ownership, cherry-picking rules challenged by trade agreement
Telstra accidentally redirects NBN voice calls
</article>
</item>
<item>
<title>TPG commits to preserving iiNet's customer focus</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
 	 	After suffering a blow in Monday's merger between Amcom Telecommunications and Vocus Communications, TPG has refocused its attention on the pending takeover of telecommunications rival iiNet.
 	 	With the schedule for the TPG takeover of iiNet laid out, the telcos have published a scheme booklet for investors in a plea to get them to vote in the non-compulsory meeting next month.
 	 	The consumer watchdog said it will look into what will come of  iiNet's strong customer service, indicating that many customers have raised concerns that TPG would reduce the level of customer service that iiNet is known for. According to ACCC chairman Rod Sims, the commission will also look at the ramifications for competition if the merger of two of the five largest suppliers of fixed broadband in Australia were to go ahead.TPG chairman David Teoh has broken his silence on the situation, publishing a short, one-page letter to iiNet shareholders within the 336-page scheme booklet. 	 	With TPG boasting a net profit after tax of AU$171.7 million last financial year and AU$106.7 million for the half year ending January, Teoh considers that the iiNet and TPG businesses are highly complementary in terms of geographic presence, market segments, and corporate customer base. 	 	"The combined businesses will provide broadband services to over 1.7 million subscribers, and will be well positioned to deliver scale benefits in an NBN environment," he said. 	 	"TPG acknowledges that the value of the iiNet business is a result of the high levels of customer service provided by iiNet staff, and for this reason intends to maintain the iiNet brand as part of a dual-brand strategy. Moving forward, TPG intends to preserve and foster this key strength." 	 	iiNet chairman Michael Smith posted similar views in his foreword.
 	 	iiNet shareholders have been given the option to receive cash  for their shares, or elect to receive TPG shares plus a special dividend. If the deal goes ahead, iiNet shares will be suspended from trading on Monday, August 24, 2015, and the new TPG shares will trade on the ASX on Tuesday, September 8, 2015.
Australian agencies accessing metadata more than ever before
Dallas Buyers Club wants to know torrenters' wage and download history
Optus' three-part strategy to becoming a multimedia provider
Cosmic communications: African telcos turn to satellites to bridge the connectivity gap
</article>
</item>
<item>
<title>The Apple challenge to the PC enterprise fortress</title>
<article> This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please view our cookie policy.
While PCs have been the platform of choice within business for a long time, the usage of PCs and Macs in  enterprises is currently on par, according to recent research from analyst firm Telsyte, signalling the growing presence of Apple within the enterprise sector.Telstye senior analyst Rodney Gedda said that while Apple may never have had any real intention to be used in the enterprise space, Macs are increasingly being offered as a choice by businesses to their staff -- mainly driven by the demands of employees who have come to appreciate the device as consumers -- or employees who are bringing in their own Apple devices.

Once a pariah in the enterprise, Apple has quietly emerged as a darling of executives and professionals because of the ease of use of the iPhone and the iPad. We look at how the influx of Apple devices is changing the tech landscape in business.
"There are differences in use cases, but for the most part, the business uptake of Apple devices is similar to that of PCs, and that's including bring-your-own or company-supplied devices; there's a combination of all that going in enterprise," he said.Gedda said the other primary reason why Macs are growing in popularity is because the applications that are available on OS X are compatible with what businesses are after."Apple markets itself as having numerous applications, including many for business. You can't really have a platform these days without applications," he said. "In terms of openness, Windows is technically more of an open [ecosystem], but in terms of being able to develop, deploy, and manage applications, the two are pretty much on par. You might have slight advantages with Windows, but it has its own server issues as well, compared to OS X."Cloud accounting software firm Xero is one company that recently vouched how important it is to make its applications available on Apple devices. The New Zealand-based company signed a partnership with Apple to deliver its accounting software across all devices: Macs, iPads, and iPhones.
"iOS is key to our customers' success, and we're focused on creating a beautiful, design-led experience that not only delights users, but helps them run their businesses effortlessly and flawlessly."At the same time, Apple itself has realised that there has been an increase in demand for its devices in the enterprise, recently forging a partnership with IBM to collaborate on building exclusive industry-specific applications on iOS. Some of these applications include the Retention app for the insurance industry; the Expert Tech app for telcos; Case Advice for government and law enforcement; and Incident Aware for social workers.Gedda added that the other advantage Macs have over PCs is that Apple is known for delivering a fairly consistent life cycle in terms of security, ongoing manufacturing support, updates, battery life, and ease of use -- common features that businesses assess when deciding what devices to invest in. This is unlike Android or Windows, which, Gedda said, often struggle to provide such consistency.The Australian Securities Exchange (ASX) is one company that has begun offering employees Macs as their choice of device. ASX CIO Tim Thurman explained that as part of his vision of implementing a mobile strategy within the company -- aside from being a "disruptor" in a rather traditional company -- is because he wants to ensure that employees are given the flexible choice to work how they want. Staff members are given the choice to work on a Mac, Surface, or ThinkPad, which can also be for personal use. Thurman said about a third of his staff members are currently using a Mac as their device of choice. "There are a lot of people out there that are Apple savvy, so they choose Mac for their own personal use, and the fact that they can manage it better," he said.As for how the company copes with running three different devices, Thurman said the Macs are installed with Parallels to enable users to switch between Windows and Mac OS X applications. He said this enables Mac users to log on to the company's network system through JumpBox, which "protects us if they might have anything on their device". He added that company emails are accessed via Good, which is "secured down to the device, so it works out quite well for us". "We did quite a lot of work trying to figure out what was going to be the best [device] option for us to use, but my goal was to make employees the flexibility to use for their own personal device," he said.However, the ASX is not alone. The Commonwealth Bank of Australia, the University of New South Wales, and Toyota Australia are among some other Australian companies that are also using Apple devices in their environment.
IoT, mobility, supply chain intelligence on GT Nexus roadmap
OPM breach: We get exactly the IT security we're willing to pay for
As IBM boards Spark bandwagon, is Databricks in danger of being eclipsed?
My open-source WordPress plug-in: Lessons learned from a release gone wrong
</article>
</item>
<item>
<title>Bartesian: The capsule-style drink machine for your cocktails</title>
<article>When you're having a few friends round for drinks, you don't necessarily want to be taking time away from the party to mix drinks. At the same time, some fancy cocktails would be pretty neat. There have been several cocktail machines seeking crowdfunding, but the Bartesian, currently seeking funding on Kickstarter, wants to make it even simpler. Based on Keurig-style coffee capsule technology, all you have to do is pop in a pod and press a button.
"The initial idea was actually from a friend of ours. Story goes she was at a hotel for business, and wanted a cocktail after work. The hotel had no bar, but had a Keurig in the room. Boom, lightbulb," explained Bartesian co-founder Bryan Fedorak, a former bartender and engineer."She wasn't interested in pursuing the idea further, so [co-founder and mechanical engineer] Jason [Neevel] and I kept developing it.  We had always liked beer and whiskey, and cocktails were always this special type of drink beyond our reach, really. We are literally liberating cocktails for people."To make cocktails, the Bartesian needs just two things: basic spirits -- vodka, rum, tequila and gin -- which are supplied by the user and a Bartesian capsule that contains the remaining ingredients. You would purchase the spirits and fill the machine's reservoirs so that you'd be able to make a whole bunch of cocktails before having to refill.
To start with, there will be six cocktail capsules available: Margarita (tequila, Cointreau, lemon juice and lime juice), Sex on the Beach (vodka, peach schnapps, cranberry juice and pineapple juice), Cosmopolitan (vodka, Cointreau, cranberry juice and lime juice), Bartesian Breeze (rum, strawberry, pineapple, lime and coconut juice), Uptown Rocks (gin, white grape, peach and lemon juice with a sprig of cilantro) and the Zest Martini (vodka, grapefruit juice and cucumber bitters).
The system uses a barcode on the lid of each capsule to read the cocktail, and automatically adjusts for each one, so you don't even need to input recipe instructions. There's even provision for those who prefer their drinks "shaken, not stirred." The Bartesian comes with a cocktail shaker, so you can deposit the cocktail into the shaker rather than a glass and shake it all up for a martini, 007-style.And, while it might not provide you with a morning caffeine buzz, it does have what Fedorak feels is an important advantage over a Keurig machine. The capsules can be thrown in with your general household recycling.On the Kickstarter page, the team has posted a list of cocktail capsules that could be made in the future, and Fedorak said that Bartesian is also considering the possibility of capsules that you can fill yourself to experiment with your own spirits and mixers.The machine is currently being offered as a reward with a minimum early bird Kickstarter pledge of $249 at time of writing, which includes 12 capsules. After the Kickstarter campaign ends, the Bartesian cocktail machine will retail for over $400. You can also add packs of 12 capsules to your reward for $20 per pack. Head on over to the Kickstarter page to check it out.
</article>
</item>
<item>
<title>Zuckerberg donates $5M to scholarship fund for undocumented immigrants</title>
<article>Facebook CEO Mark Zuckerberg and his wife, Priscilla Chan, have made a $5 million donation to a scholarship fund that helps undocumented immigrants attend college.In a Facebook post Wednesday, Zuckerberg called the donation to TheDream.US fund an "investment" toward creating scholarships for more than 400 undocumented students in the San Francisco Bay Area during the next five years. Noting that many young immigrants attend school legally in the US, Zuckerberg said a lack of documentation is often a stumbling block to a college education and federal aid."America was founded as a nation of immigrants," Zuckerberg wrote in his post announcing the donation. "We ought to welcome smart and hardworking young people from every nation, and to help everyone in our society achieve their full potential. If we help more young immigrants climb the ladder to new opportunities, then our country will make greater progress."A longtime proponent of immigration reform, Zuckerberg argued in 2013  that changes to immigration and visa policy were necessary to boost the  US economy and job market. In March of that year, he was part of a  group of tech executives that sent a letter  to President Barack Obama that said the system hampers hiring and  R&amp;D due to visa shortages, green card delays, and issues that arise  when employees have family members who would also need to gain  residency.Founded in 2013 by former Washington Post publisher Don Graham and Bill Ackman, CEO of Pershing Square Capital Management, among others, The Dream provides $25,000 in scholarship aid to help undocumented students earn bachelor's degrees at national partner colleges.
In specifying the donation's focus, Zuckerberg said he and his wife "wanted to help extend their efforts in the Bay Area as part of our ongoing efforts to support social and economic programs in our local community."While hefty, the donation pales in comparison to others made by Zuckerberg, who is part of a campaign led by Microsoft founder Bill Gates and investor Warren Buffett to get the richest people in the US to donate most of their wealth to philanthropic causes. Zuckerberg's first major project was giving $100 million in Facebook stock to public schools in Newark, N.J., in 2010. Two years later, Zuckerberg pledged 18 million Facebook shares to the Silicon Valley Community Foundation, a donation that at the time was worth nearly $500 million.
</article>
</item>
<item>
<title>Khail and Ashley's best (and worst) of E3 2015 (Tomorrow Daily 194)</title>
<article>LOS ANGELES -- We're here at the Los Angeles Convention Center this week for E3 2015, and it's crazy in here. We all agree, there really aren't any major losers this year in terms of the big announcements, but we definitely found some room for improvement in some specific spots of the show and the games within it.Thanks to Jess McDonell for taking the time to stop by our show to tell us her personal bests and worsts, too!Lastly, don't miss our virtual-reality experience from the show floor -- it's your chance to join us and check out some of the biggest booths in a cool new way!

Here are some links and notes for all the things on the show today:
Of course, you can find us everywhere on social media. Like, follow and heart us as you desire!Tomorrow Daily on: Facebook | Twitter | Tumblr | Instagram | Google+Ashley on Twitter | Khail on TwitterSubscribe to Tomorrow Daily:iTunes (HD) | iTunes (SD) | iTunes (HQ) | iTunes (MP3)RSS (HD) | RSS (SD) | RSS (HQ) | RSS (MP3)Follow all the latest news from E3 2015 on CNET and GameSpot.
</article>
</item>
<item>
<title>NASA gets a step closer to looking for life on Jupiter moon Europa</title>
<article>NASA has made clear its intentions to get a closer look at Jupiter's fascinating moon Europa in the coming decade, and on Wednesday the space agency shared more details about how it plans to explore the Jovian satellite with its red-striped icy shell and hidden liquid ocean beneath."Europa is the most likely place to find life in our solar system today, because we think there's a liquid water ocean beneath its surface, and we know that on Earth, everywhere that there's water, we find life," Robert Pappalardo, a project scientist for NASA's Europa mission, said in the video below. On Wednesday, NASA announced that its mission concept to send a spacecraft to survey Europa and assess its habitability passed its first major internal review and is moving into a development phase called formulation.
"Observations of Europa have provided us with tantalizing clues over the last two decades, and the time has come to seek answers to one of humanity's most profound questions," John Grunsfeld, associate administrator for NASA's Science Mission Directorate, said in a release.
The Galileo mission of the 1990s was the first to collect strong evidence that salt water warmed by tidal heating and a rocky sea floor could provide the necessary building blocks for life to survive on Europa, inspiring one of the more realistic sci-fi movies in recent memory, "Europa Report."Unlike the manned mission to Europa portrayed in the movie, NASA first plans to send an unmanned probe to orbit Jupiter in the 2020s, arriving several years later and making 45 flybys of Europa to study the surface and interior composition of the tantalizing moon.
Pappalardo explains that the mission plan to orbit Jupiter and pass by Europa every few weeks rather than orbit the moon itself was chosen because of the high doses of radiation Europa receives from the huge planet, which could adversely affect a spacecraft.The mission could also be able to get samples from water and/or gas plumes that may be shooting out into space from the surface of Europa.
If evidence of life is found in the plumes or within Europa, the implications could be nothing short of existential."If there is life in Europa, it almost certainly was completely independent from the origin of life on Earth," Pappalardo explained. "That would mean the origin of life must be pretty easy throughout the galaxy and beyond."If this is the first you've heard of the Europa mission, check out the video below and have some gloves handy to pick your mind up off the floor after it gets totally blown.
</article>
</item>
<item>
<title>Major security flaw may affect 600M Samsung smartphones</title>
<article>Millions of Samsung Galaxy smartphone owners may be at risk of eavesdropping of calls, theft of data and installation of malware -- and there isn't much they can do about it.The flaw is found in Swiftkey keyboard software preinstalled on 600 million of the South Korean electronics giant's smartphones, mobile security company NowSecure said Wednesday. Affected users are powerless to address the vulnerability because they cannot uninstall the software.Affected devices include the recently released Galaxy S6, as well as the S5, S4, and S4 Mini on all major carriers, NowSecure said.Samsung said it will release a fix for the problem in the next few  days, accessible through its service Samsung Knox. It will come in the  form of a security policy update that can be downloaded onto the phones."Samsung  takes emerging security threats very seriously," Samsung said in a  statement. "In addition to the security policy update, we are also  working with Swiftkey to address potential risks going forward."
Consumers can be forgiven for feeling whipsawed by security flaws and breaches that compromise their data held by retailers and banks and now on the mobile devices they use. Target in 2013 reported 40 million people had their credit card numbers stolen from its point of sale terminals, and followed up that report with news that another hack got the names, email addresses and phone numbers of 70 million customers. JPMorgan Chase, the largest bank in the country, reported last year that 76 million households and 7 million small businesses had their account information stolen. And on Monday, password manager service LastPass announced hackers had stolen the email addresses and master password clues of its users.NowSecure said Samsung was notified in December 2014 of the problem."While Samsung began providing a patch to mobile network operators in early 2015, it is unknown if the carriers have provided the patch to the devices on their network," NowSecure said in its report. All of the phones either have no patch available or the status of the patch is unknown, according to the list.The phones are vulnerable to attack from a variety of fronts, according to NowSecure's technical analysis of the flaw. A less sophisticated hacker who's nearby a phone might gain access through unsecured Wi-Fi connections. Or a serious attacker could use a more involved approach to gain access from much farther away, according to NowSecure.As a result, the flaw would appear to be a pervasive and serious problem until fixed."To reduce your risk, avoid insecure Wi-Fi networks, use a different mobile device and contact your carrier for patch information and timing," NowSecure said in its report.However, some security professionals noted that an attack might have limited returns for hackers."It appears there needs to be a lot of things in place for this to work properly," Nathan Collier, senior malware intelligence analyst at Malwarebytes Labs, said in an email about NowSecure's description of how an intruder could break into a phone.Noting that he didn't expect to see anyone carrying out such an attack, Collier said it wasn't the typical route taken by people trying to take over computers and devices. "Malware authors are looking for big returns using the path of least resistance, and having to write code for several different phone models is quite tedious. Samsung is aware of the issue. Hopefully they will be providing a patch for their customers shortly."
</article>
</item>
<item>
<title>Here's everything that mattered at E3 2015 (pictures)</title>
<article>This year's E3 game developer conference is still underway, but all of the major press conferences are now behind. And that means it's time to sift through all of the trailers and promises to pick the biggest news from E3 2015.
Microsoft's Xbox One will be able to play a select number of Xbox 360 games later this year. If you bought your game from the Xbox Marketplace, you'll be able to download it to your Xbox One. If you bought your game on disc, you'll need to keep the disc in your console as a sort of authentication, and then download the game. About 100 titles will be available at launch, and developers need only grant Microsoft permission to get their older game into your hands.
Valve's Steam Machine platform is almost upon us. The aim is to bring console-simplicity to proper PCs: we've waited quite some time for these machines to make their debut, and one of the first is almost here. The Alienware Steam Machine is a black box packing Intel and Nvidia hardware, and it'll arrive as early as October 16 for folks who pre-order.
Pre-order from GameStop.
The fans asked, and clamored, and pleaded, and Square Enix seems ready to answer their wishes -- and reap the mountain of cash that fans will undoubtedly hand over to reclaim a tiny piece of nostalgia.
Don't call it a comeback: PC gaming may seem like it's been on the ropes for years, but a combination of virtual reality, living-room-friendly Steam Machines, and E-sports have seen a resurgence for the PC.
Few series inspire the same amount of fervor as Fallout, so news that Fallout 4 is on the way is...exciting, to say the least. Better still, those of us who pre-ordered the Pip-Boy edition will have a gargantuan gadget to wear on our wrists. I for one can't wait until November 10.
Valve CEO Gabe Newell hasn't had very kind things to say about Microsoft over the years, but the companies will nonetheless be joining forces on the virtual-reality front. Couple this news with word that Microsoft will be partnering with Oculus on its own VR ambitions, and it's clear that Microsoft has all of its virtual bases covered.
 Minecraft and HoloLens: a match made in heaven. It turns out that the survival-oriented, castle-building and tree-punching simulator is a perfect fit for Microsoft's augmented-reality headset. 
Many of us had long since written The Last Guardian off as a loss, never to see the light of day. Shame on us for doubting Sony, I guess: The Last Guardian is coming to the PlayStation 4. We still don't have a release date, but seeing some gameplay in action might just be enough to encourage us to keep hope alive until 2016.
Nintendo's bizarre E3 conference involved quite a few puppets and 3DS game, but generally played it safe. But that's OK! Nintendo fans are getting a new Star Fox game, cool Amiibo figurines to collect, and Super Mario Maker, which will let us create our own Super Mario adventures.
There was more news for Xbox One owners: a brand new dashboard is coming later this year. Microsoft is promising faster performance, support for trading messages in-game with your friends, and support for Cortana. That last bit is especially interesting: you'll be able to ask Cortana if your friends are online, or get tips on the games you're playing.
</article>
</item>
<item>
<title>Here's what mattered at E3 2015 (pictures)</title>
<article>This year's E3 game developer conference is still underway, but all of the major press conferences are now behind. And that means it's time to sift through all of the trailers and promises to pick the biggest news from E3 2015.
Microsoft's Xbox One will be able to play a select number of Xbox 360 games later this year. If you bought your game from the Xbox Marketplace, you'll be able to download it to your Xbox One. If you bought your game on disc, you'll need to keep the disc in your console as a sort of authentication, and then download the game. About 100 titles will be available at launch, and developers need only grant Microsoft permission to get their older game into your hands.
Valve's Steam Machine platform is almost upon us. The aim is to bring console-simplicity to proper PCs: we've waited quite some time for these machines to make their debut, and one of the first is almost here. The Alienware Steam Machine is a black box packing Intel and Nvidia hardware, and it'll arrive as early as October 16 for folks who pre-order.
Pre-order from GameStop.
The fans asked, and clamored, and pleaded, and Square Enix seems ready to answer their wishes -- and reap the mountain of cash that fans will undoubtedly hand over to reclaim a tiny piece of nostalgia.
Don't call it a comeback: PC gaming may seem like it's been on the ropes for years, but a combination of virtual reality, living-room-friendly Steam Machines, and E-sports have seen a resurgence for the PC.
Few series inspire the same amount of fervor as Fallout, so news that Fallout 4 is on the way is...exciting, to say the least. Better still, those of us who pre-ordered the Pip-Boy edition will have a gargantuan gadget to wear on our wrists. I for one can't wait until November 10.
Valve CEO Gabe Newell hasn't had very kind things to say about Microsoft over the years, but the companies will nonetheless be joining forces on the virtual-reality front. Couple this news with word that Microsoft will be partnering with Oculus on its own VR ambitions, and it's clear that Microsoft has all of its virtual bases covered.
 Minecraft and HoloLens: a match made in heaven. It turns out that the survival-oriented, castle-building and tree-punching simulator is a perfect fit for Microsoft's augmented-reality headset. 
Many of us had long since written The Last Guardian off as a loss, never to see the light of day. Shame on us for doubting Sony, I guess: The Last Guardian is coming to the PlayStation 4. We still don't have a release date, but seeing some gameplay in action might just be enough to encourage us to keep hope alive until 2016.
Nintendo's bizarre E3 conference involved quite a few puppets and 3DS game, but generally played it safe. But that's OK! Nintendo fans are getting a new Star Fox game, cool Amiibo figurines to collect, and Super Mario Maker, which will let us create our own Super Mario adventures.
There was more news for Xbox One owners: a brand new dashboard is coming later this year. Microsoft is promising faster performance, support for trading messages in-game with your friends, and support for Cortana. That last bit is especially interesting: you'll be able to ask Cortana if your friends are online, or get tips on the games you're playing.
</article>
</item>
<item>
<title>Fitbit prices IPO at $20 per share, above previous price range</title>
<article>Fitbit, the first wearable-tech focused company to go public, on Wednesday priced its initial public offering at $20 per share, higher than its previous estimated range per share.The San Francisco-based company, a leading maker of fitness- and health-tracking gadgets, raised $731.5 million on the sale of nearly 36.6 million shares. The IPO values the 8-year-old company at about $4.1 billion.Fitbit's IPO comes amid heightened mainstream interest in the market for wearable technology. Nearly every traditional technology company -- from Intel and Microsoft to Samsung and LG -- has a device for sale or a hand in the market. Apple gave the sector a boost with the launch of its Apple Watch, a smartwatch that doubles as a fitness and activity tracker. Samsung and LG, as well as smaller companies like Jawbone and Pebble, are also competing in the wearables market. Even nontraditional technology companies, like watchmaker Fossil, are seeing opportunities in the wearables space.Founded in October 2007 by James Park and Eric Friedman, Fitbit focuses its efforts on a wide range of simple, colorful devices meant to be clipped or strapped to the body for counting steps, measuring sleep activity and monitoring workouts. Fitbit also develops proprietary companion software for smartphones and the Web. In addition to displaying stats, the software includes training tips and a calorie tracker designed to improve the user's health regimen.
In eight years, Fitbit has come to dominate the market for wearables. Sales rose 142 percent last year, to $745.4 million, according to industry tracker NPD Group. The company also sold nearly 11 million devices last year, to capture 68 percent of the activity tracker market, up from 58 percent in 2013. Jawbone held 19 percent of the market in 2013; its share of the market in 2014 is not known.Fitbit's shares are expected to begin trading Thursday on the New York Stock Exchange under the symbol "FIT." The lead underwriters are Morgan Stanley, Deutsche Bank and Bank of America Merrill Lynch. Fitbit says it plans to use the cash raised through the IPO to fund its ongoing research and development, and has said it may also use a portion to acquire another company.
</article>
</item>
<item>
<title>Judge: Ellen Pao must pay Kleiner Perkins $276,000 in legal fees</title>
<article> 	Ellen Pao may still have to write a hefty check to Kleiner Perkins after all, but it could be worse. 	A San Francisco judge tentatively ruled Wednesday that Pao must pay $275,966.33 to her former employer, Silicon Valley venture capital firm Kleiner Perkins Caufield &amp; Byers, as a result of losing her high-profile sex discrimination case against them. 	More than a month after her loss, Pao and her attorneys had requested Judge Harold Kahn dismiss the order for her to pay nearly $1 million in legal fees to the storied VC firm, arguing that the amount was "grossly excessive and unreasonable." 	But Kahn said Wednesday that Pao's motion to strike the costs was both "granted in part and denied in part," as his tentative ruling of nearly $276,000 in fees is less than three-quarters of the nearly $973,000 Kleiner Perkins was seeking as its legal right. Pao's fee will pay mostly for the cost of Kleiner's expert witnesses.  	Kahn said he took into consideration the financial resources of both parties and ordered Pao to pay the firm an amount similar to what she paid for her expert witnesses. A hearing on the matter is scheduled Thursday in San Francisco Superior Court, after which the ruling could become final.In March, a San Francisco jury of six men and six women rejected Pao's claim of discrimination and retaliation over her 2012 firing from Kleiner Perkins. The monthlong trial featured accusations of a boys club-like atmosphere at the firm, testimony about alleged poor job performance during Pao's seven years there, and discussion of Pao's affair with a married colleague.
 	Pao had sought $16 million in lost wages in the trial that rocked Silicon Valley, captured the nation's attention and cast a spotlight on gender and racial disparities in tech. 	Less than a month after the verdict, Kleiner Perkins said it would waive its legal fees if Pao agreed not to appeal her loss. After considering it, Pao filed a notice to appeal on June 1, which gave her 40 days to list her arguments. 	Still, Kleiner Perkins spokeswoman Christina Lee said Wednesday the firm was pleased the court reached a fair result on the fees. 	"This tentative ruling recognizes that our settlement offer was reasonable and made in good faith," Lee said. "It also recognizes the cost rules still apply when a plaintiff refuses a reasonable settlement offer and forces the parties to go through an expensive trial." 	Pao, the current interim CEO of the social-networking and new site Reddit, had no comment, her spokeswoman Heather Wilson said Wednesday.
</article>
</item>
<item>
<title>These headsets promise a taste of VR without breaking the bank</title>
<article> 	The virtual reality conversation is currently dominated by devices like the Oculus Rift, HTC Vive, and Sony Project Morpheus, impressive gadgets that'll leave you tethered to a gaming PC or console. The Samsung Gear VR is a bit more mobile, but it only works with select Samsung phones. 	And then there's the matter of price: we can expect to spend hundreds for each of these headsets, to say nothing of what you'll need to spend on the hardware to drive it. 	There's a cheaper way. You aren't going to get a stellar experience, but there are quite a few VR headsets out on the market, or on the way that offer an appreciable experience for far less. These headsets all require an Android or iOS smartphone, and you'll find plenty of VR apps kicking around your app store of choice -- many are labelled as being designed for Google Cardboard, but they'll work with these devices, too.  And if you're looking for something to do with your new VR headset, why not  check out CNET, straight from E3?
 	Google's Cardboard is probably the most recognizable member of the cheap-VR bunch, as it was one of the first devices to champion dirt-cheap virtual-reality experiences.  Of course I use the word "device" loosely: as the name implies, it's just a piece of cardboard, coupled with velcro and a pair of lenses. You can pick most of the components up at your local hardware store to build your own using  Google's instructions, or pick up one of the numerous unofficial Google Cardboard dopplegangers -- they start at as little as $15 (about £9, AU$19).
 	Immerse's Virtual reality headset is a little pricier than Google's offerings, at $47 (converted, about £30, AU$61). But you're also getting a bit more than a cardboard box to hold up to your face. You can strap it onto your head, to start, and foam inserts make for a more comfortable experience. The plastic case means it'll stand up to a bit of roughhousing, and the pair of lenses are adjustable, so you can potentially use them without wearing your glasses.
 	Archos is no stranger to...uhh...inexpensive wearable tech of questionable value, so it stands to reason that the company would get into the cheap virtual reality market. The Archos virtual reality glasses are no exception: £25 (which works out to about $40, AU$52) gets you a chunky plastic case to slot your Android or iOS phone into, and strap onto your head. Archos will also be offering a video player that'll also let you watch 2D and 3D movies -- that app will be available later this year.
 	Mattel brought back a classic earlier this year,  and it's been revamped to deliver a virtual- and augmented-reality experience on a budget. You can just slide your Android or iOS smartphone into the View-Master, as it's compatible with the many Google Cardboard and Cardboard-ready apps kicking around the Google Play and Apple App stores. The little plastic reels you might be familiar with make a return too: point the View-Master at them to get a glimpse  of dinosaurs, or of San Francisco. The new View-Master is expected to arrive later this year, for $30  (about £19, AU$39).
</article>
</item>
<item>
<title>FCC could get first Net neutrality complaint soon</title>
<article>The Federal Communications Commission's new Net neutrality rules have been in effect for less than a week, and the agency is about to receive its first formal complaint from a company alleging harm.Commercial Network Services, a San Diego-based company that operates webcams and streams live video feeds, said it will soon file a formal complaint against cable giant Time Warner Cable for charging it to deliver its streaming videos to its broadband customers, according to Barry Bahrami, CEO of the company. The Washington Post first reported the news of the complaint.Bahrami accuses Time Warner Cable of "double dipping" by charging its broadband customers for access to the Internet and also charging companies, like Commercial Network Services, for delivering video to consumers who subscribe to Time Warner Cable's broadband service.  Because Commercial Network Services has refused to pay the additional fee that Time Warner Cable is charging it to send traffic to its customers, Bahrami says that Time Warner Cable is directing Commercial Network Services' traffic through a congested connection that serves Time Warner's broadband customers. Bahrami says by doing this Time Warner Cable is severely degrading the quality of its streams, such as the San Diego Web Cam, which offers live streaming of the San Diego harbor."This could all be changed in a few minutes if it were not for Time Warner greed," he said 
Once filed, Commercial Network Services' complaint will be the first formal complaint that the FCC has received since its Net neutrality rules went into effect last week. Net neutrality is the principle that all Internet traffic be treated equally and that wireless carriers and Internet service providers not put businesses or customers at a disadvantage.Earlier this year, the FCC passed new rules to protect Net neutrality to replace rules that had been thrown out by a federal court in January 2014. As part of the new rules, the FCC expanded the scope of Net neutrality and in addition to formulating clear cut "bright line" rules that prevent broadband providers from blocking or slowing down traffic and prevents them from charging for so called "fast lanes," the new rules also for the first time allow the FCC to determine if commercial deals between private companies exchanging Internet traffic are "fair and reasonable" or whether these deals could harm consumers' access to the Internet. Instead of applying blanket restrictions on companies exchanging Internet traffic as the agency has in the "bright line" rules, the FCC will examine disputes over Internet "interconnection" on a case-by-case basis. While the FCC's ruling from one case to another could vary, how the agency handles this first complaint once it's filed could give broadband and other Internet companies a better sense of how far the FCC will go in terms of regulating the Internet. The dispute is similar to disagreements that streaming video provider Netflix has had with other broadband providers Comcast and Verizon. Netflix's CEO Reed Hastings publicized the disputes his company had with these two broadband providers last year. As a result, the FCC expanded the scope of its Net neutrality rules to include a provision that allowed it to examine these deals more closely.In a separate Net neutrality development today, the FCC said it plans to impose a fine up to $100 million fine on AT&amp;T for allegedly misleading customers who subscribe to its unlimited data plans. The FCC has accused AT&amp;T of violating the transparency rule of the agency's Net neutrality regulation. The transparency rule was the only part of the FCC's original 2010 Open Internet order that was not thrown out when the court ruled against the FCC in 2014.Critics, who oppose the FCC's new rules, say the FCC has overstepped its authority by even examining commercial agreements between companies exchanging Internet traffic. They fear the agency could try to use its authority to set rates on services or take other actions that could stifle competition. FCC Chairman Tom Wheeler has denied that this is the agency's aim. While defending the new rules in front of a congressional hearing in March, he said he looked forward to getting a Net neutrality complaint filed under this new complaint process so that he could show critics how high the the FCC has set the bar for intervening in such commercial deals.Commercial Network Services hasn't yet filed its formal complaint with the FCC, but Bahrami said the paperwork will be filed in the next few days. The FCC declined to comment. Time Warner Cable said in a statement that it is not violating the FCC's rule. It claims it does not charge companies exchanging traffic with it to pay fees so long as the amount of traffic the companies exchange is roughly equal. It said that under its policy, Commercial Network Services does not qualify for such an arrangement. It also denies that it is deliberating slowing down the company's traffic to its broadband customers.  And Time Warner Cable is confident the FCC will side with it in this dispute. "Time Warner Cable's interconnection practices are not only 'just and reasonable' as required by the FCC, but consistent with the practices of all major ISPs and well-established industry standards," the company said in its statement. 
</article>
</item>
<item>
<title>Nintendo having to increase production estimates to keep up with Amiibo toy demand</title>
<article> 	 	LOS ANGELES -- When it comes to Nintendo's Amiibo line of toys, demand is still far larger than the company had expected. 	 	Nintendo has been significantly raising its internal production estimates for the figurines, which are molded to look like popular characters, such as Mario or his pal Yoshi from the action adventure game Super Mario Bros. 	 	But what they look like is only a part of what they do. When a player touches one of them to one of the company's game controllers or its 3DS handheld, the character appears in the game's world. The toys, of which the company has shown more that 60, were first released last year.
 	 	The toys have become one of the fastest-growing segments of the video game industry, tallying more than $4 billion in sales since the first iterations, created by game maker Activision Blizzard, were released in 2011.
 	 	Nintendo announced its involvement last year, but it's not alone. Walt Disney has begun offering its own toys, called "Disney Infinity," creating figurines based on its characters like the superhero family from the Pixar movie "The Incredibles." And Warner Bros. has announced it will be fielding its own take on the genre with a game called Lego Dimensions. 	 	Activision Blizzard meanwhile has been creating new characters each year for its game series Skylanders. This year, it partnered with Nintendo to bring Amiibo toys into Skylanders games. 	Eric Hirshberg, head of the Activision division that makes Skylanders, said his teams have faced unintentional moments where demand was higher than supply. But, thanks to prior experience making accessories like plastic controllers shaped like guitars for its Guitar Hero series of games, he said Activision has been able to handle demand well. 	"There is way more value in each one of these toys than toys that don't come to life," he said, adding "They're cool toys in and of themselves." 	 	So far the market appears to still be expanding quickly.
 	 	Reggie Fils-Aime, president and chief operating officer of Nintendo's North American division, said he was shocked by how many Amiibo toys each customer is buying, though he declined to say how many. And that's beyond the surprisingly high demand. 	 	When Nintendo launched new Amiibo toys in April, it sold more than a million toys in the US during their first 30 days on the market. 	 	"We're at a point where we have to take our volume estimates and double them or triple them based on the levels of demand we're seeing," he said during an interview here at the Electronic Entertainment Expo, the video game industry's biggest trade show. "We're working very hard to meet that demand." 	Fils-Aime said Nintendo has been working with its suppliers to "push the envelope" for production schedules in an effort to catch up, but "the consumer demand continues to outpace supply." 	 	Nintendo said it plans to offer new Amiibo toys for the holidays, including one made of yarn, and another to commemorate the 30th anniversary of the Super Mario Bros. game that popularized the company's products. It is also expanding the technology to collectible cards, which can also be tapped to controllers and handheld devices to bring characters into games. 	 	"For us it's making sure we continue to innovate on the gameplay," he said. I know our content is strong. I know what we're executing is unique and compelling."Follow all the latest news from E3 2015 on CNET and GameSpot.
</article>
</item>
<item>
<title>Square gives Magic Johnson a seat on its board</title>
<article>Mobile-payments company Square will be adding some magic to its team.Former NBA star Earvin "Magic" Johnson will join its board this summer, Square said in a release on Wednesday."I am ready to get to work as a member of Square's board of directors so that any business, in any community, can compete and win," Johnson said in the release. Square makes small, square-shaped readers that allow small businesses and individuals to take credit card payments on their tablets or phones. The company, which recently pivoted to a merchant-focused model to support its popular card-reading hardware and storefront software, hopes to tap into Johnson's experience in fostering businesses in urban communities. Magic, winner of five NBA championships with the Los Angeles Lakers and a three-time MVP, is the chairman and CEO of Magic Johnson Enterprises and chairman of the charity, the Magic Johnson Foundation. Last year, he joined the board of Hero Ventures, the company that puts on the Marvel Experience Tour.

"As a successful entrepreneur and advocate for economic inclusion, Earvin's unique perspective will be invaluable to our community of sellers and our board," Square CEO Jack Dorsey said in the release. It's been a busy week for Dorsey, who will take over as interim CEO of Twitter -- the company he co-founded -- in July. Dorsey will be replacing outgoing CEO Dick Costolo. Square and representatives for Magic Johnson weren't immediately available to comment on when Johnson would start.
</article>
</item>
<item>
<title>Google zooms in on smart-home vision with Nest Cam</title>
<article>SAN FRANCISCO -- When Google's Nest bought Dropcam, maker of a smart security camera, the arrival of Nest's own home-monitoring device seemed only a matter of time.Smart-home company Nest -- which is owned by the search giant -- made it official Wednesday, announcing its latest Internet-connected device for the household: the Nest Cam.The camera shoots high-definition security footage, lets you zoom in on different areas of the house and has sound and motion sensing features. The camera will also adjust the lighting of footage at night to make it easier to see. The product will cost $200 and will start shipping next week. It will be available at retailers like Best Buy, Amazon and Target.Nest also announced a cloud video service called Nest Aware that will let you save video footage instead of just live-streaming it. A subscription to the service will cost $10 a month to let you view footage from over the past 10 days, or $30 a month for footage over the last 30 days."We've changed the conversation about the connected home," Tony Fadell, Nest's co-founder, said during a press event here.

The product unveiling takes place as Google and other Silicon Valley giants become more ambitious about their role in consumers' homes. But the move -- which could give Nest valuable insight into people's behavior while at home -- will also likely raise concerns from privacy advocates who think Google already has too much access to people's personal data. Google relies on that sort of data to make its services better and, in some cases, attract advertisers. The smart-home market is already crowded. Apple last year unveiled Homekit, which lets people use their iPhones to control smart devices around the house. Last week, Apple said Homekit would be able to control security systems, window shades and motion sensors. Also last year, Samsung bought SmartThings, a startup that aims to be a hub for Internet-connected products. The camera is the third device developed by Nest, co-founded and led by Fadell, a former Apple executive. He's considered a hardware guru and played a key role in the development of Apple's original iPod and iPhone. Nest's other products are a smart thermostat and a smart smoke detector. Google bought Nest in February 2013 for $3 billion. 
The new product doesn't come as a surprise. Nest announced it was acquiring Dropcam, which makes a Web-connected home-security camera, almost exactly a year ago for more than $550 million. Earlier this month, pictures of the Nest Cam leaked on the blog Droid Life.Google has also been focused on becoming the software platform that powers the smart home. At the search giant's annual developer conference last month, the company announced Brillo, a platform that aims to make all the disparate Web-connected devices in a home work together. The key to the software is that it will work with low-powered devices that don't have much computing punch. Google said it would be available for software developers in the third quarter.The smart-home effort is not new for Google. In 2011, the company introduced Android@Home, which also allowed developers to build apps so several household devices could be controlled by an Android phone. The platform never gained traction with app makers.Nest on Wednesday also introduced a new version of its smoke detector, the Nest Protect. One new feature is the ability to silence the alarm from a smartphone app. Nest also announced a home insurance program with Liberty Mutual and American Family Insurance to give customers rewards -- like discounts -- for owning a Nest Protect. The company also touted new features that let Nest products communicate with each other. For example, if the smoke detector senses smoke, the thermostat will automatically shut off.
</article>
</item>
<item>
<title>Keep watching those cat videos. They may be good for your health</title>
<article>So you spent your Saturday night watching videos of Nala cat or Henri? Don't feel embarrassed: you might have been doing something good for yourself. 	 	A new study in the journal Computers in Human Behavior surveyed 7,000 people via social media about their viewing of cat videos and how it affects their moods. Thirty-six percent of participants described themselves as a "cat person" and 60 percent said they liked both cats and dogs. 	 	Participants in the study reportedly had fewer negative emotions such as anxiety, sadness and annoyance after watching cat videos, even when they viewed Internet cats at work or while studying. The pleasure they got from watching cat videos seemed to outweigh any guilt they felt about procrastinating. 	
 	 	The survey also found that cat owners and people who tend to be shy and agreeable were more likely to watch cat videos. Twenty-five percent of the cat videos people watched were ones they sought out; they simply happened to come across the rest.
 	 	Internet cat videos are big business. There were more than 2 million cat videos posted on YouTube in 2014, according to Internet data cited in the study. With almost 26 billion views, cat videos had more views per video than any other category on YouTube.
 	 	Myrick said the results suggest that future work could explore how online cat videos could be used as a form of low-cost pet therapy. 	 	"Some people may think watching online cat videos isn't a serious enough topic for academic research, but the fact is that it's one of the most popular uses of the Internet today," Myrick said. "If we want to better understand the effects the Internet may have on us as individuals and on society, then researchers can't ignore Internet cats anymore." 	 	Craving a cute kitty fix now? Click on the video below and enjoy.This article originally appeared on CBSNews.com under the headline "Cat videos may be good for your health." 
</article>
</item>
<item>
<title>Are these the stars that shaped the universe?</title>
<article>In the beginning there was nothing. Then there was something: the Big Bang, forging gas -- hydrogen, helium, lithium. At some point in the chaos that followed, stars formed from this primordial soup. Pristine gases spun into stellar bodies. A generation of stars (whose existence is theoretical) created light in the darkness.These Population III stars, as they are known, are theoretically the turning point for the universe: taking the gases and turning them into the heavier elements: carbon, oxygen, iron, nitrogen and metallic elements.
But though there had to have been a first generation of stars, we've never actually seen them. This is because, massive, hundreds of times larger than the sun, they burned huge, hot, bright -- and fast. Scientists believe that these stars from the dawn of time burned out after just two million years.We haven't seen them -- but we may now have seen the very first evidence of their existence.
A team of researchers led by David Sobral from the Institute of Astrophysics and Space Sciences in Portugal has found what it believes to be good evidence for clusters of Population III stars in a galaxy located some 13.02 billion years away -- 800 million years after the Big Bang.Using the European Southern Observatory's Very Large Telescope, with  help from the Subaru Telescope and the Hubble Space Telescope, the team found a number of very bright, very young galaxies as part of a wide survey of very distant galaxies (as opposed to the more common narrow survey of a smaller patch of sky).But it was a galaxy they named CR7 that caught their attention -- by far  the brightest galaxy ever seen at this stage of the universe, three  times brighter than the previous holder of that title, Himiko, which had been thought to be one of a kind.
As well as being exceptionally bright, CR7 contained strong ionised helium emission -- and, crucially, no sign of any heavier elements. Both the ionised helium and the lack of heavier elements are required for Population III stars."The discovery challenged our expectations from the start, as we didn't expect to find such a bright galaxy," Sobral said. "Then, by unveiling the nature of CR7 piece by piece, we understood that not only had we found by far the most luminous distant galaxy, but also started to realise that it had every single characteristic expected of Population III stars. Those stars were the ones that formed the first heavy atoms that ultimately allowed us to be here. It doesn't really get any more exciting than this."The survey found blue and red clusters of stars, indicating that Population III stars did not all form at once, but in waves -- and the team directly observed what it believes to be the last wave of Population III stars, alongside regular stars. This means that Population III stars may be easier to find than thought -- not tucked away in the farthest, dimmest galaxies, but close enough to be observable. 
</article>
</item>
<item>
<title>Google's Nest reveals upgrades, new camera for smart home</title>
<article>Google is making a bigger push to become the leader of smart home gadgetry with Nest. In this CNET Update report, learn about the new Nest Cam, along with Amazon's updated Kindle Paperwhite.
The Update report also catches you up on the latest in social media: Facebook launches the Moments app for sharing photos, Twitter auto-plays videos, and YouTube builds a Gaming hub.CNET Update delivers the tech news you need in under three minutes. Watch Bridget Carey every afternoon for a breakdown of the big stories, hot devices, new apps, and what's ahead. Subscribe to the podcast via the links below.Subscribe:iTunes (HD) | iTunes (SD) | iTunes (HQ) | iTunes (MP3)
RSS (HD) | RSS (SD) | RSS (HQ)| RSS (MP3)Download the audio version of today's episode:
</article>
</item>
<item>
<title>Mirror's Edge: Catalyst doesn't let you use guns, dev says at E3 2015</title>
<article>The new Mirror's Edge game, Mirror's Edge: Catalyst, doesn't allow players to use guns, a departure from the original game."In Mirror's Edge: Catalyst, you won't be using any guns at all," senior producer Sara Jansson told Polygon. "We've completely removed that aspect of the game. You can't even pick them up."
Combat in the first Mirror's Edge was perhaps its most maligned aspect; taking down enemies with guns, in particular, rather than fleeing seemed to conflict with what seemed like the most ideal way to play.This is welcome news to me, particularly after becoming concerned that combat would play a major role in the game following the release of a trailer at E3 this week. That may still prove to be the case, but the absence of firearms looks to be a positive move.Catalyst is due out in 2016 for PS4, Xbox One and PC. For more, check out our impressions from E3.
</article>
</item>
<item>
<title>Tony Hawk's Pro Skater 5 plays on nostalgia to great effect</title>
<article>There was a time when anyone could pick up a pad and chase high scores in a skating game. Then the Tony Hawk titles withered away, the Skate games came along with realistic physics and complex controls and I became sad.I have no ill will toward EA's Skate series, it was an impressively authentic realisation of skating and, as a person with multiple friends "about that skate life," I got a buzz off how much they enjoyed the series.But it was always bittersweet, as the thrills of a realistic skating experience in a video game were lost on me. I preferred the arcadey feel of the Tony Hawk series. In particular, the way it was so eager to positively reinforce my pathetic attempts at tricking with points -- however meagre they were.
Tony Hawk's Pro Skater 5 is a return to that classic skate-first-ask-questions-later gameplay, where the fingers take priority over the brain. I'll be the first to admit that I was not very good at the THPS games and, if the reactions from the developers watching me play the newest entry are any indication, I'm not very good at this one either. But I still had fun.There's an unmistakably retro feel to Tony Hawk's Pro Skater 5. It feels fast, loose and forgiving. Its developer, Robomodo, is taking every opportunity to include callbacks to previous titles. Our demo, for example, started off in the Bunker, an amalgamation of Warehouse and the Hangar.
From the moment you get your hands on the game, it feels familiar. Reverts, manuals and most of all the tricks of the Tony Hawk trade are still there, and gameplay is still all about racking up points and rubbing it in your friend's face when you break his or her record.However, Pro Skater 5 makes a few new tweaks. First among them is pushing, which lets players pick up speed by using the right trigger button and slow down using the left trigger. It's a great way to reduce downtime after eating floor during a failed trick, and lets you fine-tune your approaches.Another tweak is the special meter, which is now activated with a button press instead of just automatically kicking in when it fills up. When the bar is filled, your board glows blue, and you can pick exactly when you want to put it into effect, avoiding annoying situations where it pops while you're doing a transition.The third big change relates to grinds. Robomodo has introduced a new mechanic called Slam, which lets players shift weight and bring the board down immediately, which is handy for rescuing any overshot jumps and controlling when you want to hit a rail.It feels like there's a good mix of familiar and friendly old mechanics, and smart new tweaks. They help in making it more accessible and more responsive to play, but without overcomplicating a game that thrives on simplicity.Our demo also took us to a level known as School 3, a new version of the classic School. This time around, it will be playable in multiplayer sessions running on dedicated servers, so in the words of its developer, the park is going to be there "long before you get there, and long after you leave."
Levels in Tony Hawk's Pro Skater 5 can be played with up to 20 other people in a freeskate mode where everyone just hangs out and tricks together. Once you've gathered friends, you can queue up to do missions together, challenge everyone in the session to a high-score challenge chases.Pro Skater 5's online mode is designed to serve as a player lobby, without the pressure of having to find something to do, and the inconvenience of sitting through menus to do it. It should feel like a real skate park, where friends can compete, teach or learn, and generally just chill out with each other.Finally, Robomodo has introduced the ability to create skate parks, with on-the-fly editing and remixing. As you character jumps into the air, you can slide into the creation mode and place a grind or a ramp in the perfect place. All of your creations can then be shared online for others to enjoy.There's a classic feel to Tony Hawk's Pro Skater 5 that -- thanks in large part to the soundtrack -- ignited a feeling of nostalgia for the originals and then immediately satiated it. As as unskilled Tony Hawk player, I'm excited to see the series return, and glad to see that it's sticking so close to its roots.
</article>
</item>
<item>
<title>Hop aboard the International Space Station in Ultra HD</title>
<article>
When astronaut Terry Virts headed to the International Space Station in December 2014 as commander of Expedition 43, he brought with him a camera capable of capturing content in 4K resolution. 
Back on Earth as of June 11, Virts and NASA are starting to share some of the footage captured during the mission, which  included, among other things, astrophysics research and physical-science investigations. A 4-minute video posted to NASA's YouTube channel on Monday gives us our first real glimpse of the space station in glorious 4K resolution. In addition to getting a better, more clear look at some of the experiments astronauts conduct aboard the ISS, the footage features beautiful, jaw-dropping shots of the Earth and the moon as seen from space.Oh, and there's a floating taco, too, because of course there's a floating taco. What space video would be complete without one? Check out the incredible footage in the video above, then get ready for more beautiful shots of space as NASA continues to upload 4K video in its ongoing mission to boldly go where no man has gone before.And in case you missed it, the day before Virts returned to Earth after more than six months flying high above the planet, he tweeted a fantastically clear picture of the Egyptian pyramids from on high. 

</article>
</item>
<item>
<title>WebAssembly LLVM Backend Being Discussed</title>
<article>
</article>
</item>
<item>
<title>Dota 2 Reborn Powered By Source 2 Is Now Available!</title>
<article>
</article>
</item>
<item>
<title>It's Been Three Years Since Linus Torvalds' Huge NVIDIA Rant</title>
<article>
</article>
</item>
<item>
<title>Updated Plans For Adding SPIR-V Support To LLVM</title>
<article>
</article>
</item>
<item>
<title>Mir 0.14 Works To Further Reduce Lag</title>
<article>
</article>
</item>
<item>
<title>AMD A10-7870K Godavari: RadeonSI Gallium3D vs. Catalyst Linux Drivers</title>
<article>Last week I started posting AMD A10-7870K Linux benchmarks for this "Godavari" APU that's effectively a Kaveri Refresh and slightly faster for its four CPU cores and Radeon R7 Graphics over the former high-end Kaveri, the A10-7850K. In today's articles are some benchmarks of the Radeon R7 Graphics on the A10-7870K when running Ubuntu and testing the open-source RadeonSI Gallium3D driver against Catalyst on Linux.
The same A10-7870K configuration as last week was used with the ASUS A88XM-E motherboard, 2 x 8GB of DDR3-2400MHz memory, and 250GB Samsung 850 SSD. Ubuntu 15.04 x86_64 was running on the system while testing its stock Radeon driver stack (Linux 3.19 + xf86-video-ati 7.5.0 + Mesa 10.5.2), then testing an upgraded stack with the Linux 4.1 kernel and Mesa 10.7-devel via the Oibaf PPA, and then testing the Catalyst Linux driver packaged for Ubuntu Vivid via fglrx-updates (fglrx 15.20.2 / OpenGL 4.4.13374
As a reminder, the A10-7870K Godavari quad-core has a 3.9GHz base frequency with 4.1GHz turbo frequency and boasts Radeon R7 Graphics running at 866MHz while the rest of the features/specifications are identical to the A10-7850K Kaveri. This 95 Watt TDP APU currently retails for $140~150 USD.
A variety of OpenGL Linux workloads that run fine on both driver stacks were tested on the same hardware. All of the benchmarks were conducted via the open-source Phoronix Test Suite benchmarking software.
</article>
</item>
<item>
<title>AMD Will Be Working On Open-Source Fiji GPU Support In The AMDGPU Linux Driver</title>
<article>
</article>
</item>
<item>
<title>OpenGL 4.3's Framebuffer-No-Attachments Added To Mesa</title>
<article>
</article>
</item>
<item>
<title>Linux Mint 17.2 Is Near With Cinnamon/Mate RCs</title>
<article>
</article>
</item>
<item>
<title>Debian Launches A Diversity Sponsorship Travel Program</title>
<article>
</article>
</item>
<item>
<title>SUSE Continues Working On AMD HSA Support In GCC</title>
<article>
</article>
</item>
<item>
<title>Ubuntu Looks To Fill Vacated Developer Membership Board Seat</title>
<article>
</article>
</item>
<item>
<title>Marek Posts Mesa Tessellation Support For RadeonSI Gallium3D</title>
<article>
</article>
</item>
<item>
<title>X.Org Server 1.17.2 Released</title>
<article>
</article>
</item>
<item>
<title>Don't Bet On &quot;X12&quot; Succeeding X11 Rather Than Wayland (Or Mir)</title>
<article>
</article>
</item>
<item>
<title>NVIDIA Performance Counters Headed To Linux 4.2</title>
<article>
</article>
</item>
<item>
<title>Next AMD Catalyst Linux Update Appears To Have OpenGL 4.5</title>
<article>
</article>
</item>
<item>
<title>AMD Announces The Rx 300 Series, Fiji-Based Fury X, R9 Nano, Project Quantum</title>
<article>
</article>
</item>
<item>
<title>OMAP DRM Gaining Atomic Mode-Setting For Linux 4.2</title>
<article>
</article>
</item>
<item>
<title>OpenSUSE Tumbleweed Switching Over To GCC 5.1.1</title>
<article>
</article>
</item>
<item>
<title>Wine-Staging 1.7.45 Gets Better vRAM Detection, Makes Use Of Ping</title>
<article>
</article>
</item>
<item>
<title>OpenELEC 6.0 Beta 2 Packs In Linux 4.0, Mesa 10.6, Kodi 15.0 Beta 2</title>
<article>
</article>
</item>
<item>
<title>The Flopped Ouya Console Gets Acquired By Razer</title>
<article>
</article>
</item>
<item>
<title>Tweaking Your Fedora Installation For Maximum Productivity &amp; Features</title>
<article>No Linux distribution is absolutely perfect for any and all use cases. Some use older software than the user would prefer, some lack the polish that comes with a distribution integrating all the pieces together, some projects might be heading down a direction that the user disagrees with. Many users end up finding themselves in the arms of the Fedora Project or its cousin the CentOS Project, as it provides the tri-fecta of up-to-date software, distribution level integration, and tweak-ability that Linux users so often enjoy.
The Fedora Project has always had a stance, both for legal and philosophical reasons, to not ship or make available by default non-free software such as Google Chrome, Adobe Flash, Oracle Java, DVD and MP3 playback, etc. While this stance is arguably a noble one and one to be commended, it does tend to run contrary to what a user might consider a &quot;working desktop.&quot; 
There are two main ways to achieve all of the software necessary for a daily-usage installation: automatically, or manually. 
Fedy, formally called Fedora Utils, is a desktop application that tries to automate and make-easy the installation of common third party software, repositories, and utilities for the Fedora Desktop. As we covered last week it allows one to install MP3 support, Encrypted DVD Playback, Oracle Java, Adobe Flash, Android Studio, Google Chrome, tweaks such as better font rendering, and user-defined plugins for specific tasks.
Installation of Fedy requires running the command:
wget http://satya164.github.io/fedy/fedy-installer
Then going into the directory you downloaded it to and running:
Fedy will ask to download initial packages as part of the installation but eventually it will finish up and the you will have a &quot;Fedy&quot; entry in your Application Menu. 
By default Fedy opens to show the Applications tab, the user can click the &quot;Install&quot; button next to any application. After clicking &quot;install&quot; Fedy will prompt for your user password and then begin downloading it silently behind the scenes. After it has been installed the &quot;install&quot; changes to say &quot;Remove.&quot; 
The alternative tab for Fedy is the &quot;Tweaks&quot; tab that allows for things like setting the SSD scheduler, enabling tap-to-click, better font rendering, amongst other things. 
While Fedy is a bit more on the user-friendly side, minus the need to run terminal commands, it is not the perfect solution for everyone. Sometimes Fedy can bug out, or crash, also with the recent removal of its command-line interface it is no longer a viable scripting interface. Enter solution number 2: the manual way.
This section is not meant to cover every piece of software that fedy supports. This section simply pulls out some of the highlights and covers them. If you would like an article in regards to your personal favorite application let me know on the forums and I will see about posting an article dedicated to that application.
and select &quot;YUM for Linux (YUM)&quot; via the drop down menu. This will download a .RPM package called &quot;adobe-release.&quot; Depending on your environment you might be able to just double click the downloaded RPM package to install it. If that does not work you need to bring up your Terminal and do:
After the package has been successfully installed you can use either dnf or your graphical package manager to install the package &quot;flash-plugin&quot;
Select the appropriate package for your architecture (32 or 64bit) and let it download. You should get a file called &quot;google-chrome-stable.&quot; As per the above Adobe Flash instructions, you can attempt to install Google Chrome by double clicking the .rpm. If that should fail you need to run the following commands:
Either way it will prompt you to download additional packages. When the operation is complete you should have a Google Chrome entry in your Application Menu.
Where it states &quot;Select Your Distribution&quot; you want to pick Fedora 16 32bit. From there it will download the Skype RPM package. As with Chrome and Flash above, double click the RPM package and let it install any additional packages it requests. Afterwards load up Skype from your applications menu and it should 'just work.'
The easiest way to download both the free and non-free RPMFusion repositories in one go, as long as you are on Fedora 14 or higher is to execute the following command:
su -c 'yum install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm http://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm'
If you are terminal-averse or want to download the RPM packages for later use then links to actual .rpm's are available here.
With RPMFusion installed and enabled the possibilities of Fedora open up considerably.
sudo dnf install steam libtxc_dxtn.i686 ibtxc_dxtn.x86_64
Please note that the version of Steam in RPMFusion is the base upstream release from Valve. If you are a heavy user of a gamepad, such as the Xbox 360 / Xbox One controller, then there is an alternative repository available that has a patched version of the Xpad driver. This alternative version contain patches from Valve to improve the driver and its behavior. This repository also contain the SteamOS session files and binaries for running a Steam-only system.
The alternatively repository is available to be enabled via
sudo dnf config-manager --add-repo=http://negativo17.org/repos/fedora-steam.repo
And then installed via
sudo dnf install steam libtxc_dxtn.i686 ibtxc_dxtn.x86_64
</article>
</item>
<item>
<title>LinkedIn says private bug bounty program works for it better</title>
<article>LinkedIn plans to continue closely vetting researchers for its bug bounty rewards program, saying it reduces the number of distracting erroneous and irrelevant reports.The decision to keep its program private “gives our strong internal application security team the ability to focus on securing the next generation of LinkedIn’s products while interacting with a small, qualified community of external researchers,” wrote Cory Scott, LinkedIn’s director of information security, in a blog post.Security researchers with vetted backgrounds are invited to participate, which allow them to have the same experience as if they were on LinkedIn’s internal security team, Scott wrote.Many large technology companies such as Google, Yahoo and Facebook have public bug bounty programs, which pay rewards for valid security vulnerabilities usually based on their severity.Scott wrote that the vast majority of bugs reported to LinkedIn from the general public “were not actionable or meaningful.” The private program, launched last October, “has a signal-to-noise ratio of 7:3, which significantly exceeds the public ratios of popular public bug bounty programs.”The professional networking site will still review bugs submitted through its catch-all security email address, security@linkedin.com, he wrote.LinkedIn uses HackerOne for its bug program, which is a company that has a secure platform that manages security vulnerability information and handles disclosure information and rewards. LinkedIn has paid out US$65,000 since the program launched, Scott wrote.
Jeremy reports on security and regional news for the IDG News Service.More by Jeremy Kirk
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Oracle profit slides 24 percent as customers move to the cloud</title>
<article>Oracle has reported a sharp drop in profit for the quarter just ended, with customers spending more on its cloud services but less on software that runs in their own data centers.Chairman Larry Ellison portrayed the shift as a positive one and said Oracle can make more money selling cloud services over the long term. But the change didn’t seem to help it much last quarter, when its results were also battered by the strong U.S. dollar.Oracle’s revenue and profit for the quarter, the fourth of its fiscal year, missed the forecasts of financial analysts, and its stock fell almost 7 percent after the results were announced Wednesday.Oracle’s cloud business seems to be growing strongly. Revenue from software sold as a service climbed 29 percent from this time last year, to $416 million, the company said. But that’s a relatively small part of Oracle’s business, and it wasn’t enough to offset mediocre performance elsewhere.Sales of new software licenses, which account for roughly a third of Oracle’s overall revenue, declined 17 percent from this time last year, and the fourth quarter is usually the strongest of its fiscal year, because sales teams are pushing hard to meet their quotas.Revenue from license updates and support fared better but was still flat from last year because of the currency effects. A strong dollar can hurt the revenue of U.S.-based multinationals, because sales appear smaller when they’re converted back from foreign currencies.On a call to discuss the results, Ellison said cloud services are a good business for Oracle to be in, but that it takes time to get the payoff.For example, he said, if a customer buys $1 million in new software licenses, Oracle gets $1 million from the sale immediately, then collects $3 million in maintenance and support fees over the next 10 years.With cloud services, if a customer signs up for a $1 million subscription, Oracle gets no money up front, but it collects $1 million a year for the next 10 years. “It’s a much better business for us,” Ellison said.That’s the theory, at least, though it depends on the customer continuing their contract for the full 10 years.And the cloud sale has a short-term hit on profitability. That’s because, although Oracle doesn’t book the revenue immediately, it does still have to pay commission fees and other costs.“This shift [to cloud] has the effect of lowering near-term [earnings per share], but over time will increase it significantly,” said CEO Safra Catz.Cloud services are a highly profitable business, according to Ellison. “This is gonna shock you,” he said, but the profit margins on cloud software sales are about the same as those for selling software licenses and support. “It’s stunningly profitable,” he said.He also went to great pains to explain that Oracle is growing faster in cloud applications than rivals Salesforce.com and Workday—although that might not be surprising, since for Oracle it’s a newer business. “Clearly, our cloud business has entered hyper-growth,” Ellison said.Oracle’s total revenue for the quarter, which ended May 31, was $10.7 billion, the company said. That was down 5 percent from a year earlier and below the $10.9 billion that financial analysts had expected, according to Thomson Reuters.Net income was $2.8 billion, down 24 percent from a year earlier. Excluding certain one-time items, earnings per share was $0.78, well short of the analyst forecast of $0.86.Oracle’s business has shown signs of slowing—its sales last quarter were flat from the year before—but this is the first quarter in more than two years that its overall sales have dropped.That was largely due to the strengthening dollar, which can make products from U.S. companies more expensive to buy overseas. Oracle said its total revenue would have increased 3 percent without the currency impact.But even allowing for the currency effect, sales of new software licenses were down 10 percent. Revenue from license updates and support fees climbed 8 percent on that basis, and total hardware revenue climbed 5 percent.
James Niccolai covers data centers and general technology news for the IDG News Service, and is based in San Francisco.More by James Niccolai
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>US visa system won&#039;t be back online until at least next week</title>
<article>A computer problem that has brought down a key State Department system for issuing visas and passports at U.S. embassies worldwide won’t be fixed until at least next week.The glitch that has hit the Consular Consolidated Database has affected applications filed since May 26, and while the government hasn’t provided an estimate of the number of people affected, a weeklong outage of the system last year affected 200,000 applications.An unidentified hardware failure is preventing the State Department from processing and transmitting the security-related biometric data checks at embassies and consulates. Because this data is mandatory in processing applications, the failure has halted the entire system.More than 100 engineers from the government and the private sector are working around the clock on the problem, said John Kirby , State Department spokesman, at a briefing on Wednesday.“For all the hard work, we don’t expect that the system will be online before next week,” he said.The failure comes as the popular summer travel season begins in the U.S. and as students planning to study in the country starting in the fall begin making preparations for travel.“We regret this inconvenience to travelers, recognize that this is causing hardship for those that are waiting for visas, and in some cases their family members or employers in the United States,” said Kirby.But he added that as this is related to security of the country, it can’t be bypassed.The State Department repeated an earlier assertion that the glitch is not related to a cyber security issue and is a different problem from that which it suffered in July 2014.It is advising travellers to continue checking for updates on its travel.state.gov website.
Martyn Williams covers mobile telecoms, security, Silicon Valley, and general technology breaking news for the IDG News Service, and is based in San Francisco.More by Martyn Williams
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>AT&amp;T, WhatsApp get low marks from EFF for data disclosure policies</title>
<article>The Electronic Frontier Foundation released the latest version of its annual “Who Has Your Back” report on tech companies’ data disclosure policies Wednesday afternoon, giving perfect five-star ratings to companies including Apple, Adobe, Dropbox and Yahoo.This year’s publication is the fifth edition of the EFF’s reporting on tech companies’ policies around disclosing information to governments in response to data requests, and it brings major changes to the organization’s framework.“The criteria we used to judge companies in 2011 were ambitious for the time, but theyve been almost universally adopted in the years since then,” the EFF said in its report.Most of the criteria from the EFF’s past reports have been rolled into a single framework for “Industry-accepted best practices,” which have been adopted by all but one of the companies surveyed. The organization also judged companies on their willingness to inform users of government requests for their data, except when required by law or in emergency situations.Under the new criteria, in order to earn a star for informing users about requests, a company now has to commit to telling affected users when a gag order about the request has been lifted or the emergency has passed.In addition, each company now is judged on whether it discloses its policies for retaining data (such as what happens to a user’s files after they are deleted), whether it discloses content removal requests, and whether the companies have advocated against the putting backdoors into encryption.WhatsApp and AT&amp;T scored lowest of all the companies in the report, each receiving just one star. The Facebook-owned messaging app was given a year to prepare for its first inclusion in the report, but it was the only company on the list that hadn’t adopted the EFF’s list of best practices, such as publicly requiring a warrant and publishing a transparency report.AT&amp;T hasn’t changed much since its appearance on last year’s report: While the company now publicly requires a warrant before disclosing data, AT&amp;T still does not promise to inform users of data requests.Twitter and Google both scored lower this year than last, because while both companies pledge to tell users about requests for their data, neither guarantees that it will tell them about a request after a gag order lifts or emergency conditions make it untenable to disclose anything. Twitter’s policies say it may inform users after such a disclosure becomes possible, but the company won’t guarantee that it will do so.Microsoft missed two stars this year (compared to a perfect score last year) because it doesn’t publicly disclose its policies on data retention and hasn’t yet published a report on government content removal requests. The latter will be fixed later this year, though: the company told the EFF that it plans to disclose content removal requests by September. (It’s not clear whether Microsoft plans to release a data retention policy.)Interestingly, Tumblr’s rating diverged from Yahoo’s perfect score, because the social network doesn’t follow its parent company’s example of revealing its data retention policies and disclosing requests from governments to remove content. The company did not respond to an inquiry about whether it plans to change its policies.Overall, the EFF said it was “pleased to see major tech companies competing on privacy and user rights.” The advocacy group says it believes the adoption of policies it calls for in the scorecard is part of a broader shift by tech firms toward pushing back against government data requests.
Blair Hanley Frank is primarily focused on Microsoft and its competitors in the desktop OS, office suite and browser business for the IDG News Service, and is based in San Francisco.More by Blair Hanley Frank
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Microsoft reorg solidifies role of Surface, HoloLens and other devices to sell Windows</title>
<article>Microsoft reorganized with a bang on Wednesday. It ousted big-name executives Stephen Elop and Mark Penn. It gave Windows leader Terry Myerson the Devices group that Elop once led. Driving these and other dramatic changes are a fundamental corporate shift that wasn’t explicitly stated: At the modern Microsoft, hardware is now a very big deal.
The reorganization encompasses most aspects of Microsoft’s business, tossing out some of Microsoft’s most noteworthy executives in the process. Stephen Elop, who led Nokia and oversaw its integration with Microsoft, is out. Ditto for Mark Penn, the controversial political operative who was instrumental in Microsoft’s “Scroogled” campaign.
Satya Nadella and Stephen Elop, in happier times.
Instead, Microsoft now sees its “mobile first, cloud first” vision from chief executive Satya Nadella organized around three initiatives: to reinvent productivity and business processes, to build the intelligent cloud platform, and create more personal computing.
Instead of different business units governing hardware and Windows, Microsoft now has placed them under one roof: Terry Myerson will oversee the Windows and Devices Group, combining the Operating Systems Group and the Microsoft Devices Group that Elop formerly helmed.
“WDG will drive Windows as a service across devices of all types and build all of our Microsoft devices including Surface, HoloLens, Lumia, Surface Hub, Band and Xbox,” Nadella said in an email to employees. “This enables us to create new categories while generating enthusiasm and demand for Windows broadly.”
Windows as a service is an evolution that Microsoft has talked about for some time, although it still hasn’t clarified its vision: whether Microsoft will ask future Windows PC owners to pay on an annual, rather than a one-time basis, or simply continue to add security updates and new features over time, as it’s emphasized over the past few months. And no, that aspect will be critical to how you engage with Microsoft and its products in the future.
But Microsoft hasn’t said what it plans to do in that regard. What it is saying, today, is that hardware matters. A lot.
Microsoft’s Surface Pro 3 and Surface 3 launched Microsoft into the productivity hardware business.
Scott Guthrie will lead the Cloud and Enterprise team, focused on Microsoft’s server-side offerings, while Qi Lu will continue to oversee the Applications and Services Group that encompasses Office and related productivity products. But, chances are, it’s the new WDG group that you’ll be thinking of when you think of Microsoft.
Historically, of course, Microsoft developed Windows, and you bought a PC to run Windows on. Under the new vision, Microsoft not only wants you to buy a PC to run Windows on, but also a Surface tablet, a Microsoft-powered smartphone, and a Surface Hub to connect it to while at the office. Oh, and a Band, too, to remind you of your upcoming meetings. No, there’s no reason to believe that Microsoft will stop promoting its products on other ecosystems. But the Microsoft’s hardware business now enjoys a pride of place that it never did before.
The face of Windows: Terry Myerson.
“This is a recognition of the new market reality that the OS is part of the device and the device experience depends on the OS,” said Steve Kleynhans, an analyst with Gartner, in an email. “Users no longer distinguish between the device and its operating system so there needs to be close alignment between the hardware and the OS.”
In other words, Windows isn’t just synonymous with Microsoft applications like Office, but with Microsoft’s hardware as well.
This shouldn’t come as a great surprise. Both of the first two Surface Pro generations had a sense of the prototype about them, as Microsoft took lessons it had learned in the game console business and applied them to the general productivity market. But the Surface Pro 3 launch was Microsoft’s debutante ball, where it confidently strode into the hardware space.
About the only weak point in Microsoft’s Windows ecosystem is the Microsoft Band, which doesn’t run Windows. Is a Windows 10 powered Band on the horizon?
Nadella said then that Microsoft’s goal was not to compete with its OEMs, but to “create new categories and spark new demand for our entire ecosystem”. According to Kleynhans, “Microsoft’s hardware division is about showcasing the best user experience using Microsoft technology, so combining these two makes sense to ensure the two groups leverage each other as the OS and devices evolve.”
Some feel, however, that the reorganization elevates Windows again, as Directions on Microsoft’s Wes Miller does. “To me it is the sign that hardware and software are at best peers, but also that the software and experiences will likely drive the hardware—and ensure a stronger relationship with OEM partners again,” Miller said in an email.
I tend to believe that Microsoft didn’t set out to create the best user experience for Windows, but the best Windows devices, period, setting the pace for its hardware partners to follow. If they do, great. If not, the industry will move forward without them. It’s a bold, ambitious vision, and one that’s been absent from the computer industry. Hololens, the Surface Hub, and the Xbox One together move the industry forward. Can one man, Myerson, oversee all that? As Kleynhans notes, it’s a tremendous vote of confidence in Myerson, in Windows 10, and in Myerson’s vision for it all.
But here’s the bottom line:  yes, you’ll still be able to choose whatever PC you want to run Windows 10.  But now, more than ever, Microsoft cares whose name is on the box.
Updated at 3:50 PM with comments from Directions' Miller.
As PCWorld's senior staff writer, Mark focuses on Microsoft news and chip technology, among other beats. He has formerly written for PCMag, BYTE, Slashdot, eWEEK, and ReadWrite.More by Mark Hachman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Fitbit IPO shows wearable devices have a healthy pulse</title>
<article>Wearable devices that track steps and monitor sleep may have been dismissed as fads, but Fitbit’s upcoming initial public offering paints a different picture.Wearables like Fitbit’s health and fitness trackers may not be as ubiquitous as smartphones, but they are becoming must-have devices for millions of people.Moreover, their technology has room to mature so that future models of these devices can offer new wellness benefits, and thus retain existing users and attract new ones, said Ramon Llamas, an IDC analyst.Fitbit can please investors with its global distribution network and varied portfolio of products, which meets the needs of entry-level buyers as well as high-end consumers, Llamas said. But beyond investors, people in general are really excited about what new “experiences” Fitbit’s devices and, by extension, wearables can provide.This helps explain Fitbit’s decision this week to raise its IPO price range to US$17 to $19 a share and to increase the number of shares it will offer to 34.5 million. Fitbit initially said it would issue 29.9 million shares priced between $14 and $16.These new figures give Fitbit a valuation of up to $3.9 billion approximately, and mean it could raise as much as $655 million when it goes public this week. The San Francisco company is expected to set the price for its IPO today and begin trading Thursday on the New York Stock Exchange.Fitbit is unique among wearable device makers since “it has laid out a product road map of how it got here” and other vendors may not be as prepared, said Llamas. Given the amount of money Fitbit is expected to raise, Llamas called its IPO “significant” and shows that wearables “have staying power.”Fitbit’s IPO comes as more companies are entering the wearables market, including most notably Apple, whose smartwatch includes health tracking features. Google’s Android Wear platform runs wearables that can be used for monitoring fitness activities.But smartwatches that, in addition to health and fitness functions, also offer communication features, like the ability to receive email and text message notifications, aren’t a threat to Fitbit, said Angela McIntyre, a Gartner analyst.“There’s an advantage to having a device that’s oriented more toward fitness tracking,” she said.Compared to smartwatches, fitness trackers typically offer longer battery life and simpler user interfaces, she said.In Fitbit’s case, its devices are generally more affordable than smartwatches since they have fewer features and therefore less components, McIntyre said. For example, an entry-level Apple Watch costs $349 compared to $99.95 for Fitbit’s two entry-level devices, the One and the Flex, and to Fitbit’s most expensive product, the Surge at $249.95.In addition to appealing to consumers, lower prices can also attract businesses interested in getting Fitbit devices for their employees as part of corporate wellness programs, said McIntyre. Companies may find that giving workers a Fitbit to encourage healthy living leads to lower health care costs, she said.While McIntyre couldn’t provide figures on how many Fitbit sales are made to businesses, she pointed out it has a program for companies interested in using the devices for wellness programs. Fitbit didn’t respond to a request for comment.Fitbit’s main competitors are other fitness trackers vendors including Garmin and Jawbone, which recently filed two patent-infringement lawsuits against Fitbit, as well as Asian consumer electronics companies like HTC and Xiaomi, according to Llamas.Xiaomi was the world’s second largest wearable vendor in the first quarter of 2015 based on device shipments with Fitbit ranking first, according to IDC.To fend off rivals, Fitbit needs to use the data its devices collect to deliver health notifications that are relevant to users, Llamas said.“Fitbit has done a great job with individual data like calories burned, but what should it tell me to have a better life?” he said.For example, a Fitbit could suggest going to bed early on a Monday if a user didn’t get much sleep over the weekend, or it could predict the onset of an illness with the addition of a sensor to measure body temperature, Llamas said.“These are the forward-thinking applications that Fitbit could grow into,” he said, adding that Fitbit doesn’t need to develop a smartwatch with functions that go beyond health and fitness tracking.While developing services to provide users with wellness insights is key to keeping Fitbit competitive, smaller changes, like adding sensors and improving battery life, can also go far in making its products stand out, McIntyre said.Entering partnerships with health care providers and insurers is another way for Fitbit to grow and protect its market share, she said.
Fred O'Connor writes about wearables, Apple, IT careers, health IT and general technology news for the IDG News Service.More by Fred O'Connor
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Intel continues wearables buying spree with smart-goggles maker Recon</title>
<article>Intel’s purchase of Recon Instruments, a smart eyewear company, adds to the tech company’s small pile of investments in the fledgling wearables market. The deal (for an undisclosed amount) was announced Thursday and follows the purchase of smartwatch maker Basis last year; the inking of a long-term deal with Luxottica for smart eyewear, including Oakley; and finally, teaming up with MICA and Opening Ceremony for smart bracelets.
Intel plans to integrate the company into its New Devices Group, whose task is to help Intel push into a new family of smart device platforms. Intel said it would continue selling the Recon goggles, as well as continue designing new products.
Recon’s Snow2 smart goggles.
Some of Recon’s knowledge will be used elsewhere, too, as Intel works hard to keep up with the evolution from the PC through phones and tablets into the Internet of Things. “The growth of wearable technology is creating a new playing field for innovation, and we’ve made tremendous strides in developing products and technologies to capture this next wave of computing,” said Josh Walden, a senior vice president and general manager in charge of Intel’s New Technology Group. 
Dan Eisenhardt, the co-founder of Recon, said that being brought under Intel’s wing will give them the funding to establish a developer relations business. More importantly, however, it will give the wearables maker access to Intel’s semiconductor design and manufacturing expertise. At the Consumer Electronics Show in January, Intel announced the Intel Curie embedded processor, designed to power a new generation of embedded devices. 
Intel's strategy has yet to pay off with the dividends enjoyed by Intel’s PC or server processor business. With Recon, however, it has the option either to partner with smartglass makers—or create its own. 
Why this matters: Intel’s commitment to this brave new world of wearables is a little surprising. But if wearables had taken off even a decade ago, they would have been powered by the likes of Texas Instruments, Motorola, Zilog, or any number of other embedded chip makers. Now, Qualcomm is Intel’s chief rival in this space. By getting in on the ground floor, Intel can get its name out in the wearables space and help drive the first few generations of wearable designs.
As PCWorld's senior staff writer, Mark focuses on Microsoft news and chip technology, among other beats. He has formerly written for PCMag, BYTE, Slashdot, eWEEK, and ReadWrite.More by Mark Hachman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>News site held liable for hateful comments, prompting free-speech concerns</title>
<article>A court ruling that holds an Estonian news portal liable for hate speech in comments on its website has triggered fears for the future of online news startups.
The news portal had argued, unsuccessfully, that requiring it to censor or moderate reader comments violated rights to freedom of expression guaranteed by Article 10 of the European Convention on Human Rights.
The European Court of Human Rights, in a ruling Tuesday, found no such violation. But two judges who issued a dissenting opinion warned that making the portal monitor each and every comment before publication could result in “a disproportionate interference” with the news portal’s freedom of expression.
The ruling is significant for media organizations and could affect how news sites deal with reader comments. It pits laws on freedom of expression against the European Union’s E-Commerce Directive, which is usually seen as providing a way for websites to avoid liability for user-generated content, as long as they agree to respond promptly to requests to remove illegal material.
The Computer and Communications Industry Association, whose members include Facebook, Google and Yahoo, warned that the judgment confused the legal protections intermediaries can rely on, limiting use of user content and discouraging investment in new online services.
The court emphasized that its judgment should apply only to professional news sites that invite comments on their stories, and not to other online forums such as discussion boards, bulletin boards and social media sites.
The Estonian case began in 2006, when news portal Delfi published an article about a ferry company’s decision to change its route to certain islands, breaking ice where it would have been possible for ice roads to open. This delayed for several weeks the opening of the ice roads, which provide cheaper, faster connections to the islands than the ferry service.
In the comment section under the article, readers responded with offensive and threatening posts about the ferry operator and its owner, the court said. About six weeks after publication, the ferry company’s lawyers requested that the comments be removed, and Delfi promptly complied, refusing a request for 500,000 Estonian kroons (then about $38,000) in damages.
The ferry company took its complaint to court, where a judge initially found that, under Estonia’s implementation of the E-Commerce Directive, Delfi had no liability for comments on its site.
The ferry company appealed, and was awarded 5,000 kroons in damages as the case traveled up through the Estonian court system until in 2009 the Estonian Supreme court held Delfi liable, rejecting its argument that it was neutral and merely a technical service provider. The portal had not only failed to prevent publication of the comments but also failed to remove the comments on its own initiative, the court found.
Having failed to make its argument that it was a mere conduit deserving protection under the E-Commerce Directive, Delfi changed tack and took its case not to the Court of Justice of the European Union, but to the European Court of Human Rights, which accepts cases from 47 signatory states of the European Convention on Human Rights. There it argued that the judgment of the Estonian courts violates the convention’s Article 10, which covers freedom of expression.
The court initially rejected Delfi’s arguments in 2013, but the company again appealed. In Tuesday’s ruling the court’s Grand Chamber upheld the earlier decision against Delfi.
Delfi’s editor in chief, Urmo Soonvald, said in an article on Delfi’s portal that freedom of speech in Europe has been badly hit.
While the judges attempted to finesse a distinction between requiring that the portal only take down manifestly illegal content of its own initiative, and requiring it to review user generated content prior to publication, Lorna Woods, a professor of Internet law at the University of Essex in the U.K., was dismissive of this argument. “Both effectively require monitoring (or an uncanny ability to predict when hate speech will be posted),” she wrote on the blog of the London School of Economics Media Policy Project.
“Both approaches implicitly reject notice and take down systems, which are used—possibly as a result of the E-Commerce Directive framework—by many sites in Europe,” she wrote.
Journalist and media law consultant David Banks played down the ruling’s impact on news sites, in the U.K. at least: “I think here in the U.K. the effect will be minimal. News sites here, because of the way our libel laws operate, are used to liability occurring if they fail to remove content where they have been notified of a problem.”
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>SAS automates data modeling for fast analysis</title>
<article>SAS wants to supercharge your business analysis, through new software that automatically builds multiple models of data and picks those that best predict future events.“If you take too long coming up with a model, you lose a lot of value,” said Sascha Schubert, SAS technology marketing director, speaking of today’s competitive environment. “You want to be efficient in your analytics.”With a customer base of over 75,000 organizations, SAS has long been known for its advanced statistical analysis software. With this new offering, called SAS Factory Miner, the company is trying to go one step further to help its customers, by generating models for their data.Traditionally, business analysts construct data models by hand, choosing from a data table, or multiple tables, the variables to examine closely, and trying to understand how they work together to produce a desired outcome.With so much data today, however, it can be difficult to pinpoint the specific factors that are key indicators. Were sales last weekend good because of some discount pricing, or because people had more time to shop during the weekend, or some other, hidden factor?As its name indicates, SAS Factory Miner automates this process of building and testing models, which could lead to better models that are generated more quickly than what could have been done by hand. It could also help alleviate the need for an organization to hire more data scientists, who are much in demand.SAS Factory Miner can use any source of data, as long as the data itself can be formatted into a table. The software, run from a server and accessed with a browser, offers a graphical point-and-click interface. It comes with a set of customizable templates for creating baseline models. Analysts can fine tune or revise any of the computer-generated models.To help pick the best models, the software uses a number of machine learning algorithms that, through repeated testing of the models, can recognize patterns to anticipate future performance. One unnamed customer used an early version of the software to build 35,000 different models in order to find the best approach for a marketing campaign.This approach also helps enable what Schubert called stratified modeling, in which large sets of data, such as sales, can be divided into smaller segments. Stratified modeling can offer more accurate results, though its use has been limited by the time it takes to build the models, Schubert said.A financial institution could build different models for different sets of potential users, based on spending ability, buying behavior, or other factors. These models could then be used to generate more appealing credit card offers.The technology can be used in multiple ways by business, especially in the field of marketing, Schubert said. It could be key to reducing customer churn, predicting future demand, personalizing offers, and managing risk.For instance, a manufacturer could use the software to build more accurate predictions for when equipment will fail. A firm in a fiercely competitive field could build models for each of its competitors, and then run them in unison to get a full view of the industry.SAS Factory Miner will be generally available around the middle of July. The company did not provide pricing, noting the price varies by the size of the installation, and the work it will do.
Joab Jackson covers enterprise software and general technology breaking news for the IDG News Service, and is based in New York.More by Joab Jackson
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Nest Labs revamps its entire product line: Goodbye Dropcam, hello Nest Cam </title>
<article>Nest Labs launched two products today: its first security camera, a slimmed-down version of the popular Dropcam Pro called the Nest Cam, and a second-generation Nest Protect smoke and carbon-monoxide detector. The company also announced a software update for its Nest Learning Thermostat that adds at least one new feature: notifications when temperatures in the home drop low enough to freeze pipes. And for the first time, consumers who have all three devices in their homes will be able to access each device from a single app.
Like the  highly rated Dropcam Pro  it replaces (Nest acquired Dropcam in a $555 million deal in 2014), the Nest Cam records video at 30 frames per second in 1080p resolution, and it delivers the same wide-angle 130-degree field of view. Sound and motion sensors built into the camera will trigger the camera to record clips that are uploaded to the cloud. According to a Nest press release, the Nest Cam provides better night vision than the original thanks to new infrared LEDs and algorithms that can discern the difference between a flashlight and sunlight, so the camera won’t become confused and switch to day mode when a bright light passes in front of the camera.
The Nest Cam is essentially a slimmed-down version of the Dropcam Pro. 
If you have a Nest Protect and a Nest Cam in the same room, a camera button will appear on your phone if you receive a smoke or CO alert. Touch the button and you’ll be able to see what’s going on in your home. The camera will also record a clip of what triggered the alert, even if you don’t subscribe to the new Nest Aware cloud service. This feature will also be available with the older Dropcam and Dropcam Pro cameras.
A new app will unify control of all three of Nest Labs’ products. 
Buyers who do sign up for Nest Aware (subscriptions cost either $10 per month for 10 days of storage, or $30 per month for 30 days) will also benefit from more powerful cloud-based motion analysis; they’ll be able to create activity zones within the camera’s field of view, so that alerts are generated only when motion is detected in that area (such as a door); and they’ll be able to edit, save, and share up to three hours of video clips.
The new Nest Protect is significantly smaller and features a new sensor that Nest says does a better job of differentiating between dangerous levels of smoke and the harmless smoke that results from, say, overzealous cooking in the kitchen or that wafts in from the barbecue on the patio. And when the detector does report a false alarm, you’ll be able to silence it with an app on your smartphone or tablet (the first-generation Nest Protect was recalled because its “wave” feature could prevent the alarm from going off in the event of a real fire). The redesign also includes a new smoke chamber that’s designed to address another common complaint about the first-gen product: dust and insect incursions.
While it retains the same name as the original Nest Protect, the new model features a number of hardware improvements.
Experts say you check the status of your smoke alarm every month to make sure it’s in working order, but no one actually remembers to do so. The new Nest Protect will automatically check itself monthly to ensure it’s in working order, testing its speaker and horn and using its built-in microphone to ensure they went off.
You can also manually check the alarm using the app on your smartphone or tablet. For even more peace of mind, a new feature called Whole Home Nightly Promise will trigger every Nest Protect in your home to generate a green glow when you turn out the lights to report that everything is in working order, or a yellow glow if there’s something amiss with any of them. This feature is available in both first- and second-generation Nest Protects.
One feature notably absent from the Nest Cam is facial recognition. The ArcSoft Simplicam and more recently, the Netatmo Welcome home-security cameras can both detect human faces and identify who that person is. These cameras will send alerts when an unrecognized person passes in front of the camera, so you’re not bombarded with alarming messages about motion detected in your home.
Today’s announcements unify three elements of Nest Labs’ connected-home strategy: Security cameras, climate control, and fire protection. Like Apple with its HomeKit initiative, however, Nest still has a long way to go when it comes to offering a complete connected-home strategy, one that includes lighting controls, smart door locks, door/window sensors, and other home systems.
Nest won’t try to reinvent any of those wheels, relying instead on third-party products that use its Thread low-power network protocol. I’m also waiting to see if the company will do anything with the awesome Revolv smart hub that it acquired—and promptly killed—in 2014.
Michael is TechHive's lead editor and covers the connected-home and home-entertainment markets. He built his own smart home in 2007, which he uses as a real-world test lab when reviewing new products. Michael also reviews routers and networking products for TechHive and PCWorld.More by Michael Brown
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>FCC plans to fine AT&amp;T $100 million over &#039;unlimited&#039; data plans</title>
<article>The Federal Communications Commission plans to fine AT&amp;T $100 million for misleading customers by throttling speeds on the lines of millions of customers who had “unlimited” data plans.
The FCC alleges that AT&amp;T did not adequately disclose to its customers on 4G “unlimited” data plans that their speed would slow drastically after they had reached a monthly data allowance of 5GBs. The policy began in 2011.
Data speeds were significantly slowed to 512kbps from the advertised 5Mbps to 12Mbps under something AT&amp;T called its maximum bit rate policy. That very name is clearly at odds with an unlimited plan, a senior FCC official told reporters on a conference call.
By not clearly informing consumers of this policy, AT&amp;T was in violation of the Open Internet Transparency Rule of 2010, which requires Internet providers offer clear and accurate information to consumers so they can make informed choices on Internet service, the FCC said.
The fine is the largest proposed by the FCC in its history, but it’s not a done deal yet. AT&amp;T has 30 days to respond in writing to the FCC’s charges after, which the commission will adjudicate the complaint and determine a final fine.
The FCC said it’s aware that the fine, while large, is a fraction of the revenue AT&amp;T made from offering its unlimited plan to consumers. It is also considering other redress, including requiring AT&amp;T to individually inform customers that its disclosures were in violation of rules and to allow them out of applicable contracts with no penalty.
AT&amp;T said it plans to vigorously dispute the FCC’s assertions.
“We have been fully transparent with our customers, providing notice in multiple ways and going well beyond the FCC’s disclosure requirements,” it said in a statement.
An AT&amp;T spokesman pointed to previous FCC guidance on disclosure of network management policies that noted providers must, at a minimum, provide a “publicly available, easily accessible website” and that “broadband providers may be able to satisfy the transparency rule through a single disclosure.”
AT&amp;T had demonstrated this through notifications on billing statements, text messages sent before throttling, detailed information about the process on its Web site and in the language on its customer agreement, he said.
Nonetheless, the carrier recently changed the way it throttles data throughput for customers with 4G unlimited data plans. In the past it would automatically slow speed when the 5GB limit was reached, but in May is said it “may” slow speeds after 5GB in accordance with network management requirements.
Correction: AT&amp;Ts rate plan covers speeds ranging from 5 Mbps to 12 Mbps.
Martyn Williams covers mobile telecoms, security, Silicon Valley, and general technology breaking news for the IDG News Service, and is based in San Francisco.More by Martyn Williams
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Twitter acquires machine learning startup Whetlab</title>
<article>To boost its in-house machine learning efforts, Twitter has acquired Whetlab, a startup that makes it easier for companies to use machine learning tools.As part of the acquisition, announced Wednesday, Twitter will shut down Whetlab’s beta service on July 15, and will no longer accept sign-ups for the product. Current users will be able to export their data from Whetlabs’s website in either tab-separated format or JSON.It’s not exactly clear how Twitter plans to use Whetlab’s technology to enhance its existing machine learning plans. However, the startup’s tool seems useful for any company implementing machine learning techniques. The technology, which was developed by researchers at Harvard, Toronto and Sherbrooke universities, takes in information about the problem a user wants to solve with machine learning. It then gives the user a series of suggestions to help them optimize a machine learning model to solve the problem.Users don’t have to send Whetlab their data, nor do they need to use a special machine learning toolkit, according to the company’s product page. “You can run your code on your own private machines at whatever scale you want. What we do is help you use those resources optimally by telling you what you should try next,” the page reads.The acquisition makes sense for Twitter, which is trying to offer users more customized experiences to improve adoption of its microblogging service. Machine learning is likely critical to those initiatives, and acquiring technology that makes the application of machine learning technology easier could benefit Twitter’s in-house development process.The deal comes at a tumultuous time for the company, which announced last week that CEO Dick Costolo would be stepping down on July 1.
Blair Hanley Frank is primarily focused on Microsoft and its competitors in the desktop OS, office suite and browser business for the IDG News Service, and is based in San Francisco.More by Blair Hanley Frank
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Google is preparing a virtual, online Pride parade with Androidify app update</title>
<article>Google’s Androidify app got bumped to version 4.0 Wednesday, which includes lots of LGBT-themed content and other customizations. 
The Androidify site teases an online Pride parade to coincide with the festivities taking place in many cities June 27 and 28. To get your character included, you need to create one before then and submit it to the Android gallery.
The update includes many Pride-themed clothing accessories, like wigs, pants, and hairstyles. You can also outfit your robot with a Pride-themed shirt and the trademark rainbow flag to make it stand out in the parade.
You can get the update when it rolls through the Google Play Store. Or an updated build is available now from APK Mirror.
The impact on you: Google likes to have a lot of fun with its Android characters, and this is the most recent gift that you can play around with. The company, like many in Silicon Valley, is also a strong supporter of LGBT causes. So offering everyone a playful way to show their support for the annual Pride event is a natural fit.
Derek Walter is a freelance technology writer based in Northern California. He is the author of Learning MIT App Inventor, a hands-on guide to building your own Android apps.More by Derek Walter
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Why Sword Coast Legends is the digital Dungeon &amp; Dragons RPG you&#039;ve been waiting for</title>
<article>Hayden writes about games for PCWorld and doubles as the resident Zork enthusiast.More by Hayden Dingman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Mark Penn, architect of the snarky &#039;Scroogled&#039; campaign, is leaving Microsoft</title>
<article>Mark Penn, Microsoft’s executive vice president of advertising and strategy, will leave the company in September to open a private equity firm.
Called the Stagwell Group, Penn’s new firm raised $250 million in capital from investors including former Microsoft CEO Steve Ballmer, and could make up to $750 million in acquisitions using leverage. With that money, Stagwell will focus on investing in advertising, research, data analytics, public relations, and digital marketing services, the company said.
In a letter to employees on Wednesday, Microsoft CEO Satya Nadella said that Penn first discussed his plans to depart the company “a number of months ago.”
During his tenure at Microsoft, Penn was perhaps best known for his role as the architect of the firm’s ”Scroogled” campaign, which was launched in 2012 and involved a series of advertisements criticizing Google for its collection of consumer data and the performance of devices running Chrome OS. Along with the ads, Microsoft also sold apparel emblazoned with the Chrome logo and slogans like “Keep Calm while We Steal Your Data.” The combative tone of the “Scroogled” ads proved divisive, and Microsoft ultimately discontinued the campaign in 2014.
He also led the creation of Microsoft’s 2014 Super Bowl advertisement. The TV ad, which was entitled “Empowering,” focused on ways that technology helped people and was widely well-received.
Being away from Microsoft could free Penn up to once again serve as a political advisor. He worked on multiple campaigns for Bill and Hillary Clinton. In the political realm, Penn is well-known for identifying and focusing on “soccer moms” as swing voters in the 1996 election. He also served as chief strategist for Hillary’s failed 2008 presidential bid, and she’s one of the front-runners to become the Democratic Party’s presidential candidate in 2016.
The announcement of Penn’s departure comes alongside a larger executive shake-up at Microsoft. Stephen Elop, the former Nokia CEO who led the company’s devices and services group, has left the company. Terry Myerson, who was the head of Microsoft’s Operating Systems Group, will now lead the Windows and Devices Group, which includes his previous responsibilities along with oversight of Microsoft’s hardware efforts. Dynamics CRM head Kirill Tatarinov and Executive Vice President of Advanced Strategy Eric Rudder are also leaving.
Blair Hanley Frank is primarily focused on Microsoft and its competitors in the desktop OS, office suite and browser business for the IDG News Service, and is based in San Francisco.More by Blair Hanley Frank
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Cisco to make $10 billion investment in China</title>
<article>Cisco plans to invest US$10 billion in China, although its sales in the country are slumping due in part to persistent security concerns surrounding U.S. technology.The investment marks a “new chapter” for the company, and it includes agreements with the Chinese government to expand in areas including research, and job creation, Cisco said on Wednesday.The $10 billion investment will be made over several years, and will help spur technology innovation in the country, Cisco said, without further elaborating. It called the move a “renewed commitment” suggesting that the investment would be added on top of its existing operational expenses in China. Cisco could not be immediately reached for comment.As part of the investment, the company has signed agreements with China’s National Development and Reform Commission, a high-level government agency, and with an association connected to the Ministry of Education.Cisco has been in the Chinese market for over two decades supplying networking equipment, including what’s used in the country’s Internet infrastructure. But its business in the country has struggled in recent years due to “geopolitical challenges”, according to the company.Following leaks about the U.S.’s secret surveillance programs, China has made cybersecurity a priority. As a result, the country is considering stricter regulations that could push U.S. tech firms out of the market.In Cisco’s case, it also had to contend with competition from Huawei Technologies and ZTE, two Chinese companies that have become major players in the networking sector and are moving into enterprise services.But even as China focuses more on cybersecurity, Cisco said on Wednesday it’s committed to the market. The company announced the investment as both its outgoing CEO John Chambers and incoming CEO Chuck Robbins met Chinese government officials.Cisco will continue to “work closely” with the Chinese government and its partners to better meet demands for the market, Cisco’s head of the Greater China Owen Chan, said in a statement.Back in 2007, Cisco made another huge investment in the country that reached $16 billion. Most of the money was geared toward doubling its manufacturing.The market opportunity in China is huge. The country has over 600 million Internet users, and there’s a growing need for more data centers, and faster networking infrastructure.
Michael Kan covers IT, telecommunications, and the Internet in China for the IDG News Service.More by Michael Kan
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Report: Huawei confirms it&#039;s building the next Nexus phone</title>
<article>More signs point toward China-based Huawei as the company behind a new Nexus smartphone.
A Huawei employee confirmed the company is already working on the device, according to an IBTimes UK report. If so, it would be the first go at a Nexus phone for Huawei, which would has been steadily trying to build its presence in the U.S. market.
The question then is if Huawei is the only partner. Our most recent rumor was that Huawei was working on a 5.7-inch device, with LG making a 5.2-inch smartphone that would be a follow-up to the popular Nexus 5. 
Such a scenario would appeal to a wider variety of buyers than the Nexus 6, which is rather unwieldy given its colossal size. Also, a small size reduction to a 5.7-inch phone might make it more attractive ,as it’s the same size as the Galaxy Note 4. This would give buyers a manageable, large-screen phone that runs pure Android.
The story behind the story: This partnership makes a lot of sense for both Google and Huawei. Google has been trying to get its paid Play Store apps onto Chinese Android phones, but hasn’t been able to reach a deal with the Chinese government. Huawei’s U.S. division president, Zhiqiang Xu, told us in an interview the company wants to be among the top three phone makers here. This could be the glitzy product Huawei is looking for.
For comprehensive coverage of the Android ecosystem, visit Greenbot.com.
Derek Walter is a freelance technology writer based in Northern California. He is the author of Learning MIT App Inventor, a hands-on guide to building your own Android apps.More by Derek Walter
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Salesforce Marketing Cloud aims to blur the lines between marketing, sales and service</title>
<article>Marketers have long struggled with the challenge of engaging customers across channels, but an updated version of Salesforce’s Marketing Cloud could help.Unveiled on Wednesday, the new software offers several enhancements designed to give companies a single place for planning customer “journeys,” or managing their interactions with a brand across sales, service, marketing and more. It also aims to make it easier for marketers to orchestrate ad targeting across the digital advertising ecosystem.First, Marketing Cloud’s updated Journey Builder tool promises to give marketers the ability to guide customers on journeys across channels and devices and ensure that those customers always get the right message in the right place at the right time. Essentially, it does that by enabling companies to connect every interaction customers have with their brand across every department, making it easier to see and manage the overall picture.New pre-built Sales Cloud and Service Cloud events and activities in Journey Builder are part of the new capabilities making those cross-channel integrations possible. So, too, are new pre-built Journey Triggers, which enable marketers to automate inbound event-driven triggers, such as a customer joining a loyalty program or downloading an app, so that the customer is automatically sent a message on any channel. Triggers can also automatically modify data in the customer contact record or set up wait times to adjust the journey in real-time based on customer interactions across sales, marketing and service.“The most important issue in digital marketing is for the marketer to understand where the customer is in his or her journey,” said Denis Pombriant, managing principal at Beagle Research Group. “We think we know these things, but what we know is based on assumptions, and while some assumptions are on target, many others are not.”The result is that vendors can miss critical opportunities, Pombriant added.“Journey mapping via Journey Builder enables us to do away with pure assumptions by replacing them with testable hypotheses about reality,” he said.The updated tool also extends the journey-mapping process beyond marketing and into the front office, Pombriant noted, making it easier to maintain a consistent customer focus.The second newly updated piece of Marketing Cloud is its Active Audiences ad platform, which syncs ad targeting with CRM to help marketers run more relevant ads wherever they execute campaigns.New partnerships with companies including LiveRamp, LiveIntent, Neustar and Viant aim to empower Salesforce users to activate CRM data and run ads across more than 100 digital advertising networks and technologies.Full integration with Journey Builder, meanwhile, helps marketers coordinate their advertising efforts with customers’ entire brand experience, including their purchase history, engagement with customer service teams, emails and mobile messages opened, and where they fall in the sales cycle.Finally, through Salesforce’s partnerships with Facebook and Twitter, Active Audiences now enables marketers to reach customers and prospects on those social sites.Salesforce Marketing Cloud and Journey Builder are generally available today for customers. Journey Builder pricing starts at $3,750 per month. New Journey Builder activities, triggers and events with Sales Cloud and Service Cloud will be available in the fourth quarter of this year.Active Audiences, meanwhile, is generally available today for customers. New Active Audiences display features are expected to be generally available in the third quarter. Active Audiences pricing starts at $4,200 per month.
Katherine Noyes has been an ardent geek ever since she first conquered Pyramid of Doom on an ancient TRS-80. Today she covers enterprise software in all its forms, with an emphasis on Linux and open source software.More by Katherine Noyes
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Samsung launches app to record your mobile gaming highlights</title>
<article>If you have a Samsung device there’s now an easy way to record and share your gaming exploits with the world.
The new Game Recorder+ app offers a simple solution and works with most flagship Samsung devices: Galaxy S6, S6 Edge, Galaxy S5, Note 4, S4, Note 3, and Note 2.
With just a few steps I was able to record a gaming session with Zombie Defense (one of the best tower defense games on Android) and upload it to YouTube.
Game Recorder+ also keeps the microphone on in case you want to add commentary while you’re playing. If you don’t want your face featured as prominently, you can reduce the size of the selfie circle, or select a static profile image from your library. Hopefully, Samsung will update the app to allow you to disable the circle entirely.
When you launch a game the Game Recorder+ app will place a button at the bottom of the screen. Just touch record to start saving your gameplay. A “game boost” feature may also appear from time to time, which Samsung says gives device’s memory some extra breathing room so things don’t slow down. 
Perhaps the most interesting part of this app is that it appears in the Google Play Store, instead of Samsung's own app store (as most of its apps do). Is this the start of a positive move away from a fragmented company app store?
Why this matters: Game recording is a big business. That’s why Google is launching YouTube Gaming this summer after it tried to snatch up Twitch. Amazon bought the gaming clip service for $970 million. If the game sharing fever extends to mobile, look for Amazon and Google to turn their attention to all those Android gamers also.
For comprehensive coverage of the Android ecosystem, visit Greenbot.com.
Derek Walter is a freelance technology writer based in Northern California. He is the author of Learning MIT App Inventor, a hands-on guide to building your own Android apps.More by Derek Walter
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>New modems to fuel superfast broadband over copper</title>
<article>Alcatel-Lucent and chipmaker Sckipio Technologies are debuting modem technology that will help make speeds of hundreds of megabit per second over copper cables a reality.
The technology that makes it possible is called g.Fast. Step-by-step, chipmakers and equipment manufacturers are getting it ready for large-scale commercial services. They are expected to arrive next year.
After getting the network equipment to work, vendors are increasingly focusing on the modems.
Alcatel-Lucent has launched the 7368 ISAM residential gateway, which uses g.Fast and 802.11ac to offer speeds surpassing 750Mbps, it said. The product has four Gigabit ethernet ports and two USB 3.0 ports. It can also be used to deliver broadband over VDSL2. The thinking is that operators can offer VDSL2 today and then upgrade to G.fast.
Sckipio, meanwhile, has announced a new line of G.fast reference designs to support G.fast modems inside an SFP (small form-factor pluggable) module. This approach lets modem manufacturers develop products that aren’t tied to one access technology, and also makes upgrades easier. Sckipio has also developed a reference design for an affordable modem.
Neither company offered any details on pricing and shipping.
The speed increase offered by g.Fast is needed for applications such as streaming 4K video (and in the future, 8K video), IPTV, cloud-based storage, and HD video calls. At the International CES trade show in January, Sckipio demonstrated G.fast’s ability to carry 4K TV.
Operators backing g.Fast include British Telecom, Telekom Austria and Swisscom.
Last month, Swisscom said it was testing G.fast data transmission under real conditions for the first time, offering speeds up to 500Mbps. The operator is planning to use G.fast for all its FTTS (fiber to the street) and FTTB (fiber to the home) connections from 2016. Because g.Fast performs at its best over short distances, the technology will be used in combination with fiber.
Telekom Austria also expects to launch commercial services next year. BT is a bit more cautious and has stated its first services will come next year or in 2017.
Mikael Ricknas reports on telecommunications, cellular, and mobile technology for the IDG News Service, and is based in London.More by Mikael Ricknäs
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Amazon pulls popular video app Kodi from its store, blames piracy</title>
<article>Amazon has drawn a line in the sand on piracy by banning the popular Kodi media player from its app store.
Kodi, an open-source program formerly known as XMBC, disappeared from the Amazon Appstore last week. When the Kodi team asked for an explanation, Amazon emailed several days later to explain that the app “can be used to facilitate the piracy or illegal download of content,”  AFTVnews reports  .
Keep in mind that Kodi doesn’t come with any illegal content. Out of the box, it’s simply a way to view video and music files, either on the device itself or streamed over a local Wi-Fi network. You can certainly use Kodi to watch illegally-downloaded movies or ripped DVDs, but in that regard it’s no different from Plex, another popular media app that Amazon welcomes in its store. And while Kodi offers a list of add-ons for streaming Internet music and video, none of its built-in options come from pirated sources.
Perhaps the sticking point for Amazon involved the more illicit add-ons that you can acquire from outside the main Kodi app. These add-ons, such as Phoenix and Genesis, pull in copyrighted movies and TV shows from various web sources for extremely easy access. But again, Kodi doesn’t include these add-ons in its app. Installing them involves downloading the add-ons to a computer, transferring them to the Kodi device, and then loading them up within the Kodi software.
That seems like a fine distinction to make—after all, any web browser or file manager can facilitate piracy on some level—though it’s worth noting that Amazon has made similar judgments in the past. The Appstore is now clean of emulators (apparently banned in the last year or so) and apps for downloading torrent files.
Meanwhile, Google doesn’t seem to have any issue with letting these apps in its Google Play Store for Android. In fact, an official version of Kodi just arrived on Android last week, complete with Android TV support.
The impact on you at home: While the move makes Amazon’s position on pirate-friendly apps clear, it won’t have much practical effect if you’re tech-savvy enough to use Kodi and install outside add-ons. Sideloading Android apps onto Amazon Fire tablets isn’t much more complicated, and even before the ban, you still had to sideload Kodi onto Amazon’s Fire TV and Fire TV Stick. While you’re at it, you can even pick up  a customized version with all the legally questionable add-ons  pre-loaded.
Jared writes for PCWorld and TechHive from his remote outpost in Cincinnati.More by Jared Newman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Master of Orion reboot hands-on: A PC gaming classic dragged into the modern era</title>
<article>“It is a legend. We need to get it right,” says Wargaming CEO Victor Kislyi in reference to the company’s upcoming Master of Orion  remake/reboot/sequel. And he’s not wrong. Master of Orion is one of those hallowed PC games—a 4X title so ahead of its time that it’sstill referenced as one of the greatest in the genre, twenty years later. Even by us.
That’s quite a legacy to live up to, which is—I assume—why Wargaming stresses how much it’s playing safe. The original races, the original storyline—in a lot of ways, this new Master of Orion endeavors to recreate the original except, as Kislyi points out, with the benefits of 2015 hardware.
Further reading: 33 must-see PC games revealed at E3 2015
And on that front, Wargaming succeeds. If there’s one thing that’s immediately clear about Master of Orion, it’s that the game is beautiful. If I put Civilization: Beyond Earth, Galactic Civilization III, and Master of Orion in a room together, Master of Orion is coming out on top as far as graphics.
It’s the little touches. For instance, every building you create is rendered out on the surface of your planets—planets that are constantly in motion, with ships and satellites orbiting overhead. Or it’s the fidelity of Master of Orion’s classic races, now rendered in full 3D and animated in thematically appropriate ways (i.e. the Mrrshans move like cats).
The races are in fact one of the most impressive parts of Master of Orion’s whole. Voice acting, for instance, is also tailored to each race. Playing as the Alkari, our advisor tossed in random bird squawks which—while somewhat a parody of each alien’s role—does help make each race feel suitably different and appealing. Each race also opens with a fully-animated, unique cutscene to sell the backstory, which is a nice touch—it avoids the follies of Beyond Earth’s more blank-slate approach to factions or   Age of Wonders III's info-dumps.
I came away from my demo feeling like Wargaming has succeeded in selling the spectacle of Master of Orion—of recreating that space opera feel of the original.
But I’m still not sure how it plays, aside from your standard 4x trappings. The galaxy map is back, you send fleets out to chart and colonize new systems and planets, you build ships, you engage in standard Civ-esque “trading favors” diplomacy. The biggest difference is a UI overhaul that looks much more modern and sleek—definitely an improvement over the original.
Again, though, that’s a graphics thing. The whole game is beautiful, but by adhering so close to the original in other respects I’m curious whether Master of Orion brings anything new to the table. One of the reasons the original is so revered is because it was so ahead of its time—but it’s been much-emulated by a host of spiritual successors over the past 20 years. This new game, aside from being gorgeous, doesn’t seem incredibly dissimilar from (for example) Galactic Civilization III. In fact, what I’ve played of GalCiv seems more compelling, thanks to its amazing custom-ship builder.
But it’s honestly hard—almost impossible—to know. With Master of Orion today I got a brief demo of a not-so-brief game, and picking up on the subtle differences between 4X games is difficult even in longterm play, let alone over such a short timespan.
For now, my feeling about Master of Orion is it’s simply “another space 4X game”—lovingly updated, but entering into a niche that’s gotten pretty crowded lately. It is, of course, “another space 4X game” with the benefit of a twenty-year legacy and name, but even so that might not be enough. Being faithful to a legend doesn’t just mean recreating it for a new audience—it means pushing it forward. I’m convinced Wargaming has done that graphically. Now I’m waiting to see whether they’ve also done that from a mechanics perspective.
Stay tuned for more coverage of E3 2015 from PCWorld, and be sure to follow my Twitter account for up-to-the-minute impressions and shots of the show floor.
Hayden writes about games for PCWorld and doubles as the resident Zork enthusiast.More by Hayden Dingman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Uber driver deemed an employee by California Labor Commission</title>
<article>Martyn Williams covers mobile telecoms, security, Silicon Valley, and general technology breaking news for the IDG News Service, and is based in San Francisco.More by Martyn Williams
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>New Kindle Paperwhite: higher resolution, same price</title>
<article>Jared writes for PCWorld and TechHive from his remote outpost in Cincinnati.More by Jared Newman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Raspberry Pi Foundation produces first official case for the micro PC</title>
<article>Ian is an independent writer based in Tel Aviv, Israel. His current focus is on all things tech including mobile devices, desktop and laptop computers, software, social networks, Web apps, tech-related legislation and corporate tech news.More by Ian Paul
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Star Wars Battlefront hands-on: Not the Battlefront sequel you&#039;re looking for, but still pretty good</title>
<article>I'm ruined. I don't think it's necessarily my fault, in this case, but one thing is becoming undeniably clear to me: As far as Star Wars Battlefront is concerned, I'm ruined. I'm checked out. I'm "not the target audience," or however you want to put it. I am the Fallout 2 fanatic who felt burned by Fallout 3, or the Doom veteran who wished Doom 3 were a bit less obsessed with monster closets. I am, in other words, the "scorned fan."
And it's not because EA and DICE aren’t trying, and it's not because EA and DICE have made a bad game. I am sure plenty of people will play Battlefront later this year and enjoy it. In fact, I'm pretty sure I'm going to play it and wrangle some fun out of it.
It's not the Battlefront III I waited and waited (and waited) for though. It's just not. I don't think it's worth making excuses, or trying to pretend otherwise. Rather, I think it's time to accept that this Battlefront is going to be a wholly different beast from the Battlefront I know and love, and then grow to love it (or at least tolerate it)—like when your favorite band puts out an album and you listen to it a dozen, two-dozen times just hoping there's some connection.
Because I didn't feel that connection during my E3 demo. EA plunked us right down in the middle of Hoth, i.e. the Star Wars equivalent of a Normandy Beach level, and I just felt…nothing. Actually, that's not true—I spent a large portion of my demo marveling at the graphics, which are utterly fantastic even on console hardware. The AT-STs are particularly fun to watch, as are the retro spark-filled explosion effects. It’s just utterly gorgeous and extremely unique-looking, in the same vein as   Alien: Isolation.
But that Battlefront feeling, that feeling of playing with the most amazing set of Star Wars action figures? I didn't feel that at all, which surprised me.
There are a number of reasons, many of which I already had concerns about going into the demo. Some are personal taste—I think TIE Fighters look really silly flying in the atmosphere, but they have to because we don't have space battles, and how could you have a Battlefront game without any TIE Fighters?
Others are just not great design from a Battlefield-esque standpoint—like the scripted nature of the Y-Wing bombing runs, rather than ceding control to real pilots. Part of what made the original Battlefront so great was the scope—jumping into an AT-AT and walking it around, for instance. This new Battlefront is bigger in almost every way, and yet somehow feels smaller.
That's partly because it hordes the biggest toys away from players, partly because the matches are capped at a relatively-small 40 players, and partly because our match seemed to get hung up on a single chokepoint, with control going back and forth over the course of the game. There wasn't really much impetus to explore most of the space DICE built out.
Further reading: 33 must-see PC games revealed at E3 2015
And none of those are huge complaints. The game doesn't seem broken, nor does it seem terrible. It just doesn't hit the expectations I had after a decade-long wait. I still remember when footage leaked of the now-canceled Battlefront III. I remember watching as the demo went seamlessly from a planetary battle to a space battle. I remember thinking "Hot damn, this was worth the wait."
DICE's Battlefront? It didn't hit those same notes. It definitely isn't as ambitious as that leaked Battlefront III footage. It feels…well, like Battlefield: Hoth. Not so much the way   Battlefield: Hardline felt like a reskin of Battlefield 4, but there's still that feeling of same-ness, that familiarity, that creeping feeling of "Yeah, this is definitely made by DICE."
Maybe it's not fair to go in with such a heavy bias. Actually, it's definitely not fair. I have a feeling  some Last Guardian fans  will be similarly disappointed, and maybe even some Half-Life 3 fans some day in the far-off future. Is it DICE's fault my expectations are too high? No.
I think it's worth mentioning though, because I think a lot of you are in a similar position—you've waited for this game for a long time, and now you're curious "Will it live up to my own unreasonable, self-inflicted hype?" And for me, the answer is "Not yet."
Notice, that's not a hard "No." I'm still disappointed in the lack of space battles. I'm disappointed in the scope. I'm disappointed in how much the levels are scripted.
But I also—and this is an important point—want to play a Star Wars shooter. Battlefront is a good one of those, even if I have my qualms about dubbing it a good Battlefront II sequel. Again, to return to my earlier metaphor: It's like when your favorite band changes styles and you go "This is a good album, it's just not a good [Insert Band] album." Or, more appropriate, it's like when your favorite band "sells out"—everything is cleaner, more polished, more approachable…but just a little more lifeless than before.
Then one day you're driving down the road and the band's latest single comes on the radio and you find yourself absent-mindedly tapping the beat out on the steering wheel, singing under your breath. And in that moment you realize: "Hey, this will never be my favorite but…well, it isn't half bad."
Stay tuned for more coverage of E3 2015 from PCWorld, and be sure to follow my Twitter account for up-to-the-minute impressions and shots of the show floor.
Hayden writes about games for PCWorld and doubles as the resident Zork enthusiast.More by Hayden Dingman
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>How to find the secret Start Menu built into Windows 8.1</title>
<article>Freelance journalist (and sometimes humorist) Lincoln Spector has been writing about tech longer than he would care to admit. A passionate cinephile, he also writes the Bayflicks.net movie blog.More by Lincoln Spector
View more PCWorld videos &#187;
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>How one bad keystroke can lead you to SpeedUpKit &#039;scareware&#039;</title>
<article>Dozens of misspelled domain names that spoof major brands are leading unsuspecting PC users to a questionable tune-up application called SpeedUpKit.
Since people are unlikely to seek out the application, its promoters rely partly on people misspelling the domain name for prominent brands to lead them to it. If you try to access the obituary website legacy.com from a Windows PC in the U.S., for instance, but type “legady” by accident, you’re likely to end up on a page promoting SpeedUpKit.
The practice, known as typosquatting, can sometimes violate consumer protection laws or constitute trademark infringement. Big brands police the web for such misspellings, and domain name registrars often try to stop the practice, but it still happens.
SpeedUpKit, which costs $30, claims to clean registry entries and junk files from a user’s PC. But a test of the application showed that it finds hundreds of problems even on a brand new computer.
On a fresh installation of Windows 7, the trial version of SpeedUpKit found 645 issues with the computer’s registry. And it flagged the computer’s “system registry health status” as “danger” in red capital letters.
Security experts often classify such programs as scareware. They’re applications that may have some legitimate functionality, but are really intended to scare non-savvy computer users into buying security products they probably don’t need.
Microsoft, Adobe, Google, Wikipedia and the New York Daily News are among the companies that have been targeted by SpeedUpKit for typosquatting, according to DomainTools, a company that provides investigative tools for domain name research.
The domain names were registered by Paul Cozzolino of Boynton Beach, Florida, records show. For example, Cozzolino registered ewwgoogle[dot]com, a variation of google.com.
If browsed in the U.S. on a Windows computer, the site redirects from ewwgoogle[dot]com to systemloginfo[dot]com, which was registered by Cozzolino last month, according to DomainTools. A warning that displays there says the computer’s antivirus software may be out of date. Another pop-up says “Please repair MSIE security updates.”
If users continue to click through the prompts, SpeedUpKit is downloaded. It offers to fix 10 issues for free, but pushes people to buy the full program.
Cozzolino couldn’t be reached for comment despite several attempts by email and phone.
According to his LinkedIn profile, Cozzolino moved from Florida to Portland, Oregon, around October last year. He started a company called CallTactics, which specializes in online advertising and managing inbound calls.
CallTactics worked in part with EZ Tech Support, a Portland-based inbound call center that shut down last week, according to a former EZ Tech employee who requested anonymity.
EZ Tech Support fielded calls from a variety of online advertising campaigns that primarily used adware. In some cases, adware baits people by offering a free utility, such as media player or a security scan, but often pushes paid-for software.
People who called EZ Tech were pushed to buy Defender Pro Antivirus for $300 and a one-time computer servicing for $250.
The FTC has taken a dim view of such schemes. Last November, it filed two federal lawsuits alleging a handful of mostly Florida-based telemarketing and software companies conned people out of $120 million.
The lawsuits alleged the companies falsely convinced people their computers had problems in order to sell them ineffective and overpriced software.
Jeremy reports on security and regional news for the IDG News Service.More by Jeremy Kirk
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>HTC&#039;s decision to slap ads on BlinkFeed app angers users</title>
<article>As HTC faces shrinking revenue, the smartphone vendor is testing ads over its BlinkFeed media aggregation app, and some users aren’t happy.
On Tuesday, HTC said it had begun rolling out an update to its BlinkFeed app that would include advertisements for users in the U.S., the U.K., China and a few other markets.
The ads, for now, will be in a limited number, and promote sponsored apps, in addition to HTC accessories and devices, the company said in a blog post.
“Because these are native ads, they will appear like a typical BlinkFeed post rather than as a pop-up or banner ad,” HTC added. However, users will be given an option to opt out from seeing the ads.
Many HTC customers are opposed to the change. Several users posted to HTC’s blog post, complaining.
“I don’t buy a phone to enjoy my amazing ads,” wrote one user.
BlinkFeed debuted back in 2013 as a software feature found on HTC phones. It works as a scrolling news and social media feed that can sit on the handset’s home screen. Users can customize the information sources displayed.
In its Tuesday blog post, HTC stressed that the inclusion of ads was a pilot project. “It’s too soon for us to say when or if these promotions will be rolled out to additional markets,” HTC added.
Financially, the company has been struggling, amid heated competition with Apple, Samsung and emerging Chinese smartphones vendors. Earlier this month, HTC revised its earning projections for this year’s second quarter, stating that revenue would be down further than expected on weakening demand for high-end phones.
The dire situation has caused some to wonder if HTC should be acquired. But HTC has said it will continue developing hit products, and look for opportunities outside smartphones.
Michael Kan covers IT, telecommunications, and the Internet in China for the IDG News Service.More by Michael Kan
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Amazon may soon look to you for package delivery</title>
<article>Ian is an independent writer based in Tel Aviv, Israel. His current focus is on all things tech including mobile devices, desktop and laptop computers, software, social networks, Web apps, tech-related legislation and corporate tech news.More by Ian Paul
Microsoft Lumia 640 Review: It's affordable, but it's missing a few things we love about Lumias
Hardcore Hardware: CyberPower Trinity Xtreme
Intel 750 SSD ushers in NVME and stupid fast performance
View more PCWorld videos »
Close
Jun 16, 2015 12:01 AM
Sibling Rivalry: The awesome GeForce GTX 980 Ti pushes the GeForce Titan X off the cliff
READ THE RELATED ARTICLE:
Hardcore Hardware: Nvidia GeForce GTX 980 Ti 
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
Visit other IDG sites:
&copy; 1998-2015, IDG Consumer &amp; SMB
</article>
</item>
<item>
<title>Oracle profit slides 24% as customers move to the cloud</title>
<article>Larry Ellison speaks at Oracle Open World in San Francisco on Sept. 22, 2013.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>U.S. visa system will be offline until at least next week</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>AT&amp;T, WhatsApp get low marks for data disclosure policies</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Hiring recent college grads: 5 strategies that work</title>
<article>This year’s STEM grads know what they want and are more prepared for the workforce than ever. According to a new report from Accenture, the class of 2015 is skilled, motivated and loyal — and anxious to join the ranks of IT.
That’s great news for IT departments, where hiring is expected to increase in the second half of the year, according to Robert Half Technology (RHT). Twenty-two percent of CIOs plan to add more staff, up from 19 percent in the first half of the year and just 14 percent from a year ago, according to the RHT survey.
John Reed, senior executive director at RHT, says that this job seeker’s market means hiring managers need to be strategic and prepared, particularly when hiring from the class of 2015.
Here are five interview strategies that will help you make smarter decisions and nab this year’s top new talent.
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Tired of sitting all day? Tech pros take a stand</title>
<article>Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Twitter acquires machine-learning startup Whetlab</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Microsoft shakes up the exec org chart as Myerson climbs, Elop falls</title>
<article>Microsoft today shuffled its executive ranks, giving Windows chief Terry Myerson more responsibilities and casting out former Nokia CEO Stephen Elop.The Redmond, Wash. firm also pared the number of major engineering divisions from four to three.CEO Satya Nadella broke the news Wednesday, with Microsoft issuing both a statement and the email Nadella sent to employees."To better align our capabilities and, ultimately, deliver better products and services our customers love at a more rapid pace, I have decided to organize our engineering effort into three groups that work together to deliver on our strategy and ambitions," Nadella said in the email.Two of the three groups existed prior: Cloud &amp; Enterprise (C+E), led by 18-year veteran Scott Guthrie, and Applications &amp; Services (ASG), headed by Qi Lu. Guthrie's group is getting Dynamics, Microsoft's customer relationship management (CRM) and and enterprise resource planning (ERP) software, which previously was in its own oddball division.Myerson, who previously directed the Operating System group, will continue to do that while also picking up the Devices team, which Elop had run since his return to Microsoft after the $7.9 billion Nokia acquisition finalized last year. Former CEO Steve Ballmer did the deal just months before he stepped down.Myerson's new group, a mash of his OS team and Elop's Devices division, will be called "Windows and Devices Group," or WDG for short.Nadella touted the combination of OS and devices. "This new team brings together all the engineering capability required to drive breakthrough innovations that will propel the Windows ecosystem forward," he said. "This enables us to create new categories while generating enthusiasm and demand for Windows broadly."The creation of Myerson's WDG was a return to how Microsoft organized its engineering efforts immediately after the mid-2012 launch of its Surface and Surface Pro tablets, the company's first-ever personal computer. Then, Julie Larson-Green, who was later shunted to a different role after Elop came aboard, ran the engineering of Microsoft's OSes and its hardware."That's a massive group," said Patrick Moorhead, principal analyst at Moor Insights &amp; Strategy, in an interview today. "I think it might almost be too big to manage."But Rob Helm, an analyst at Directions on Microsoft, saw it differently. "I think this underlines the fact that Microsoft's hardware business is going to be the first and best customer of Windows," Helm said.Moorhead viewed the rearrangement of executives' chairs as a continuation of the massive reorganization that Ballmer instituted in July 2013. The then-CEO called it "One Microsoft," and touted it as an efficiency move that would make the firm nimbler in a market quickly skewing towards mobile, which Microsoft has largely missed."A reorganization is never really done," Moorhead argued. "Companies in high tech are especially in a perpetual state of reorganization."Again, Helm had another take. "This is the start of the Nadella era," Helm said. Nadella took the CEO office in February 2014 after a several-months search that some believed would result in the appointment of an outsider, not a 22-year insider.Several long-time lieutenants of Ballmer and co-founder Bill Gates before that were put in place to provide advice to Nadella, Helm said. But many of them have since moved on; the removal of others today was simply another part of the same process."Nadella started with the organization that he was given, but he's now put his own stamp on it," said Helm.Several top-level executives will leave the company in the coming months, Nadella said today, as fallout of todays' moves, including Elop; 25-year veteran Eric Rudder, vice president of advanced technology and education; and Kirill Tatarinov, who had been running Dynamics.Also out: Mark Penn, the former political advisor who joined Microsoft in 2012, ran the anti-Google Scroogled campaigns, and was most recently the company's chief insight officer, a role that was never clearly defined. Penn will found a new digital marketing equity firm, Stagwell Group, that has raised $250 million in capital, some of it from Ballmer's billions.Kurt DelBene, who was pushed out in 2013 by the Ballmer reorg but brought back by Nadella in April as the head of corporate strategy and planning, will continue in that role.Neither Moorhead nor Helm viewed the changes as indicative of a switch in strategies at Microsoft. Nadella's email, in fact, trumpeted the same mantras that have become writ at the company, including "productivity services and platforms" and "mobile-first, cloud first world."Although Wall Street has often urged Microsoft to divest itself of the Nokia business, there was no clue of such a move in today's executive game of musical chairs, said Moorhead."What will be interesting, though, is who leads the hardware under Myerson," Moorhead said. "Microsoft needs a star hardware person. If they do that, then no, [Microsoft won't dump Nokia]. But if they put a long-time software person under Myerson [to head hardware], then maybe."Earlier this year, Microsoft filed documents with the U.S. Securities and Exchange Commission (SEC) that implied the company would soon take a massive write-off of its Nokia acquisition that could run into the billions.According to The Verge, Jo Harlow, a former Nokia executive who came with Elop to run Microsoft's phone business, will also leave Redmond.Nadella's high-level strategy remains intact, Moorhead maintained. Today's changes were of a tactical grade. "But the mobile and cloud strategy may or may not include a hardware business," Moorhead said.
Four Microsoft top-level executives out in another reorg that also pares the number of engineering groups to three from four.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Cisco to make $10B investment in China</title>
<article>John Chambers recently stepped down as Cisco's CEO and chairman after 20 years with the company.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Update: Calif. decision could undermine Uber&#039;s business model</title>
<article>In a decision that could have major implications for the way Uber does business in its home state, the California Labor Commission has ruled that a driver was an employee when she was driving for the company.Uber has always claimed that its drivers are contractors, a classification that means Uber doesn't have to provide them multiple employment benefits, but the ruling challenges that assertion.The decision was reached earlier this month by the labor board after a hearing on a dispute between a former Uber driver, Barbara Berwick, and Uber. Berwick had sued Uber, claiming she was owed unpaid wages, car expenses and interest.The decision became public on Tuesday when it was filed with the state court in San Francisco. It was first reported on Wednesday by Reuters."Defendants hold themselves out as nothing more than a neutral technological platform, designed simply to enable drivers and passengers to transact the business of transportation," the California Labor Commission wrote in its finding. "The reality, however, is that defendants are involved in every aspect of the operation."It said that Uber vets drivers and approves them, controls the cars they drive, monitors their ratings, manages their access to the Uber app, and controls the amount of money they can earn by setting rates.Therefore, the labor commission ruled, the driver was an employee of Uber.As a result, the court ordered Uber to pay the driver for the miles she drove as an employee, at the Internal Revenue Service auto reimbursement rate of 55 cents per mile. It also ordered Uber to pay the road toll charges she incurred while working for Uber.With additional interest, Uber was ordered to pay $4,152.Uber has appealed the ruling.In a statement, the company noted a 2012 ruling by the same labor commission which found a limo driver was a contractor and not and employee. In that case, the commission said Uber was engaged in technology and not the transportation industry."Five other states have also come to the same conclusion," the company said. "It's important to remember that the number one reason drivers choose to use Uber is because they have complete flexibility and control. The majority of them can and do choose to earn their living from multiple sources, including other ride sharing companies."The ruling follows a similar one last month in Florida, where the Department of Economic Opportunity ruled that an out-of-work Uber driver is an employee and so can collect unemployment insurance. The driver's account had been deactivated by Uber following a disagreement over damage to his car leading the driver to pursue a claim for unemployment benefits, according to The Miami Herald.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Update: FCC to fine AT&amp;T $100M over &#039;unlimited&#039; data plan throttling</title>
<article>FCC Chairman Tom Wheeler.
The Federal Communications Commission plans to fine AT&amp;T $100 million for misleading customers by throttling speeds on the lines of millions of customers who had "unlimited" data plans.The FCC alleges that AT&amp;T did not adequately disclose to its customers on "unlimited" data plans that their speed would slow drastically after they had reached a monthly data allowance of 5GBs. The policy began in 2011.Data speeds were significantly slowed to 512kbps from the advertised 5Mbps to 12Mbps under something AT&amp;T called its maximum bit rate policy. That very name is clearly at odds with an unlimited plan, a senior FCC official told reporters on a conference call.By not clearly informing consumers of this policy, AT&amp;T was in violation of the Open Internet Transparency Rule of 2010, which requires Internet providers offer clear and accurate information to consumers so they can make informed choices on Internet service, the FCC said.The fine is the largest proposed by the FCC in its history, but it's not a done deal yet. AT&amp;T has 30 days to respond in writing to the FCC's charges after, which the commission will adjudicate the complaint and determine a final fine.The FCC said it's aware that the fine, while large, is a fraction of the revenue AT&amp;T made from offering its unlimited plan to consumers. It is also considering other redress, including requiring AT&amp;T to individually inform customers that its disclosures were in violation of rules and to allow them out of applicable contracts with no penalty.AT&amp;T said it plans to vigorously dispute the FCC's assertions."We have been fully transparent with our customers, providing notice in multiple ways and going well beyond the FCC's disclosure requirements," it said in a statement.An AT&amp;T spokesman pointed to previous FCC guidance on disclosure of network management policies that noted providers must, at a minimum, provide a "publicly available, easily accessible website" and that "broadband providers may be able to satisfy the transparency rule through a single disclosure."AT&amp;T had demonstrated this through notifications on billing statements, text messages sent before throttling, detailed information about the process on its Web site and in the language on its customer agreement, he said.Nonetheless, the carrier recently changed the way it throttles data throughput for customers with 4G unlimited data plans. In the past it would automatically slow speed when the 5GB limit was reached, but in May is said it "may" slow speeds after 5GB in accordance with network management requirements.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>IDG Contributor Network: EoT: More than meets the API</title>
<article>This article is published as part of the IDG Contributor Network. Want to Join?
Does Intel have a major advantage in tablets?
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>New modems to fuel superfast broadband over copper</title>
<article>Alcatel-Lucent 7368 ISAM residential gateway works with both VDSL2 and g.Fast.
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>BrandPost: The Landscape Is Changing in the Security Technology Industry</title>
<article>Thought-provoking analysis and commentary on how organizations can transform their businesses with truly great data. Sponsored by Informatica.
As a result of massive data breaches experienced by major corporations in every industry, data security is the talk of the tech world. It has also become a board-level topic. As these highly-publicized data breaches show, security – or the lack of it – can have enormous economic and brand/reputation impact. A security failure can cause a company to go into crisis mode.Compounding today’s data security challenge, there is a huge mismatch – a massive gap – between what customers actually need and what today’s security technology can deliver. From a customer’s perspective, security technology should protect an enterprise against anything that could materially impact it, such as a massive data breach. It also should be easy to use, unobtrusive and not create an impediment to productivity or critical activities, such as connecting to partners, developing new applications or automating new business processes.But, today’s security technology is not yet up to the challenge of meeting the needs of the enterprise. The gap between what customers want and what security technology can deliver is massive because, for the longest time, the security industry has been focused on creating point-solutions for security practitioners rather than holistic answers to an enterprise’s security challenges. The network group, for example, has deployed network security tools. The applications group has deployed applications security tools. The end-point group responsible for PCs and mobile devices, etc. has deployed point security tools. They have all attempted to solve their piece of the really big security puzzle in order to avoid being seen as the cause of any security lapses.A medieval castle under siege by a foreign invader provides a good analogy – an illustration of why the old approach to enterprise security no longer works. During the siege, the guard at each gate or tower has determined that the enemy is not going to get past him and breach the walls of the fortress – while paying little attention to the fact that the enemy invader’s assaults are coming in through the air now, not just from the ground.So, now, there’s a massive shift going on in the world of data security. The perimeter-less world of pervasive computing is disrupting the security infrastructure. The individual security defenses now being deployed on a point-solution basis are no longer adequate to the task. The various security point solutions, such as firewalls, in fact might be thought of as active sensors that can not only detect threats but also take action to address them. These sensors generate information in real time but this is not by itself sufficient. Companies must be able to see the bigger picture. As a result, we are now seeing the marriage of security and analytics. The sensor part of a secure network provides alerts which need to be incorporated into a larger analytical environment and comprehensively analyzed using Hadoop, cloud and advanced data security intelligence.In addition to the recognition that there’s a huge, growing and unsolved problem that cannot be addressed by point solutions alone, there’s also a growing realization that security needs to be prioritized. Some business processes are critical and must be well protected. But in other areas of the business, the financial costs – the usability costs – of security are prohibitive because so much security is needed that it hinders business processes.As a result, companies today need to be able to identify what to protect – the enterprise “crown jewels” that need to be well protected. So, instead of starting at the network level and focusing on, for example, how many PCs or mobile devices are attached, companies must ask themselves, “What am I really protecting?”The answers will vary, based on industry. The U.S. healthcare industry provides an excellent example of what I mean. Healthcare is an industry that is undergoing a radical transition from B2B to B2C – from an industry in which the major connection is between providers and payors, to one in which both providers and payors strive to cement a strong loyalty-based relationship directly with consumers. Under the Affordable Care Act, for example, healthcare companies want to establish a direct relationship with consumers so that, when they change employers, the consumers will continue to have a relationship with them. To cement that relationship, healthcare providers are collecting a lot of consumer data and that raises the issue of security to a very high level. The creation of exchanges under the Affordable Care Act also means a lot more data will be exchanged and the greater exchange of data will create a lot more security holes.The food and beverage industry provides another example. For food processing companies, the most important thing is food safety. What is it about food safety that needs to be protected? The answer is not as simple as it seems. In addition to the physical safety having to do with antibiotics and keeping the workplace clean, food processing companies also have supply chains and these supply chains are information-based. So, the question is: “How do you ensure your food is safe throughout your supply chain?”To answer the question of what to protect and how to protect it, companies now are starting to think about data security from a top-down perspective. “What is most important to me, what roles does information play and how do I keep that information secure?”At Informatica, we are working on answers to these questions by focusing our development efforts on data security intelligence. This intelligence leverages and repurposes metadata (data about data) that has been collected and refined over many years for tasks such as data integration and data quality.All security devices and controls are important, but they are part of a larger picture. You need to see the forest, not just the trees. And, this requires the marriage of security and analytics. Everything we are doing in the area of big data – everything we do with regard to data integration, data quality, data security – enables us to play a major role in the changing security landscape by enabling our customers to not get lost in the trees.
EQ and Metadata: the Yin and Yang of Big Data
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Alabama researcher devises a way to harness unused IoT power</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>VMware&#039;s Identity Manager offers authentication for Web, native apps</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Amazon Web Services jumps on Spark bandwagon</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>IDG Contributor Network: Blame Chomsky for non-speaking A.I.</title>
<article>Professor Noam Chomsky revolutionized linguistics in 1957 with his publication of Syntactic Structures, and his Chomsky hierarchy from the previous year remains a foundation stone in computer science for programming languages. But programming languages are a far cry from speaking A.I., and Chomsky’s unprecedented success in that part of linguistics should bear the blame for holding back the advancement in another part of linguistics -- the use of human language for A.I.Obviously, how we use language to communicate is key, but there are a few flavors of the science of language, or linguistics. Chomsky studied formal linguistics, or "the formal relations between linguistic elements," but another type, functional linguistics, studies "the way language is actually used in communicative context." In other words, amazingly, Chomsky's approach, unlike functional linguistics, is not concerned with actual communications!Chomsky’s linguistics, without communications, has been responsible for A.I. that doesn’t speak. While it’s not his fault that others used his approach to solve the wrong problems, we now have the opportunity to progress with different science.Formal linguistics emerges from early computer daysHow did we get here? The birth of A.I. was tumultuous. A number of new sciences were coming together, computer science and linguistics in particular, and they were still being developed.This early work in A.I. was dominated by mathematicians partly due to the archaic stage of digital computers, but while human brains can be good at mathematics, it is just one of the skills they can learn. The problem arises when trying to fit a mathematical model to a non-mathematical brain.Cognitive science, my discipline, focuses on how our brains work. It combines computer science with philosophy, linguistics, neuroscience, psychology and anthropology. It emerged with the goal of replicating cognition on machines roughly 20 years after A.I. was named at the 1956 Dartmouth Summer Research Project on Artificial Intelligence.In the first sixty years since computers exploded into our world, we have seen formal and computational linguistics dominate, despite their scientific conflicts. Early success is good, but hitting the target once isn't the same as hitting a bulls-eye. Also, hitting the bulls-eye once isn’t the same as doing it repeatedly. Science is about ongoing accuracy, hitting the bulls-eye every time.Clearly, we need a new goal.Setting the new goalHAL in 2001: A Space Odyssey and Sonny in i, Robot both use conversational language beyond the capability of today’s artificial, computational languages. Emulating them is a good, revised target because speaking A.I. will be most useful to us if it mimics human communications accurately.As I wrote recently on this blog, in 1969 John Pierce of Bell Labs advised us to work out the science before pushing ahead with engineering. But probably due to frustration at the lack of progress for over a decade, engineering based on statistics was embraced anyway, before the science was ready.To meet the increasing demand for speaking A.I., the key is functional linguistics combined with a brain-based platform. Our goal should be to talk like Sonny because, like the evolution of personal computing, once unleashed, progress will be unstoppable.The right linguisticsPatom theory is my computing approach, in which stored patterns do the work of programmers. But in 2006, as I was adding patterns to the system, the limitations of Chomsky's linguistics hit me.What's the best way to extract meaning from a matched sentence?I spent a lot of time researching the answer and decided to create my own model. It was a big decision because it was like starting a whole new scientific investigation. The implementation was difficult, too, because Chomsky's model was a bit like working in an office tower with a broken elevator where each floor possibly held something important. Moving between floors to check was annoying!And then while browsing in a New Jersey bookshop, I stumbled across the answer. How could I have a degree in cognitive science, but still have missed out on the answers, based on more than 30 years of development, from Role and Reference Grammar (RRG) theory?RRG deals with functional linguistics and considers language to consist of three pieces – grammar linking to meaning in context. You know, word sequences and meaning in conversation. Communication!RRG was developed with the inspiration that all human languages are based on common principles and that clauses (parts of sentences) contain meaning. Its success in modeling the range of human languages is impressive. Speaking A.I. can use RRG’s linking algorithm to map word sequences in context to meaning, and vice versa.It was an eye-opener.The science speaks for itself in whatever language you read it.I subsequently met with the primary developer of RRG, Professor Robert D. Van Valin, Jr., who convinced me that I no longer needed to develop a scientific model to link phrases and meaning because RRG already explains how to do it in depth, like a cook book.It just got better and better. He also pointed out that the same algorithm works for any human language. I was sold, as it not only filled the Chomsky gap, but it meant Patom theory could be used with any language as well. [Disclosure: As our work is synergistic, Van Valin has become one of the advisers to my lab at Thinking Solutions.]Why isn’t RRG used to speak to machines?Here we have unfortunate timing. In the 1980s as RRG was being developed, programmer's continued to struggle with Chomsky’s linguistics.Without waiting for another underlying scientific solution, the industry finally decided to proceed with a method of incremental improvement for computational linguistics, based only on the statistics of sequences of sounds and words.Despite not meeting expectations, computational linguistics and its fixation on word sequences independent of meaning remains at the core of today's A.I. troubles.Our next step will build on the new scientific approach using RRG for linguistics and Patom theory for programmer-free computing. It promises progress while the dominant paradigms deliver disappointment. With a plan for the future, speaking A.I. is finally coming of age.
This article is published as part of the IDG Contributor Network. Want to Join?
A.I. is too hard for programmers
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>IDG Contributor Network: My journey to tech leadership as a minority</title>
<article>This article is published as part of the IDG Contributor Network. Want to Join?
Managing students&#039; experience of college IT
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Free download: Save R data visualization time with these ggplot2 code snippets</title>
<article>From cool iPad apps to taking a hard look at tech data claims, check in here for tech nuggets of note.
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>My ggplot2 cheat sheet: Search by task</title>
<article>There's a reason ggplot2 is one of the most popular add-on packages for R: It's a powerful, flexible and well-thought-out platform to create data visualizations you can customize to your heart's content.But it also can be a bit overwhelming. While I find the logic of plot layers to be intuitive, some of the syntax  can be a bit of a challenge. Unless you do a lot of work in ggplot2, I'm not sure how easy it is to remember that, for example, the simple task of "make my graph title bold" requires the rather wordy theme(plot.title = element_text(face = "bold")).So I've come up with a two-step method that's drop-dead simple -- at least for me -- to do my most common dataviz tasks in ggplot2. I hope it will help you, too.
Below is a cheat sheet, easily searchable by task, to see just how to do some of favorite and most-used ggplot2 options -- everything from creating basic bar charts and line graphs to customizing colors and automatically adding annotations. If you're still somewhat of a ggplot2 newbie, page 2 of this post has a brief explanation of the ggplot2 layers concept.Part 2 will make this even easier. I've created RStudio code snippets for several dozen of these tasks, so you don't even have to copy and paste -- or re-type -- these commands. Instead, you can download my ggplot2 code snippets. Find out more about the ggplot2 code snippets and download them to your own system. (Free registration required.)

Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>How a bad keystroke can lead you to SpeedUpKit &#039;scareware&#039;</title>
<article>Dozens of misspelled domain names that spoof major brands are leading unsuspecting PC users to a questionable tune-up application called SpeedUpKit.Since people are unlikely to seek out the application, its promoters rely partly on people misspelling the domain name for prominent brands to lead them to it. If you try to access the obituary website legacy.com from a Windows PC in the U.S., for instance, but type "legady" by accident, you're likely to end up on a page promoting SpeedUpKit.The practice, known as typosquatting, can sometimes violate consumer protection laws or constitute trademark infringement. Big brands police the web for such misspellings, and domain name registrars often try to stop the practice, but it still happens.SpeedUpKit, which costs $30, claims to clean registry entries and junk files from a user's PC. But a test of the application showed that it finds hundreds of problems even on a brand new computer.On a fresh installation of Windows 7, the trial version of SpeedUpKit found 645 issues with the computer's registry. And it flagged the computer's "system registry health status" as "danger" in red capital letters.Security experts often classify such programs as scareware. They're applications that may have some legitimate functionality, but are really intended to scare non-savvy computer users into buying security products they probably don't need.Microsoft, Adobe, Google, Wikipedia and the New York Daily News are among the companies that have been targeted by SpeedUpKit for typosquatting, according to DomainTools, a company that provides investigative tools for domain name research.The domain names were registered by Paul Cozzolino of Boynton Beach, Florida, records show. For example, Cozzolino registered ewwgoogle[dot]com, a variation of google.com.If browsed in the U.S. on a Windows computer, the site redirects from ewwgoogle[dot]com to systemloginfo[dot]com, which was registered by Cozzolino last month, according to DomainTools. A warning that displays there says the computer's antivirus software may be out of date. Another pop-up says "Please repair MSIE security updates."If users continue to click through the prompts, SpeedUpKit is downloaded. It offers to fix 10 issues for free, but pushes people to buy the full program.Cozzolino couldn't be reached for comment despite several attempts by email and phone.According to his LinkedIn profile, Cozzolino moved from Florida to Portland, Oregon, around October last year. He started a company called CallTactics, which specializes in online advertising and managing inbound calls.CallTactics worked in part with EZ Tech Support, a Portland-based inbound call center that shut down last week, according to a former EZ Tech employee who requested anonymity.EZ Tech Support fielded calls from a variety of online advertising campaigns that primarily used adware. In some cases, adware baits people by offering a free utility, such as media player or a security scan, but often pushes paid-for software.People who called EZ Tech were pushed to buy Defender Pro Antivirus for $300 and a one-time computer servicing for $250.The FTC has taken a dim view of such schemes. Last November, it filed two federal lawsuits alleging a handful of mostly Florida-based telemarketing and software companies conned people out of $120 million.The lawsuits alleged the companies falsely convinced people their computers had problems in order to sell them ineffective and overpriced software.Send news tips and comments to jeremy_kirk@idg.com. Follow me on Twitter: @jeremy_kirk
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Microsoft woos startups with $120K in cloud computing credits</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Huge Samsung Galaxy security flaw -- with no end in sight</title>
<article>A daily digest of IT blogs. Richi and Stephen curate the best bloggy bits, finest forums, and weirdest websites… so you don't have to. Catch the key commentary from around the Web every morning.
Samsung Galaxy S6, S5, S4 and S4 Mini phones have a massive flaw that allows an attacker to take over the device. It's in the keyboard code, of all places, thanks to a custom SwiftKey build. There are about 600 million of these things in circulation, it's thought.The bug is easy to exploit, because the phones are vulnerable to a plain-text man-in-the-middle attack. Yet it's hard to properly patch, because fixing it relies on wireless carriers all over the world getting their respective fingers out and doing something.The more you think about it, the more awful this appears: Imagine a global botnet of 600 million mobiles. In IT Blogwatch, bloggers shudder.Your humble blogwatcher curated these bloggy bits for your entertainment.Thomas Fox-Brewster raises the alarm (and he ain't lickin' chicken): Android phone owners would be forgiven for thinking major manufacturers had their backs when it came to security. ... But a serious issue affecting...as many as 600 million Samsung mobiles highlights just how wrong that assumption can be. … The SwiftKey keyboard pre-installed on Samsung phones looked for...updates over unencrypted lines, in plain text. [It] could be used to give an attacker system user level privileges and allowing them to siphon off...most information the victim would have considered private. … Having been alerted to the issue back in November 2014, Samsung...eventually delivered one to carrier networks in late March for Android 4.2 and above. ... [NowSecure] believes current devices are still vulnerable. ... Samsung had not responded to a request for comment at the time of publication.  MOREAnd Dan Goodin adds: Phones that come pre-installed with the Samsung IME keyboard, as the Samsung markets its customized version of SwiftKey, periodically query an authorized server to see if updates are available. ... Attackers in a man-in-the-middle position can impersonate the server and send a response that includes a malicious payload. … Because Samsung phones grant extraordinarily elevated privileges, [the] payload is able to bypass protections built into Google's Android. … For the time being, there's little people with vulnerable phones can do to prevent attacks. [And] carriers have consistently failed to offer security updates in a timely manner..  MORENowSecure's Ryan "Fuzion24" Welton done unveiled the vuln in London: [You're fired -Ed.]The Swift keyboard comes pre-installed...and cannot be disabled or uninstalled. Even when it is not...the default keyboard, it can still be exploited. [It] was signed with Samsung’s private signing key and runs in one of the most privileged contexts on the device, system user. … This exploit requires no user interaction. … To reduce your risk, avoid insecure Wi-Fi networks, use a different mobile device and contact your carrier for patch information and timing. ... The Play store version of the app has NO impact on the system level keyboard and does not remove the vulnerability. [The] threat will persist until...the Samsung stock keyboard...is patched through a carrier update.  MOREOh, and Welton goes on to clarify an important point: The keyboard checks for updates...the first time the keyboard is opened after every reboot and seemingly randomly every few hours.  MOREBut David Ruddock dismisses it, as "Probably Nothing To Worry About": It's actually not even clear if newer devices can be [exploited] as it was demonstrated on substantially older firmware. While there is no simple way to update the Samsung IME keyboard (you can remove the app entirely if you're rooted). … [And] this isn't an easy flaw to exploit. ... There's probably nothing much here to worry about unless you regularly frequent unsecured wireless networks.  MORENot so, says your humble blogwatcher: 
LastPass users: Your worst nightmare just came true. So what now?
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Free SSL/TLS certificate project moves closer to launch</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Amazon to ask  Congress for fewer drone restrictions</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>OK, that WOULD be a reason to call during vacation</title>
<article>True tales of IT life, fresh every weekday. Got a story of useless users, hapless bosses, clueless vendors or adventures in the IT trenches? Tell Sharky and you could collect a coveted Shark shirt.
Flashback to the mid-1990s, when modems are still in wide use for PCs and this pilot fish is a sysadmin at a local college."The college had a digital phone system that was not compatible with computer modems," says fish. "To be able to test computers at my work station, I had a separate direct phone line installed -- plain old telephone service."That's perfect for fish's needs. It lets him plug an ordinary home-type phone line into a modem for testing, and that's the only thing he needs -- or uses -- it for.Fast forward a year or so: It's vacation time for fish, who's getting out of town for a week. He can't think of any reason his time off should be disturbed -- but, knowing the realities of IT, he leaves a phone number where he can be reached while he's gone.Four days into his vacation, fish gets a call from the building administrator. She tells him the county sheriff has arrived at the building where fish works -- and he's not happy. It seems a 9-1-1 call has been made, and was traced back to the phone line in fish's office.Fish knows the phone line is still active, though he rarely uses it. And just as he tells the sheriff, when officers go into his office, they find the phone cord is draped over the work bench. It's not connected to a phone, a modem or anything else that could dial the emergency number."Further investigation determined the phone line to have excessive noise," fish says. "Apparently this noise just happened to have the right combination of tones or clicks, over a period of time, to be detected as 9-1-1."Fortunately for me, I'd been 300 miles away for four days, and nobody was home."Tell it to the judge...er, Shark. Send me your true tale of IT life at sharky@computerworld.com. You'll score a sharp Shark shirt if I use it. Add your comments below, and read some great old tales in the Sharkives.Get your daily dose of out-takes from the IT Theater of the Absurd delivered directly to your Inbox. Subscribe now to the Daily Shark Newsletter.
What you measure is what you get
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Computerworld’s guide to working with non-geeks</title>
<article>Are you a geek? If you are, this won't come as a news flash for you: You and the users you work with on the business side are pretty much nothing alike -- not in the way you communicate, not in the way you value various traits and not in the way you relate to work. 
If you're not a geek but you work with them, you probably are just as aware that geeks and users can seem sometimes to exist in parallel universes. You might be working on the same project, but there's a good chance that the geeks and the non-geeks are going to have very different ideas about what is important. They will say things to each other that they don't have the background to interpret. Their differences can make it nearly impossible for them to see that they both actually want the project to succeed.
Paul Glen has been observing the interactions of geek and non-geeks for years, gaining insights along the way about the ways that their differences can cause them to work at cross-purposes or simply misunderstand each other's motives. We have gathered here several of his columns in which he tries to help the two sides bridge the gap that separates them and come to understand why they say the things they do. 
Glen pulls no punches. He talks about how it can look to a non-geek that techies are condescending, become impatient quickly and seem like they always have to be right -- and how, in his experience, that stems from geeks being horrified at the thought of being wrong.  
Another column looks at how project planning can go awry, because business people think in terms of achieving a vision and techies are generally looking to solve problems.  Another different perspective that can cause a massive disconnect.
Included here is advice for geeks to speak in terms that business users will understand, for business users to better understand why geeks say things that seem counterproductive to the users, and for managers who need to motivate geeks and get them to work well with others. No matter which side of the geek/non-geek divide you fall on, you're sure to find it instructive.
Register to download the PDF.
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>The untold story behind Apple’s ‘Think Different’ campaign</title>
<article>Appleholic, (noun), æp·əl-hɑl·ɪk: An imaginative person who thinks about what Apple is doing, why and where it is going. Delivering popular Apple-related news, advice and entertainment since 1999. 
The 'Think Different' ad campaign started with a single sketch.
Legend has it that way back when the PC dinosaurs still walked the Earth, returning company co-founder Steve Jobs sought a mantra to show Apple was back in business, and the acclaimed ‘Think Different’ campaign was born.
Think Different with Muhammed Ali.
Sketch differentYou could argue the huge success of the campaign is a key reason that Apple and Steve Jobs remain so deeply recognizable. I thought some readers would be interested in something new I've learned about the genesis of the campaign, which turns out (or so it is claimed) to be the brainchild of Chiat\Day art director, Craig Tanimoto. Here’s what happened, according to Jonathan Littman, who spent two days with Tanimoto researching his next book:“Apple was in trouble, big time. They had only 90 days of money left,” said Tanimoto. “Steve was back at the helm, but the big question was, were they going to survive?”In 1997, the agency needed a big idea for a big problem. Tanimoto opened his sketchbook and began playing with ideas.
Among the earliest sketches, Tanimote dreamed up an image of Edison holding a light bulb with 'think different' as slogan. 
Magritte, Edison, and the Apple lightbulb moment“He doodled some Apple logos, shooting off radiant power lines. Tanimoto drew further inspiration from Rene Magritte’s seminal surrealist work, “Ceci n’est pas une pipe.”  Out popped Tanimoto’s first big concept, a huge billboard featuring a boxy Mac, and a slogan you can see as inspired by Magritte, “This is not a box,” author and former Macweek news editor, Jonathan Littman reports.What would become ‘Think Different’ was gestating, but hadn’t quite been born. Tanimoto began drawing cartoon characters, reflecting on “how some are unique and some are social outcasts”.He put a slogan on the sketches.That slogan was “Think Different”, and the slogan stuck.He created a sketch of Edison clutching a light bulb with the slogan.
Think Different also featured Ghandi, as Tanimoto's original sketches proposed.
He sketched Ghandi, with that slogan.“At that point I thought ‘this is a big idea,” he told Littman.
As you can see from Craig Tanimoto's original sketch, Einstein was in the frame.
EurekaHe did not share the idea with anyone until TBWA staff got together to discuss their ideas one day before presenting them to Jobs. “Ideas are so fragile,” he said.Acclaimed ad designer Lee Clow marched through a room full of ideas from some of the best minds in ads before stopping at Tanimoto’s.“Should it be, ‘Think Differently?’” he asked.“No,” said Tanimoto.“You’re right,” agreed Clow, before saying, “This is the campaign, everyone is working on this one.”And the campaign began, a campaign that still resonates decades later and that arguably put Apple back on the map and laid the seeds for its future fortune.
The little remembered Think Different snail
Crazy to change“Here’s to the crazy ones. The rebels. The troublemakers. The ones who see things differently. While some may see them as the crazy ones, we see genius. Because the people who are crazy enough to think they can change the world, are the ones who do,” is the short version of the text written by creative director Rob Siltanen and copywriter Ken Segall. Think Different became TV, posters, advertising. In various incarnations it featured the likes of Albert Einstein, Bob Dylan, Richard Branson, Muhammed Ali, Ted Turner, Alfred Hitchcock, Pablo Picasso and Kermit the Frog (with Jim Henson).
Think Different in outer space. On a bus shelter.
Littman spends his time chasing the evolution of ideas, working to understand and foster the spread of innovation and ideas, so it seems inevitable he’d ask Tanimoto how his idea happened.Thing is, Tanimoto doesn’t know: “It’s about being open to everything,” he said. “Good ideas just sneak up on you.”Think Different.More here.Google+? If you use social media and happen to be a Google+ user, why not join AppleHolic's Kool Aid Corner community and join the conversation as we pursue the spirit of the New Model Apple?Got a story? Drop me a line via Twitter or in comments below and let me know. I'd like it if you chose to follow me on Twitter so I can let you know when fresh items are published here first on Computerworld.
12 great iOS 9 features you&#039;ll probably use
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>What most people don&#039;t know about the NetUSB router flaw - Part 1</title>
<article>Defensive Computing is for people who use computing devices for work, not play. Rather than focus on the latest news or devices, this blog aims to be educational. Heavy on facts, light on opinions.
The recent NetUSB flaw in routers was written up by almost every tech news organization, yet, much of the story was untold and some of what was written was flat out wrong. Here, and in my next blog, I hope to correct the record, provide additional information and offer some perspective and context to the problem.The basic facts about the flaw are simple: some routers that share files and/or printers via a USB port are vulnerable to a software flaw that can certainly crash the router and possibly let malicious software run on it. The flaw was discovered by SEC Consult which wrote
NetUSB is a proprietary technology developed by the Taiwanese company KCodes, intended to provide 'USB over IP' functionality. USB devices (e.g. printers, external hard drives, flash drives) plugged into a Linux-based embedded system (e.g. a router, an access point or a dedicated 'USB over IP' box) are made available via the network using a Linux kernel driver that launches a server (TCP port 20005). The client side is implemented in software that is available for Windows and OS X ... The user experience is like that of a USB device physically plugged into a client system.
The bug is easily triggered, just give the NetUSB server some data longer than it expects. Truly bad coding.  VULNERABLE ROUTERS  Perhaps the biggest reporting mistake I saw was the list of vulnerable routers. The headline of one article suggested people check if their router is on the list. Another article referred to "the complete list of affected routers".But, there is no comprehensive list of vulnerable routers.In fact, there probably never will be.The list published by SEC Consult is woefully incomplete, a fact they clearly explained.The flaw is in code written by KCodes Technology and only they know who they have licensed it to. And, they are not saying anything; they did not co-operate with SEC Consult.What SEC Consult did then, was to look in a file called "NetUSB.info", which is part of the Windows driver setup. The file had references to 26 vendors. It is assumed that these companies have licensed the vulnerable NetUSB software from KCodes. But again, that is an assumption, as is the fact that the list is complete. Of these 26 companies, SEC Consult examined the firmware (router operating system) from five of them: D-Link, NETGEAR, TP-LINK, TRENDnet and ZyXEL. Each was found to have a least one router that included the KCodes software. In all, these five companies appear to have 92 routers with the KCodes NetUSB software.As for the other companies, SEC Consult "did not check the firmware of the remaining 21 vendors." Those companies are: Allnet, Ambir Technology, AMIT, Asante, Atlantis, Corega, Digitus, EDIMAX, Encore Electronics, Engenius, Etop, Hardlink, Hawking, IOGEAR, LevelOne, Longshine, PCI, PROLiNK, Sitecom, Taifa and Western Digital.The number of routers that SEC Consult confirmed were vulnerable was three. The other 89 just had their firmware downloaded and scanned.For the most part, to check whether a router is vulnerable, you need to either check with the company that made it, or test it yourself. Part 2 will cover testing a router.VENDOR RESPONSESTP-LINK, after being notified by SEC Consult of the NetUSB problem, fixed the problem faster than any other company. That said, TP-LINK, like other makers of consumer routers, drops support for their routers after a while. Thus, while they were very quick to fix their supported devices, it is possible that older models have the flaw too and won't be fixed.This brings up an important point - if your router is no longer being supported with updated firmware, the Defensive Computing thing to do is to get a new router.Lucian Constantin wrote in PC World that NETGEAR and ZyXEL have confirmed the flaw and are working on fixes.My favorite router vendor, Peplink, posted a note on their Announcements forum saying they have "verified and confirmed that none of our devices make use of KCodes NetUSB, therefore we are unaffected by this vulnerability." According to a forum posting, DD-WRT also does not use NetUSB.Linksys was not in the list of companies that appear to license NetUSB software from KCodes. A search of their tech support site for "NetUSB" came up empty.Asus too, was not on the list and their tech support site also has nothing about NetUSB. But, if you are doing file sharing on an Asus router you really need to be using the latest firmware. In early 2014 they had two serious issues in this area. One involved an easily exploited flaw and the other FTP defaults that were terribly insecure. D-Link had a single router, the DIR-615C, appear on the list. Searching their support site for NetUSB turned up nothing. Searching the support page for the DIR-615 also turned up nothing about NetUSB.Searching NETGEAR's support site for NetUSB, turned up a Product Vulnerability Advisory (last updated June 12th) that says "If your router supports ReadySHARE Print feature, your router is affected." For good luck, they also list 40 models known to be vulnerable.NETGEAR was one of the first companies contacted by SEC Consult. They have known about the NetUSB flaw since March 19, 2015. Yet they are not planning on releasing bug fixes until July. Owners of NETGEAR routers may want to insure the devices are registered, because the company plans on sending email notices when updated firmware is available.ZyXEL had four routers on the list of 92. They posted a very visible announcement about NetUSB on their website that says they will release updated firmware for all four models on June 18, 2015. They also note that no other ZyXEL devices are affected.TRENDnet had 14 routers in the list of 92. They too, posted a prominent NetUSB announcement (last updated June 9th) on their tech support site.Eight routers will have new firmware released between June 24th and July 20th (there is a definite date for each router). Two routers will be fixed, but the date has not yet been determined. Three routers cited by SEC Consult are not vulnerable because the buggy software is not present. Interesting.IS A ROUTER VULNERABLEAs noted above, the only routers vulnerable to the NetUSB flaw are those that share files or printers via a USB port. So, if your router has no USB port, you are safe.That said, not all USB ports are used for device sharing. My Peplink Surf SOHO has a USB port that can only be used for a 3G/4G/LTE antenna. It does not do file or printer sharing from the USB port.And, not all routers that offer file sharing via USB use the vulnerable KCodes software. DD-WRT, for example, claims to use software from the USB/IP Project.If a router does use the NetUSB software, it may well be vulnerable even if the USB port is empty. Next time: how to test a router for the presence of NetUSB, how to mitigate the problem on vulnerable routers, and some perspective.
Some perspective on Flash Player bugs
Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>Dropbox for Business to get mobile management boost</title>
<article>Computerworld's Ken Mingis and Keith Shaw discuss the 2015 Apple Worldwide Developer Conference...
Get our daily newsletter
At Google I/O 2015, Brian Stevens, the Google VP of Cloud Platforms, chatted with Computerworld about...
Computerworld’s Michael DeAgonia was among the first wave of Apple Watch buyers to actually get his...
Oracle reported a sharp drop in profit for the quarter just ended, with customers spending more on its...
My journey to becoming a chief information officer in higher education technology has seen its share of...
Been reading about what the cloud has to offer? We take a closer look at how you can leverage Amazon...
Here's your easy-to-use guide to dozens of useful ggplot2 R data visualization commands in a handy,...
Copyright &copy; 1994 - 2015 Computerworld, Inc. All rights reserved.
</article>
</item>
<item>
<title>It’s an Apps, Apps, Apps, Apps World</title>
<article>Today’s post is about one of our favorite topics – mobile apps.  These little jewels have become the lifeblood of the Internet – the ways in which we better utilize the resources – utilities, entertainment, information – on our mobile devices.  And as we saw clearly at CES earlier this year, the Internet of Things is powered by the connective tissue that is mobile apps.
jācapps COO, Bob Kernen, enjoys a unique perch, guiding the company’s mobile application efforts, while helping broadcasters and other marketers gain a better understanding of the mobile app ecosphere – and how any business can participate in it to their competitive advantage.
The great thing about the apps business is that it&#8217;s hot and you never know what&#8217;s coming next.  And those are the downsides, too.  Because every time you turn around, something&#8217;s changed.  Or another amazing must-have app has hit the store.  
Bob&#8217;s charge is to keep up with that change, stay ahead of the curve, and let us know what&#8217;s what in the app space.  Today he gives us a look at apps with the help of a little outside research.  – FJ

The other day I was reading a story about Apple and the takeaway was that one of their biggest challenges moving forward is how to top the success its had with the iPhone 6 and 6 Plus.  That&#8217;s a great  problem to have.  The more research I see, there&#8217;s more evidence that mobile apps are the currency of Web 3.0.
If the fact that 86% of smartphone usage is on apps, compared to only 14% on browsers, isn’t enough to convince you about how crucial mobile apps are to business in 2015, research firm Fiksu recently reported that app downloads hit a record high for the third straight month.  The Top 200 free iOS apps were downloaded 9.2 million times.
So the good news is that Americans continue to use their mobile devices more and more.  Time spent is up, while apps per device and apps used daily are also ascending.
New data from Nielsen confirms this.  App access and usage continue to rise nearly seven years since Apple hung the &#8220;open&#8221; sign outside its App Store:

But here is the dark side of success:  the cost to market those apps, drive downloads and usage, particularly over time, is growing, too.  It now costs, on average, $2.10 to motivate a single user to download and then use an app at least three times.  The cost per install has also grown to about $1.50.  So for the average mobile app, acquiring and retaining a large number of users – the kind of numbers that make that app monetize-able – has gotten pretty expensive.  And the sheer glut of mobile apps makes it even more daunting for marketers to grab the attention of all those smartphone owners.
But radio mobile apps aren’t “average” mobile apps.  That’s because they come with a built in, 24/7 marketing channel called your radio broadcast.  For a station with its own app, there is an incredible opportunity to encourage consumers to download and use your app.  That megaphone is something that other brands have to spend big bucks in promotion and marketing in order to match.  But from what we’ve witnessed, stations tend to give their app a big push when they first go live, then move on to other promotions, and then wonder why their installed base isn’t growing a whole lot.
This is about more than just devoting on-air time to promote your new app.  What really makes the effort to build a great audience on mobile is giving your listeners reasons to engage with the app.  Invite them to connect with you via social media, give them information on upcoming promotions, concerts and other events, let them learn more and talk back more.  Create an ongoing context for this, so that it isn’t something that’s “special” but, rather, an everyday part of the communal flow of your programming.
There’s another angle here, too.  The Edison &#8220;Share of Ear&#8221; shows that outside marketers in need of fast app downloads should look no further than AM/FM radio as a solution.

Your salespeople could be marketing your station&#8217;s ability to harness the power of what Edison calls “The Reach Monster.”  Is there a more efficient way for any consumer brand to generate downloads than radio?
We see the metrics every day here at jācapps.  When stations are promoting their apps, downloads immediately spike.  When they do it over a period of time – think about a “campaign flight” – downloads are beyond impressive.
So whether it’s for your own app – or as a way to help advertisers drive downloads for their apps – there’s no better, more efficient, and more effective “megaphone” than broadcast radio.
Broadcast radio and mobile apps are a great pairing.
Email Bob Kernen here.
</article>
</item>
<item>
<title>Missing The Podcasting Boat?</title>
<article>
Just when you thought public radio and podcasting were on a serious roll, KPFT/Houston programmer Ernesto Aguillar has thrown cold water over both. In fact, his recent blog post reads more like the ice bucket challenge.
I encourage you to read it here.
While public radio may be leading the league in podcast downloads and is often cited as a shining example of success in the podcast space, Aguillar sees it differently. He posits that public radio podcasting is more about repurposing existing programming in an on-demand format.
In fairness, NPR has been especially successful in producing and marketing its podcasts.  A recent article in Wired points to the network&#8217;s ability to attract younger audiences while generating much-needed revenue. In fact, NPR podcasting dollars have doubled since 2014.  And aren&#8217;t the poster children of radio podcasting and the shows that are energizing the space  – Serial and Invisbilia  – amazing examples of podcasting done right?
But Aguillar&#8217;s point is that rather than innovating with a different sound, alternative styles, and more varied content, the podcasting rule of thumb in public radio has generally been to simply make on-air programming available for podcasts, and thus, failing to take example of this experimental exciting space. He also contends that public radio content creators are disconnected from those who have been on the ground floor of the podcast movement.
He could have been talking about commercial radio, too. The difference is that at least public radio podcasts are being accessed at a record rate. NPR and its various shows and programs have held leading positions in the podcasting rankers, year after year.
But the lost opportunity in podcasting for broadcast radio – commercial and public – may be the inability to create fresh programming that isn’t already on the air; on-demand content that sounds different from the norm, and that connects with audiences who may not be enamored with or who aren&#8217;t spending much time listening to broadcast radio.
When you consider how commercial radio has approached podcasting, it essentially falls into the pattern that Aguillar talks about in his article. Simply posting hour #2 of the The Z Morning Crew may make it more convenient for fans of that show to access it on-demand, but it does little to expand listenership and engage consumers.
In many ways, the podcasting conundrum is eerily similar to what has played out with streaming in the broadcast world. Rather than envisioning streaming audio as the chance to try different approaches, alternative programming, new business models, and some much-needed experimentation, most streams are simply simulcasts of what’s already on the air.
The Wired article notes that the hot podcasting sector is attracting independent producers who are already invading this sector.  As has been the case with streams and connected cars, there&#8217;s a hole in the fence inviting anyone and everyone to try their hand at creating content.
So for broadcast radio, podcasting is looking like déjà vu all over again. Amplifi Media’s Steve Goldstein said it best in a recent “Blogstein” post:
“Shoving content on a new platform is a lazy wish. Each medium requires a different content focus, approach and calibration.”
Radio could do so much better. Behind the scenes content, different styles of programming, a deeper dive into talent and who they are as people are all part of a podcasting content strategy that holds great promise for radio broadcasters. Simply presenting the same warmed over programming in accessible, bite-sized segments fails to live up to the true potential of podcasting.

Working with personalities, teams, and shows to reimagine this space, and what it could offer audiences and brands goes to the heart of rethinking this burgeoning opportunity.
NPR has used its savvy digital team and its vision to make amazing progress with podcasting over the past decade.  Now that on-demand radio is becoming an established content avenue, the bar has been raised.
As broadcast television has learned, content creation in the on-demand format has invited some unlikely players.  To think this space would be led by a company whose business model was once about sending DVDs through snail mail in little red p0uches speaks to the notion that the next big radio podcast could come from an unusual suspect.
Podcasting is one boat that broadcasters can&#8217;t afford to miss.
What does it say about the competence of many commercial broadcasters when they still seem so clueless about monetizing streaming and podcasting?
That the industry needs to take a deep look at itself, especially at a time when revenue generation is challenged.  These other outlets provide opportunity to grow brands, participate in exciting new spaces, AND make money.  Thanks, Dimitri.
There appears to be no way for commercial broadcasters to make real money from podcasting. This is no different than the mistake print made when they put subscribers&#8217; content up on the Internet for free. Subscribers dropped their subscriptions and just read the news articles for free on the Internet. By basically giving our content away for free, on demand, we are allowing our listeners to get our content without being compensated for it. We are also doing our advertisers who advertise during our live shows a disservice. We are enabling listeners to not listen to our commercials. it is also easy to say that we should just create additional, unique content for the podcast. If the content is good enough to put up on the Internet, we should just put it on the air and sell commercials around it. Our on air talent only has so much time and effort to devote to creation of content,
Scott, you outline some of the tough choices that broadcasters face with many digital touch points, from streaming to podcasts.  Ultimately, popular blogs are monetizable as NPR has proved.  Perhaps the key is to create content that isn&#8217;t right for the air or for formats where long-form talk just doesn&#8217;t work.  Again, some difficult decisions but I like to believe that with strong strategies and planning, it can be done.  Thanks for taking the time to comment.
I am a big fan of podcasts — both original content and re-purposed content. I can&#8217;t listen to the radio all day, so I like the on-demand aspect. I also loved the specific topic podcasts that NPR used to produce (their pop culture podcast and their tech podcast), but have since discontinued due to NPROne. I am happy to see the offerings grow and I think there is space for many more.  Thanks for writing about this topic.
And there&#8217;s more to come.  Thanks, Sarah, for taking the time to comment.
Good insights as always, Fred, but the statement that &#8220;simply presenting the same warmed over programming in accessible, bite-sized segments fails to live up to the true potential of podcasting&#8221; sells short the fact that we had more than 450,000 audio content downloads last month at our local news and sports websites. And we&#8217;re testing a monetization model that will unlock six figure revenues against it. So the point here is that when you create really good, exclusive and compelling core content, there&#8217;s no shame in making it available on demand. And when you can monetize that without the resource investment and risk potential of creating original, unproven content, it has to be a top priority. This is not to say that we&#8217;re uninspired by the chance to &#8220;reimagine the (podcasting) space.&#8221; Guess we&#8217;ll just frame it as &#8220;the genius of the AND&#8230;&#8221;  Keep your thought provoking &#8211; and industry improving &#8211; posts coming&#8230;
Jim, that&#8217;s right and I may have been &#8220;painting&#8221; with too wide a brush.  Different brands are going to come up with the solution that fits and works for them, as you&#8217;ve done in Phoenix.  I&#8217;m looking at the broader space and potential that podcasting holds, and hope that radio pushes harder.  Thanks for the kind words and for reading our blog.
Yes, NPR slings broadcast programs online as podcasts. I listen to On The Media, and it&#8217;s strange and a little lazy hearing the furniture of a radio show in there:
&#8220;Next, we&#8217;ll hear from a man with a typewriter. This is On The Media, from NPR. [plinky plonky jazz fades] [gap] This is On The Media, I&#8217;m Brook Gladstone, now &#8211; a man with a typewriter&#8221;
But NPR&#8217;s &#8220;NPR One&#8221; product is the direct opposite of that &#8211; taking public radio and producing something truly interactive with it. I talk a lot about &#8220;atomising&#8221; (smashing your hour-long talk radio shows up into segments, with appropriate metadata) and &#8220;lego-bricking&#8221; (taking those small segments and assembling something personalised and new). NPR One does both &#8211; AND it&#8217;s feeding back data to broadcasters, to help them write better cues and stories.
NPR One proves that the broadcaster is capable of so much more. Which is why I feel that pointing the finger at NPR is in the wrong direction.
Point it at Apple, who are spending bucketloads of cash on a non-interactive linear stream destined for the most interactive device in your pocket. Point it at the BBC. Point it at lazy &#8220;best of&#8221; compilations from commercial stations. But don&#8217;t point it at NPR.
Insightful post as ever, Fred.
Thanks as always, James.  I thought Ernesto was tough on NPR and said so.  And as you know, the NPR One app has been a favorite in this space to the point where someone asked me why I always write about it.  NPR (and other public radio producers) have shown the way for all of broadcasting.  And the potential is there to aim even higher. 
Thanks so much for taking the time to read and comment.
NPR&#8217;s innovation or lack thereof not withstanding, Ernesto makes a good point regarding original podcast content.  Traditional broadcast radio produces some great shows that transition nicely to on demand listening but the truth is currently there are not enough of those shows.  Is that because they aren&#8217;t being promoted properly?  Or produced in a way that is not conducive to on demand audio as it is currently accepted?  Those are certainly factors.  The truth is whether it is one of the large radio ownership groups or NPR the world of podcasting and on demand listening are an alien world to most &#8211; listeners included. Radio boasts about the ability to be &#8220;local&#8221; as one of it&#8217;s greatest assets but the most popular podcasts, Serial in particular, are national. The winning combination, in my mind at least, is to offer up a combination of local oriented and national content, stuff that has such wide appeal that anyone in demo would be interested in hearing it.  For example an AC station could have a &#8220;behind the scenes&#8221; of the morning show podcast but also offer a podcast of longer form celebrity interviews with Ryan Seacrest, Billy Bush, etc.   Similarly a classic rock station could produce a long form &#8220;Behind the Music&#8221; style series about national artists they play terrestrially that would be of interest to their local listeners but also bring in people from outside the market as well.  An urban formatted station could do something similar with local acts in the market which would be most appealing to local listeners.  The point is the on demand audio world has to be treated like what it is and that is a new product, not a quick way to make a buck by doing the same thing online that is done over the air. A radio show that is also broadcast on TV adjusts to work in both mediums and this is the same situation.  And while radio must leverage the content it already produces, that content needs to be tweaked to fit the format, and the existing content must be paired with ground breaking, format appropriate, products to not only hold the interest of listeners but keep them coming back to hear what is new.
Thank you for another insightful blog Fred.
Sean, you hit on some key points about the challenge of podcasting.  The space requires a new set of strategies, and you bring up good examples of how stations/personalities might approach creating original content for the podcasting space.  The reality is that on-demand audio/radio could be a great place to build brands and make money.  But rehashing the existing material to just create convenient podcasts fails to realize the potential.  Thanks for taking the time to comment.
Great insights, Fred.  Currently, a virtual Berlin Wall exists between pub radio and commercial broadcasters.  Podcasting presents a great opportunity for &#8220;both sides&#8221; to learn from the other.  My tastes include both.  Our new How Do We Fix It? (Just launched) is an example.
I will check it out, Richard.  Thanks for the comment and the insight.
</article>
</item>
<item>
<title>Speed Thrills</title>
<article>We have talked a lot in recent months about the need for speed.  As we continue to be inundated with content from seemingly infinite sources, our ability to process all that entertainment and information erodes.  And because many of us are constantly running – devices in hand (or on our wrists) – we have even less time to take it all in.
You may find yourself reading fewer long-form articles or stories, online or in print.  And oftentimes, maybe the headline will simply do.
Wearers of smartwatches are experiencing this phenomenon as they adjust from having to pull a mobile phone out of a pocket, purse, or briefcase/backpack to simply glancing down at their wrist.  This may seem like a minor convenience, but after some time, the ability to be up-to-date without being distracted by reaching for a phone comes more natural.
When you see the headline “American Pharoah wins the Triple Crown,” do you really need to read the entire story?
Or when your watch displays “Carrie Underwood Dominates 2015 CMT Awards,” you pretty much know the story without having to sift through a 300 word story to read the details.
Whether it’s an email notification, breaking news, a ratings update, or a message from a co-worker, most of us are thinking, moving, and breathing in a form of digital shorthand.
We’ve been experiencing that sensation since the Apple Watch arrived at jācapps World Headquarters two weeks ago.  As you might imagine, we&#8217;re in the process of developing apps for this platform.  And the way you get a true sense for what it is, what it does, and how it feels is to wear it.
Several staffers have taken their turn with this new gadget.  And while there are different “takes” about its overall value and “game changeyness,” there is some consensus that Watch’s notification features allows wearers to be even more in touch than you are with a smartphone.
Aside from the health features, perhaps one of the most prominent positives is the gadget’s ability to quickly and effortlessly let us know what’s going on without lifting a finger or even raising an eyebrow.
And that combination of brevity and speed leads to a streamlined experience that many of us crave, whether we’re totally aware of it or not.  And that shrinking envelope where information now fits on the face of a Watch goes to the heart of how our brains are being reprogrammed at digital speed.
And the compressed envelope for both content and time is permeating our day-to-day media lives, especially when we take on the role of content creators and marketers.
Pandora’s chief revenue officer, John Trimble, tells Adweek that 8-second pre-roll videos for their “Sponsored Listening” ad format is the ideal length for mobile video consumption.  That would chop the current 15-second length in half, essentially pushing advertisers to communicate their messages in a much quicker window.
In the same story, Greg Manago of Mindshare says his team is up to the task, comparing Trimble’s concept to Vine’s 6 second length:
&#8220;The trick with mobile is to be engaging quickly without being annoying.  You need to be very creative at this length, but we love a good challenge.&#8221;
Actually, isn’t that the trick with everything?
Whether it’s getting our point across on Twitter or delivering a great morning show break, finding ways to connect and be compelling in less time isn’t just smart for PPM meters but also to satisfy listeners as well.
That doesn’t mean talking less.
It does mean making every break count by prepping, planning, and being mindful of the audience’s increasing impatience for content that doesn’t deliver or that develops glacially.
From on-demand programming, like Netflix and podcasts, to content-skipping features in apps, like Pandora, Spotify, and NPR One, the typical listener who used to turn it on and leave it on is losing his ability to focus on programming that fails to deliver in a timely way.  Spotify&#8217;s Paul Lamere tells us the likelihood that consumers listen to songs in their entirety on his service is only around 50%.  It&#8217;s that digital itch that nags at many users, motivating them to flit around to find that next piece of content that will entertain or inform us.
And yet, great content that is well-written, planned, and produced turns that notion on its head.  Many public radio shows prove that every day, breaking all the rules to present programming and features that go well beyond mind-numbing, repetitive speed breaks and other commercial radio rules.  When you’re a consistently good storyteller, audiences will buy in and hang in.  It doesn’t always work for public radio, but regular listeners will tell you that the payoffs are there more often than not.
So it doesn’t mean that long-form programming is no longer viable.
It does mean that interviews and extended segments must engage and compel.  Fortunately, this happens every day in commercial radio.  In the last two days, I’ve found myself listening to interviews that have kept me in the car (Drew Lane interviewing Jeff Daniels on Detroit Sports Radio 105.1) and late for a meeting (Ashley &amp; Drew interviewing the mother of a daughter who had been missing for 72 hours on KKPT/Little Rock).  These moments need to happen more often.
The stakes are higher.  The bar has been raised.
Less isn’t necessarily more.
But more is rapidly become less.
JacoBlog always spot on! Headlines, early &amp; often, instant and connected build the package.  You&#8217;d think there&#8217;d be a live editor working 24/7 to revamp All Broadcasting.  Shame on decent companies with an FCC license (especially big metro signals and indy TV) who doesn&#8217;t get it, deliver it and sell it. Thanks, again. http://www.broadcastideas.com
Thanks, Clark!
</article>
</item>
<item>
<title>Radio’s Most Innovative: GeoTraffic</title>
<article>
Perhaps there are three sure things in life – death, taxes, and traffic. That last dependable annoyance has been a financial boon to radio for decades. But all that may be changing.
At last year&#8217;s DASH Conference, Edison Research President Larry Rosin delivered a presentation called “Changing Route: The Future of Traffic Reporting” where he demonstrated how this long-time staple of radio content and revenue is rapidly becoming less important to an audience that requires both traffic relevancy and brevity.
From newspaper want ads to radio’s school closing reports, the Internet has disrupted the flow of information to consumers, along with the sources from which they receive it. Radio traffic reports, he warned, may not be far behind.
Rosin talked about the impact of smartphone apps and morning TV news shows on the consumer’s need of and usage for radio traffic reports. But perhaps the most poignant piece of the presentation was when he illustrated that as information is becoming more personalized for consumers, radio traffic reports still only cover the most traveled routes in the market. For many drivers, that’s a lot of information to sit through, with the good chance of not hearing traffic information for their routes.
Rosin warned that unless broadcast radio starts meeting on-demand, relevant consumer traffic needs, this content and the revenue that comes with it will be challenged, and ultimately lost.

Ironically, GeoTraffic CEO Frank Rizzo also appeared at that same DASH conference. His product addresses many of the challenges Rosin raised. GeoTraffic recently announced their platform’s “personalization” feature is out of beta and ready for public release. Thus, this seems like the right time to talk with Rizzo about the inner-workings of GeoTraffic, and its ability to use innovation to ensure a future of radio traffic reports that is both relevant and profitable.
JM: What weakness did you see in traditional traffic services that led to the creation of GeoTraffic? 
FR: Traffic is the last untapped information space. Weather, finance, sports and news are all one touch away, can be personalized to some degree, and are available on-demand in a rich media format. For some reason, traffic – despite its highly valued hyper-local content – has been ignored. There was no on-demand, human voiced traffic offering.
So we set out to build a model that focused on delivering personalized traffic for the connected car. We built it with a vision of ubiquitous cross-platform delivery, meaning it is available regardless of whether the in-car system was “Built In” or “Brought In.” We feel that&#8217;s especially important as Apple and Google continue to announce OEM deals with CarPlay and Android Auto, creating a huge opportunity to gain a front row presence with a “Brought In” solution.
JM: How does the GeoTraffic platform work with radio?
FR: As radio loses its monopoly on the dashboard, it needs to stay relevant by offering consumers what they want, when they want it. Traffic is the one constant that can provide stations with a continued presence “in-cabin.”
After years of being trained to get “Traffic on the 2’s” at specific stations, listeners can now be directed to a station’s digital platform for that information. By moving listeners from broadcast to digital and back, stations can keep listeners on the channel where they started.
That makes our platform an obvious addition to News Radio, but we also believe it means music stations can now compete for this valuable share of ear and revenue, while for the first time, maintaining the integrity of their programming schedule.
And our offering is not limited to digital. We can also provide a broadcast solution in tandem. But the long-term value for radio and us lies in the digital platform. By providing an enhanced product offering, we are creating a new and valuable revenue stream with no out of pocket costs, which seems like a pretty exciting opportunity for radio.
JM: What’s the financial structure?
FR: Our business model for integrating with broadcast radio’s digital platform is based on a revenue share of our valuable geo-targeted advertising model.
Location-based advertising is the future (actually, it is the present) and there are all types of offerings that provide some form of “geo-targeting”. But we are unaware of any that provide it on a road-by-road basis in a live, human-voiced report. We have retained the value of people talking and we offer a terrific product enhancement, along with a potentially large revenue stream.
JM: How does the “personalization” part work?
FR: Our API enables radio apps the ability to provide a frictionless experience where the user can have a geo-targeted road report delivered and then seamlessly placed into their desired content.
Our user interface is a simple, clean design. The driver chooses the roads they travel on their commute and saves them in order. Once they do that, a “My Report” icon appears and a personalized traffic report is available on-demand.
JM: How does your system address concerns about distracted driving?
FR: Our current iFrame delivery requires drivers to click the “My Report” icon, which is no different than hitting a traditional preset button. And our upcoming V3 release will be voice activated on a number of devices.
All the reports are in an audible form, meaning the driver can “steer and hear.” Also, depending on the type of integration by the host platform, some reports can be automatically delivered to the driver based on preset conditions like severity and distance. This requires no action of any kind. It just delivers a non-intrusive news segment, permitting the user to stay on their chosen platform.
JM: What have been the biggest impedements you’ve had to overcome with your new platform? How did you get past it?
FR: There have been a number of hurdles that we have had to find a path over, under, or around to get here. The most obvious was developing the required technology. Fortunately, my partner Gotce Peev joined us as CTO and went right to work. He saw the vision from the outset and has built out our platform, brick by brick. “G” is an out of the box thinker and was able to guide us over and around issues that at times seemed insurmountable.
As a cross-platform delivery system, we had to ensure that integration was simple and seamless, and most importantly, that we provided choices for both our partners and end users. We knew it would be presumptuous to assume to know what you want, so in trying to satisfy everyone, we needed to build a platform with a wide selection of options.
We accomplished that and our reports can now also be overlaid on existing mobile apps, delivered seamlessly in stream on Internet radio, integrated into navigation systems &#8212; built in or brought in &#8212; on websites.
JM: What is the role of the traffic reporter and other staffers on the ground?
FR: We begin the process of preparing a report where data ends. That means our market teams take the information that our engine provides (which includes private and public feeds, along with other information sources) and begin the requisite curation before delivering a report.
It is important to note that one of the flaws we identified with current traffic offerings is that they are essentially just a pure data feed with sometimes questionable reliability. By using a number of different sources to curate and deliver road reports, we can hyper-focus on “hot spots” and continually update our listeners on developing traffic conditions.
JM: Traffic has been a revenue stream for radio for a very long time. Do you see that coming to an end anytime soon?
FR: I would defer to advertisers to answer that question but from our perspective, a couple of facts are pretty clear.
Users are migrating away from broadcast radio for traffic. Reports that are market overviews just don’t work in the on-demand world when they may not even cover the roads I travel. It’s still news, but it’s market news – not personal news. Meanwhile, advertisers are increasing the dollars spent on digital platforms while the amount spent on broadcast is moving in the other direction.
Radio does not have to simply surrender this revenue stream as it still has a massive audience base. Instead of letting listeners migrate to another source, give them a better offering on a station’s digital platform so you can retain, if not, increase the revenue. Just consider the upside of offering your existing advertisers a geo-targeted live read sponsorship. The revenue model is really exciting.
Plus, the traffic information is more accurate and personalized for your users and can include a human-voiced weather report, contest information, special event traffic, and more. It’s a chance to grow your digital platform at no cost, other than letting your listeners know it’s there.
JM: Can radio compete with apps like WAZE?
FR: Radio needs to provide a better, more accurate offering because the most accurate product will be the most successful. In order to ensure we cover the subjective wishes of many, the platforms also need to provide choices. We have met those challenges and that offering is available today.
JM: How does the advent of autonomous driving impact systems like yours?
FR: This brought to mind the Richard Jenni joke about how the advent of the window impacted weather forecasting. Autonomous driving is certainly interesting, but I think we are a long way from an autonomous car in every driveway.
And personally, I’m not sure autonomous driving will result in being told which roads to take. Those decisions will still vary and require the element of human logic to find the best path. As I said, data is terrific. Augmentation of data with human intelligence makes it better, at least as it pertains to traffic.
JM: What would you say to someone who has an idea that could revitalize a part of the radio industry but isn’t sure how to get started?
FR: From my perspective an idea isn’t enough. GeoTraffic is well past a proof of concept and we&#8217;re still struggling to get radio’s attention. You need patience and a belief in your offering.
And you should build out your offering to work on all platforms that radio now competes in to ensure a wide target market. If it adds value to the various platforms, traction will come. Maybe not where you initially expected it, but eventually the value becomes obvious.
&nbsp;
Thanks to Mike Stern for writing this week’s RMI.
&nbsp;
&nbsp;
Check out some of our “Past Innovators” here:
</article>
</item>
<item>
<title>From the Mouths of Interns</title>
<article>There’s a lot of online and water cooler chatter about how the new Stephen Colbert version of The Late Show will fare. But Mike Stern is still pining over the end of David Letterman’s amazing run. The final show was a winner, as was that very last &#8220;Top 10 List.&#8221; Here’s a guest post that’s all about where great ideas come from. Prepare to be surprised. –FJ
On the night he retired from late night television, David Letterman’s final &#8220;Top 10 List&#8221; was a work of art. If you missed it, the topic was “The Top 10 Things I’ve Always Wanted to Say to Dave” and each item on the list was delivered by a different celebrity.
Talk about pressure to hit it out of the park. Alec Baldwin, Peyton Manning, Barbara Walters, Steve Martin, and Chris Rock were among the top ten friends of Dave who showed to deliver the final list live:

The winning line by far belonged to Julia Louis Dreyfus who, with Jerry Seinfeld already on stage, said, “Thanks for letting me take part in another hugely disappointing series finale.” Ouch!
But there were two entries that were especially interesting.  One was uttered by Tina Fey who said, “Thanks for finally proving men can be funny.”  The other came from the very last celebrity, Bill Murray, who put a cap on the bit with “Dave, I’ll never have the money I owe you.”
The reason these two quips are the most intriguing is because in a Tumblr post, longtime Letterman writer Bill Scheft revealed they were written by Caroline Schaper, the writing staff’s intern.
Created by young Schaper, Murray’s line was ready to go before the show was taped, but at the rehearsal, Fey wanted some additional options to consider beyond what was originally written for her. That’s when Schaper scored a second win by supplying her great one-liner.
And the net-net of this momentous broadcasting event was than an intern wrote 20% of the jokes fired off on this final “Top 10 List.”
What a fantastic story and resume builder for Schaper, and what a great reminder for all of us that the best ideas don’t always come from the manager’s office or from the Program Director. They can come from just about anywhere, and with so many stations running lean, radio needs that additional creative spark – no matter where it originates from.
It could be a summer intern, a part-timer, a sales rep, someone from the office staff, or yes, even a listeners (but probably not an engineer).  The bottom line is that tapping into people outside the regular team for feedback, opinions, and new ideas may help your station sound its best.
Just ask Letterman’s staff of writers.
Or better yet, ask their intern.
The legendary ad man Jay Chiat used to say that great ideas could come from anywhere, even the account executives. Whether that quip was meant to inflate or deflate the AEs only the late Mr. Chiat knows. But some of the most memorable Chiat/Day ad campaigns were created, or inspired, by people who worked outside the boundaries of the Creative Department. The award-winning campaign for Olympia Beer, featuring tales about the Artesians who tended to the beer&#8217;s artesian wells, was the brainchild of&#8230;wait for it&#8230;an account executive. You don&#8217;t need a license to be creative, and you certainly don&#8217;t need a title. Any company, or industry, that believes the opposite probably believes its competitve edge comes from its typewriters and slide rules.
Jay, thanks for the great Chiat story and taking the time to add to the conversation. Especially at this juncture, we should be willing to consider every idea, regardless of its source. Thanks again.
</article>
</item>
<item>
<title>Socially Speaking</title>
<article>A new service that measures social media impact by media brands is an eye-opener. And it suggests that a smart social strategy trumps most other variables – including the generational appeal of a station’s audience.
Net News Check reported on a new study by Shareablee, ranking the top 10 media brands in the Detroit Metro, based on social media activity. Shareablee CMO Tracy David describes the company as “the leading authority on audience intelligence, competitive benchmarking, and actionable insights for social media.” They aggregate social activity on the biggest social platforms, and work for brands like ESPN, HBO, Bloomberg, and P&amp;G to assist companies with their social content marketing.
And now we’re seeing data that can help us better understand how local media performs socially over a six month period. The results for Detroit, as you can see below, are both predictable and startling.

It’s not surprising to see TV stations and newspapers at the top of the heap. They have massive reach and they’re tweeting and posting news as it breaks throughout the day.
The interesting story is the radio stations that showed up in the Top 10. Hot 107.5 (Hip-Hop), 89X (Alternative), and Channel 955 (CHR) are talking to social media natives, so you’d expect them to be on the list. The Ticket is Detroit’s dominant Sports Radio station, and Jacobs Media clients know the connection between sports fans and Twitter, in particular.
Now to some extent, these are vanity metrics.  You can think of them as the 6+ ratings you see in the trades.  They don&#8217;t tell the whole story, but they do give you an indication of who&#8217;s having impact socially.
But WCSX’s presence on this list is fascinating. For a Classic Rock station whose average age is well north of 45 to crack the Top 10 says a lot about social media strategy and execution. It’s even more impressive when you consider they fall below 50,000 fans/followers, but still make this list.
So how do you explain how a radio station can be successful socially, whether they target Millennials or aging Baby Boomers.  WCSX PD Jerry Tarrants notes that you don’t need to appeal to a Gen Y audience to ring up strong results:
“I think by taking into account that even though our listeners are people with ‘analog values,’ they certainly aren’t idiots when it comes to social media platforms. Sure, they’re not blazing around the Internet, ear buds plugged in with 12-15 windows open and a Snapchat convo with their bestie all simultaneously going on at once. But they’re certainly capable of seeking out and absorbing content that’s tailored to their interests. Like the music they listen to, they prefer familiarity over variety and that’s what we give them with our content.”
Shareablee’s Tracy David agrees that CSX’s social activity is congruent with what their audience cares about:
“Success on social really comes down to the quantity and quality of your content. Knowing what resonates with your audience is critical for generating the kind of quality activity and interactions that can drive your overall business objectives. WCSX taps into nostalgic and memorable moments that encourage its social audience to like, comment, and share its content with friends and family. The station posts content that lets audiences re-live experiences, touching upon deep emotions. This kind of content that gets consumers to re-live stories and create narratives is a fantastic driver of engagement.”

J.T. gives major kudos to the Greater Media Detroit digital team, especially when it comes to providing content they can use that triggers meaningful social activity. Most of their activity is on Facebook. CSX has been a stakeholder in all eleven of our Techsurveys, allowing them to create a strong “sociall footprint” that guides their activity. The station dabbles and experiments with Twitter, and will soon be playing in the Instagram pond.
Tracy David says that’s a smart move: “Instagram was the fastest growing platform in 2014 with a 131% growth, and engagement on the platform has no signs of slowing down. For local media, the growth of engagement on Instagram in 2014 was 343%, outpacing that of the average U.S. brand. For DMA 1-20, content published on the platform grew by 153% in 2014, with engagement outpacing content at a 329% growth. This means that audiences are eager to engage with content they find relevant on the platform, offering a lot of opportunity for local media brands to reach more of their audience.”
It’s important to know your audience, especially the evolving Classic Rock fan. As J.T. notes, “Classic Rock has become so multi-generational now that the music isn’t the only thing shared between parents and their children. Parents want to see what their kids are up to so they have no choice but to learn how social media works. The cool thing about this is now that the parents have somewhat mastered it, they’re also using it recreationally as well. Odds are they will seek out something familiar. That’s where you come in.”
One last point revolves around that ongoing issue of station pages versus personality pages. You’ll note the absence of stations like WRIF on this chart. And yet their morning show – Dave &amp; Chuck the Freak – has an amazing track record of hitting it out of the park socially. On Facebook alone, they have nearly 160,000 fans.
Shareablee’s David explains it this way: “We approached this scorecard in the same way we do our TV rankings, where we measure the TV shows separately from the TV network. Radio stations are often looking to drive program discovery and drive traffic to their sites, so we&#8217;re looking at how well these station brands do this, separate from their various programming. Also, some stations might have more programs than others so this makes it a more apples-to-apples comparison. That said, we can always rank radio programs separately, similar to our TV show rankings.”
Perhaps moving forward, Shareablee will do just that. In the meantime, they will be releasing more DMA rankers soon, so more and more of you will have a lot to talk about.
As David reminds us, “There is still so much confusion about how to measure social, our primary goal is to work with our industry to shed light and facilitate clearer optics and transparency around social media.”
As both she and J.T. agree, the focus has to be on the quantity and quality of social content. In WCSX’s case, their performance is impressive, and hopefully serves as an incentive for other stations that may feel they’re behind the curve or that their audience isn’t “into” social media.
Socially speaking, there’s an art to succeeding with social media.
And now, more metrics in local markets to better understand the space.
But it always comes down to knowing and implementing smart social media fundamentals.  And that translates to developing a strategy, knowing your audience, being consistent, and great storytelling.  As time goes on, we learn more about the arts and craft that goes into being successful socially.
So where does your station stack up?
When it comes to measurement, the engagement is great, the core business is revenue driven. Business&#8217;s are in business to make money, so how does that social interactivity transfer to ROI&#8230;that&#8217;s the true question. The warm fuzzy feeling is great but how does that convert to listeners and customers???
That is the challenge.  Using social to drive traffic back to websites and the air.  And that&#8217;s the strategy piece that is so often missing. Thanks, Billy.
NetNewsCheck is trying this as a weekly feature, and posted its second market (Baltimore) a few days ago.  This one may be even more surprising, as #2 overall here is CCM outlet &#8220;Shine FM&#8221; (WRBS).
http://www.netnewscheck.com/article/41561/wbff-tops-baltimore-media-in-social-activity
Christian radio is a force, Eric.  Thanks for mentioning NetNewsCheck&#8217;s next market.  More to come.
</article>
</item>
<item>
<title>In Praise of Carbon-Based Lifeforms</title>
<article>Millions of people around the globe were dialed into yesterday’s anxiously awaited series of announcements from that little computer company.  But perhaps no one was more anticipatory than Bob Kernen, jācapps’ COO. Much of the activity surrounding our mobile apps company is predicated on what Apple is thinking, scheming, dreaming, and doing.  So ironically, Apple’s big day yesterday had a great deal of application to the world of broadcasting. You can read Bob’s guest post below. &#8211; FJ
I just finished watching the annual Apple Worldwide Developer Conference keynote. It’s amazing how Apple has made the pedestrian notion of a product announcement – something most companies handle with a press release and a round of media interviews – into must-see-TV.
With production values at the level of an arena rock concert, this nerd-a-palooza is an event watched by more than just your average hipster or Apple fan boy. CEO Tim Cook has stepped nicely into Steve Jobs’ shoes, and emcees with plenty of charm, if not the magician’s showmanship for which his predecessor was legendary.
Among the expected updates to the Mac OS, iOS, and now (for the first time) WatchOS, was the much-anticipated announcement of Apple Music. This new product has been in the works since the acquisition of Beats last year. And sure enough, there on stage (along with Drake and Trent Reznor) was Jimmy Iovine unveiling a new challenger for Spotify, Rhapsody, Rdio and the rest.
This isn’t a game-changer in the way that iTunes was way back in 2001 (Wow!), but as anyone who has followed Apple over the years knows, they’re less famous for inventing than they are for perfecting. Apple didn’t invent the personal computer, or the mp3 player, or the laptop, or the smartwatch, or the tablet, or the smartwatch. But they sure know how to build a better mousetrap.
This new service is more than a personalized cloud jukebox like its competitors features, and one of the things Apple will rely on to differentiate themselves in this very crowded space isn’t silicon-based, but rather carbon-based: people.
At the center of the new service are the kinds of professional curators that have largely been dis-intermediated since the emergence of Pandora and its algorithm. First and foremost is Beats1, a “global 24-hour radio station” (yes, they used the word radio) that brings together a number of top DJs from around the world, creating live (yes, live) linear music.
There are also human curated playlists. These are not unlike the user-created ones on the other services, but part of the value proposition here is that these collections of songs are crafted by people who know music, and who have credentials and credibility beyond mere fandom. It&#8217;s called programming.
And finally, Apple Music is offering Connect, a way for artists to reach out directly to their fans, providing users social interaction, exclusive content and a glimpse inside their everyday lives.
Apple is acknowledging that there is more to the relationship people have with music than can be addressed by a mere machine. They are listening to a growing number of music fans who may not be prepared to reject the algorithm, but are ready to acknowledge that music has always been about human interaction.
Spotify’s recent integration of podcasts – spoken word content – is yet another indication that pure-plays and playlist services cannot survive with music alone.  This is something that has been clear to the Jacobs Media side of our offices for many years now.  It is fascinating to see this reality now being affirmed by the globe&#8217;s biggest media and technology companies.
Like John Connor, this is a message from the future that those carbon-based lifeforms who talk into microphones, differentiate your stations, program special and feature programming, and play your music selections are not going to be obsolete any time soon.
In many ways, these modern digital media services are acknowledging the value and efficacy of old school radio. Now it’s essential that broadcasters hear that same message about the values and attributes that got FM radio to the dance decades ago.
There has never been a more important time to invest in great people, on and off the air, and yes, behind the scenes.  It is essential for radio to empower its programmers and air talent to make that connection between stations and those other carbon-based lifeforms – listeners.
This is fascinating. Apple seems to realize that its not Spotify that needs perfecting, it&#8217;s RADIO. There isn&#8217;t much you can do to make songs on demand and on self generated playlists better. But in the area of syndicated talent, there&#8217;s room for a lot of improvement.
Apple has the advantage of not having language restrictions and my guess is they won&#8217;t come close to approximating radio&#8217;s spot load. These folks have a long history of knowing what people want and a virtually unlimited research budget. And they&#8217;re light years ahead of radio in all areas of social media.
The big radio companies should be worried, but more to the point they should do something. Do some real research into how their listeners say they&#8217;ll react to the new Apple product and how it might impact their listening habits. And figure out how they can blunt the differentiation before it takes hold.
&#8220;We need to get out and tell our story better!&#8221; 
ARGH!!!
Bob, it is affirmation that radio&#8217;s content model (sans the spotload perhaps) is a good one. But it&#8217;s also a shot across many bows. Maybe a bazooka shot. Thanks for the comment&#8230;and the angst.
By the same token, The Sonny and Cher show affirms TV&#8217;s content model
True that. Not much Netflix action there.
</article>
</item>
<item>
<title>Digital Natives</title>
<article>Last month, Chicago Tribune workplace columnist, Rex Huppke, wrote a cautionary tale about brands that rely on “digital natives” to handle important web and social duties.
As he noted, &#8220;Talent isn&#8217;t age-based, regardless of the field.  People born before the advent of all things digital can be just as adept as anyone at programming or social media or Web design. And they even bring an additional attribute or two to the table.&#8221;  And those extra benefits can play a major part in a station&#8217;s social success – or failure.

In the past, we have recommended that companies look deeper into their staff rosters or intern candidates to seek out people who have digital skills that are highly intuitive.  Young people, for whom digital comes natural, cannot remember a time when there was no Internet.  Logic suggests they are going to be more knowledgeable about how to communicate on platforms like Twitter and Snapchat, text clubs, and on websites.
But if youth is the primary way you’re vetting and evaluating digital and social talent, you may be setting up your brand for embarrassment – or worse. Because a faux pas in the social space can create an epic disaster for any prominent brand.
Oftentimes, this is because while skills like posting and tweeting may be native, not knowing a station’s essence – its format, its audience, and their standards – can lead to major fails that have the potential to indelibly damage a brand.
It can be something basic – like not knowing that Dave Grohl was in a band before the Foo Fighters.
Or something a lot more serious.
There’s no shortage of these in the corporate world. And they often stem from the immaturity of social media admins who don&#8217;t have the depth or maturity to navigate the social space on behalf of a brand.  Even if they&#8217;re a digital native is no guarantee of a social media success.
You may remember the U.S. Airways incident where a simple customer complaint about a delay (which must happen multiple times a day) mushroomed into a well-publicized mess because of a totally inapprorpriate response from whoever was handling the airline’s social media accounts.  An attached pornographic photo was just the catalyst to take this mess viral.
And of course, that led to this mea culpa tweet from the airline:

It takes more than native skills to be effective as a social customer service rep. In fact, you could make the case that the following should be requirements for anyone representing your brand socially, especially considering how some consumers are just waiting to pounce on a lame action by any company – or radio station:
You may not have a strong social tool kit yourself, but if you’re a manager, use your skills to train, teach, and orient incoming digital natives about your audience, your brand, and your company.
There&#8217;s more riding  on this than most think.
Is age discrimination more prevalent today because of the strong workplace demand for younger digital natives, or is that just my perception, because in 3 years I graduate from the 25-54 demo?
Tommy, you will get no cap and gown when you turn 55.  Yes, the assumption that younger people know better goes to the heart of this.  They know how to post.  Do they know what to post? Thanks for the comment.
</article>
</item>
<item>
<title>Getting Connected</title>
<article>During the past four years, Jacobs Media has taken a lead role in helping make sense of the “connected car” for radio. From our Techsurveys to our mobile app development company, jācapps, to our DASH Conferences, we’re hacking our way through this space. Hopefully, you’re along for the ride.
During this time, we’ve taken part in a number of presentations and panel discussions about the “connected car.” Most recently, I’ve moderated a session at Convergence in Silicon Valley, and just returned from the Western Association of Broadcasters conference held in Banff, Alberta. As this post goes live, Paul is hosting sessions at the New Mexico Broadcasters Association conference in Albuquerque.
But maybe you haven’t had an opportunity to see one of these sessions in person, and get a feel for what we’re doing in the “connected car” arena – and why it matters.
I recently moderated a strong panel at Canadian Music Week’s “Radio Interactive” event in Toronto, featuring Radioplayer’s Michael Hill, Visteon’s Chris Andrews, Nielsen’s Dr. Ed Cohen, and Canadian journalist Ted Kritsonis.
CMW’s Neill Dixon and Greg Simpson were kind enough to send us the video of the entire session, so if you’d like some insight about why the “connected car” matters to radio, this is an excellent starting point.
It’s also representative of the kinds of sessions we present at our DASH Conferences. This year, DASH will be held once again in Detroit at the Westin Hotel inside the Detroit Metropolitan Airport on November 4-5. It’s an electric two-day event dedicated to radio’s future in cars. We bring in all the key players from automotive, radio, and everyone who has a role in this space.
The “connected car” may present the biggest challenge – and the biggest opportunity – facing radio today.
You can get more info on DASH here.
And here’s the panel:

</article>
</item>
<item>
<title>Radio’s Most Innovative:  GenX.fm (or “What We Did Over Memorial Day Weekend”)</title>
<article>
It&#8217;s amazing what you can do with a little free time.
Just before the Memorial Day Weekend, we issued a challenge to readers of this blog.  While the spirit of the holiday is to honor those who currently serve and have served our country in the past, we know that many of you enjoyed some much-needed down time.  While the idea was not to spend the weekend working, our hope was that an innovative idea or two might rise to the surface.
And veteran content creators Shawn Quinn and Stacy Ryder stepped up.
This week&#8217;s edition of &#8220;Radio&#8217;s Most Innovative&#8221; is a little different than usual.  It features a work in progress.  It&#8217;s clearly alpha because it&#8217;s not even beta.  But part of the spirit of our initiative isn&#8217;t just about showcasing innovators from the past or even new ventures that are up and running.  We were hoping to have the chance to visit a workshop or two along the way.
So I&#8217;m pleased to inform you that Shawn and Stacy spent much of the weekend innovating their next move.  After Shawn found himself on the beach, they could either find another radio job, leave the business altogether, or create their next opportunity.
They&#8217;re going with #3. And they&#8217;re heading down the digital highway.
Stacy explains why: &#8220;When Shawn would do interviews on his morning show, we listened over and over again as entertainers talked about how they achieved creative control through self publication. We&#8217;ve watched TV segment into 200+ channels of niche programming and go from over-the-air to digital consumption.  And, finally, we&#8217;ve listened when consultants and industry experts speak and write about the growth of digital and proper use of social media, only to watch it be brushed aside on a local level.&#8221;
And the lesson from all those years in commercial radio: &#8220;Content creates usage, (so) become the source.&#8221;
But lots of people talk a good game and blather on about new media opportunities, and yet they never get them off the ground.  Seth Godin wrote about this extensively in Poke the Box, and Lee Abrams&#8217; acronym &#8220;JFDI&#8221; is a reminder that it&#8217;s easy to walk around with ideas in your head.  Much harder to make them happen.
So over Memorial Day Weekend, Shawn and Stacy did more than just put thoughts down on a legal pad  They created a framework for what they&#8217;re calling GenX.fm.  The concept is to feature a traditional local-sounding presentation that people are familiar with using an online/mobile delivery.  This isn&#8217;t just a couple hundred songs in a box with produced drops.  GenX.fm features a staff of radio people who are staking out a different delivery system.
You can also see it here.

It&#8217;s clean, organized, and clever.  But is it going to happen?
Stacy: &#8220;This is something we&#8217;re doing.&#8221;
Architecturally, GenX.fm will have a live morning show featuring Shawn and co-host John Ralich, a stand-up comic.  Stacy handless producing.  They&#8217;ll replay the show in the afternoon, with a live host/co-host interacting socially with the audience.  Scott Garrett will be handling the local news scene, while Betty Rebel (from Pittsburgh) will be hosting &#8220;Smashed Wax Saturday Nights.&#8221;  All in all, a full lineup for an online startup.
Shawn and Stacy are looking to private fund the venture, and from a revenue generation standpoint, they&#8217;re focusing on in-stream and online/mobile advertising.
So we&#8217;re highlighting something very embryonic, but clearly innovative.  If &#8220;RMI&#8221; has inspired them in some way, we&#8217;re happy.  In the meantime, we hope to hear about more innovations that are incubating out there in the coming months.
By the way, the software they used to create their new business model is pretty cool.  It&#8217;s called LivePlan and you can access it here. 
&nbsp;
&nbsp;
Check out some of our “Past Innovators” here:
When IBM introduced its first personal computer, Apple ran an ad with the headline &#8220;Welcome, IBM. Seriously.&#8221; I thought it was quite self-serving. However, since I was working for Apple&#8217;s advertising agency at the time I decided to keep my mouth shut. I was young and foolish then. I&#8217;ve come to understand that strong competition makes you better at what you do. It&#8217;s always good to see radio professionals make a serious play in web radio, whether it&#8217;s Internet radio or podcasting. If they do their jobs well they drive the rest of us to be better at creating our content and marketing our product. I wish Shawn and Stacy the best of luck. Their success is good for the rest of us. Seriously.
Jay, thanks for the kind note and the encouraging comment.
Shawn, our pleasure.  Good luck with the venture and keep us updated.
Thanks Fred, we sure will!
</article>
</item>
<item>
<title>Amazon Updates The Kindle Paperwhite</title>
<article>The venerable Kindle is one of my favorite tech devices. I owned the Kindle 3, but the obvious shortcoming was the lack of lighting, forcing me to use a case with a clumsy light attached. The minute the original Kindle Paperwhite was announced, I quickly ordered a couple of them and they are to this day one of my favorite pieces of technology. The Paperwhite added a &ldquo;light guide&rdquo; layer to the display to evenly distribute the light from the LEDs found in the bezel, which gives the e-ink display the bright white image and makes it much easier to use in dim or dark scenarios.
In 2014, Amazon released the Kindle Voyage to the US market, which is their highest end Kindle yet. It features a 300 ppi e-ink display. Today, that same display is making its way to the mid-stream priced Kindle Paperwhite which should give it even better text rendering. The new version of the Carta e-paper display has double the pixels of the outgoing model.
Amazon is also offering the choice of a new font called Bookerly, which was created specifically for reading on digital screens: &ldquo;Bookerly is inspired by the artistry of the best fonts in modern print books, but is hand-crafted for great readability at any font size.&rdquo;
Also announced is a new typesetting engine which is listed as &ldquo;coming soon&rdquo; which offers improved character placement. They have adjusted the character spacing and the new typesetting engine will do a better job of justification and hyphenation of break words to create more consistent paragraph layouts. Amazon states that this will let you read faster with less eyestrain than the current engine.
The current features like note taking and word lookup are of course staying, but will be joined by new features like Page Flip which lets you skim ahead without losing your place. I prefer to read a book the way it was written, but I know a couple of people who like to look ahead and see what&rsquo;s going to happen so this will be a nice feature for them.
The Kindle is practically the definition of a uni-tasking device, but what it does, it does really well. The battery life is one of the keys to the experience, and Amazon states that the new Paperwhite can last up to six weeks if used for thirty minutes per day with the wireless off and the display at level ten. That works out to twenty one hours of usage between charges, and with my experience that is likely not an exaggeration.
Size and weight play a key part in the Kindle experience as well, and the Paperwhite has a 6-inch display inside of a small and thin body, and it weighs just 7.2 ounces or 205 grams for the Wi-Fi only model. The 3G option adds a tiny bit more to the total.
The new Kindle Paperwhite starts at $119 with Special Offers, jumps to $139 without Special Offers, and the 3G model costs $189 or $209. Shipments start on June 30th.
Source: Amazon
</article>
</item>
<item>
<title>Silicon Motion SM2256 SSD Controller Preview: TLC for Everyone</title>
<article>The SSD industry has been talking about TLC NAND for over three years now. We published our first post, Understanding TLC NAND, back in early 2012, but in three years we have actually seen very little TLC NAND making it to the SSD market. Samsung was an early adopter by introducing the SSD 840 in September 2012, but Samsung has always been a special case as its SSD business is fully vertically integrated. When you design and manufacture everything in-house, it&#39;s obvious that you will have a technological advantage when it comes to adopting new technologies.
TLC is tightly linked with both controller technology and NAND production&nbsp;because TLC inherently has a higher error rate, which needs a stronger controller (although admittedly Samsung has had some issues with TLC). The lack of&nbsp;proper controller is the reason why other NAND vendors haven&#39;t invested as heavily on TLC as Samsung because Micron, SanDisk and the like have to rely on whatever third party controllers are available&nbsp;on the market. Without a high volume product to put your TLC NAND into, it means that there&#39;s no reason to produce TLC in a large scale, which defeats the cost savings that TLC bring. Micron tried to promote its 25nm TLC NAND for a while a few years ago, but it quickly realized that the available SSD controllers aren&#39;t capable of creating a reliable product --&nbsp;at least not one that would bring any cost savings since the drive would need serious over-provisioning for endurance and ECC parity.
Due to the lack of controller support, nobody other than Samsung and SanDisk have a TLC SSD on the market, although SanDisk had to rely on heavy over-provisioning with 5:1 parity ratio since the Marvell controller used in the Ultra II wasn&#39;t designed with TLC in mind. Silicon Motion&#39;s SM2256 will be the first commercially available controller and firmware combo with TLC support and today we are taking an early look of the platform in the form of a reference design sample. ADATA already announced its SP550 SSD that will be based on the new SM2256 controller and available later in the summer, but given how many OEMs rely on Silicon Motion&#39;s controllers nowadays&nbsp;we will likely see a large number of SM2256 based TLC drives entering the market by the end of the year.
Architecturally the SM2256 shares the same core design as its predecessor SM2246EN. The design is modular, which allows Silicon Motion to change parts of the controller without redoing the rest.&nbsp;It features the same&nbsp;single 32-bit Argonaut&nbsp;RISC processor core&nbsp;as the SM2246EN, which is quite unique because we have seen many SSD controller vendors moving towards multi-core ARM architectures. A single custom core obviously brings efficiency gains and we&#39;ve witnessed those in the SM2246EN, but the downside of such limited CPU power is sustained performance when the controller has to perform garbage collection at the same time as processing host IOs.&nbsp;
The&nbsp;only dramatic change is in the error correction circuitry as the&nbsp;SM2256&nbsp;supports Low Density Parity Check (LDPC) error correction codes instead of more common and&nbsp;less powerful BCH ECC.&nbsp;Silicon Motion calls its ECC technology as NANDXtend, and it&#39;s a combination of LDPC hard and soft decode along with RAID5-like data recovery. The benefit of having three levels of ECC is performance because LDPC soft decode and recovery from parity both have a relatively noticeable impact on performance and are typically only needed when the drive approaches its end of life (i.e. when the NAND has been cycled a lot). Uncycled NAND has much higher reliability because the tunnel oxide hasn&#39;t worn out due to P/E cycles, so only very little ECC is needed and LDPC hard decode is sufficient and also doesn&#39;t have a dramatic impact on performance.
The reason why hard decode is faster than soft decode lies in how the voltage of a cell is sensed. Hard sensing is binary based, so for an SLC cell like in the graph above the cell can be either 1 or 0. However, as you can see, the voltage threshold distributions overlap slightly and that&#39;s actually far worse with MLC and TLC since there are more voltage states. In soft sensing the voltage distributions are divided into several segments, which requires more precision and iterations. For example in segment 4 the bit value can be either 1 or 0 as the distributions overlap, so probability algorithms are used to figure out the correct value. To be honest, ECC codes and the way they work are way over my head, but in case you are familiar with ECC and want to learn more, I suggest you simply google LDPC as there are numerous publicly available&nbsp;academic papers that go into more depth about this topic.&nbsp;
Silicon Motion claims that its NANDXtend technology can extend the endurance of TLC NAND by up to three times, making TLC more robust for heavier workloads and also allowing the use of lower quality NAND that some OEMs may use anyway due to the lack of in-house binning equipment. Unfortunately I didn&#39;t have any time to do extended endurance testing with the SM2256 yet to validate Silicon Motion&#39;s claims, but I will be sure to test that once we have a retail drive on our hands,&nbsp;
The SM2256 has eight chip enablers (CE) per channel, meaning that it can simultaneously&nbsp;talk to up to 32 NAND dies, but one CE can control more than one die, resulting in capacities of up to 2TB with one CE per four dies.&nbsp;
The engineering sample Silicon Motion sent us uses Samsung&#39;s 19nm 128Gbit TLC NAND, which is the same NAND that is found inside the 840 EVO. Samsung doesn&#39;t sell its TLC NAND to others in high volume, so we likely won&#39;t see this configuration on the market at all. The SM2256 does, of course, support TLC NAND from all other vendors as well (even Toshiba&#39;s 15nm).
Like all TLC SSD designs we have seen, Silicon Motion employs SLC caching in the SM2256 to improve performance and endurance. The size of the SLC cache is configurable by the OEM, but generally the cache size is between 3GB and 12GB depending on the capacity of the drive. The OEM can also configurate whether all IOs are cached or just smaller ones (e.g. 16KB and below), but the early stock firmware that our sample shipped with caches all IOs regardless of the size.&nbsp;
</article>
</item>
<item>
<title>AMD Shows Off Dual-GPU Fiji Card At PC Gaming Show</title>
<article>Briefly announced and discussed during AMD&rsquo;s 2015 GPU product presentation yesterday morning was AMD&rsquo;s forthcoming dual Fiji video card. The near-obligatory counterpart to the just-announced Radeon R9 Fury X, the unnamed dual-GPU card will be taking things one step further with a pair of Fiji GPUs on a single card.
Meanwhile as part of yesterday evening&rsquo;s AMD-sponsored PC Gaming Show, CEO Dr. Lisa Su took the stage for a few minutes to show off AMD&rsquo;s recently announced Fury products. And at the end this included the first public showcase of the still in development dual-GPU card.
There&rsquo;s not too much to say right now since we don&rsquo;t know its specifications, but of course for the moment AMD is focusing on size. With 4GB of VRAM for each GPU on-package via HBM technology, AMD has been able to design a dual-GPU card that&rsquo;s shorter and simpler than their previous dual-GPU cards like the R9 295X2 and HD 7990, saving space that would have otherwise been occupied by GDDR5 memory modules and the associated VRMs.
Meanwhile on the card we can see that it uses a PLX 8747 to provide PCIe switching between the two GPUs and the shared PCIe bus. And on the power delivery side the card uses a pair of 8-pin PCIe power sockets. At this time no further details are being released, so we&rsquo;ll have to see what AMD is up to later on once they&rsquo;re ready to reveal more about the video card.
</article>
</item>
<item>
<title>HTC Tests Native Ads In BlinkFeed</title>
<article>Today HTC has announced a pilot program of sorts for the display of native ads within BlinkFeed, their news and content aggregation application. Since Blinkfeed displays right on a user&#39;s app launcher, it&#39;s a natural place for HTC to try and put advertising, although users may not feel as enthusiastic about the changes as HTC does.
According to HTC&#39;s blog post about the changes, the display of native ads in BlinkFeed will initially be limited to users in the United States, the United Kingdom, Germany, Taiwan, and China. The promotions will be for sponsored apps from Yahoo, Twitter, and Appia,&nbsp;as well as HTC&#39;s own products. HTC will be providing an option to opt-out of these advertisements, and with this being a pilot program there&#39;s no indication if or when it will ever make its way to other markets.
</article>
</item>
<item>
<title>AMD Radeon Live Blog - AMD Presents: The New Era of PC Gaming</title>
<article>12:05PM EDT - AMD is talking about the rise of e-sports gaming12:06PM EDT - Display resolutions are rapidly increasing. 4K monitors have dropped from $3000 to $400-60012:06PM EDT - Smooth frame rates have become another area of focus, in addition to simply having a high frame rate12:07PM EDT - Virtual reality is also an area that AMD sees as rapidly expanding12:07PM EDT - VR requires exceptionally high frame rates and low latency12:08PM EDT - Okay, after some WiFi problems we';re here12:08PM EDT - Now on stage: Devin Nekechuk to introduce the Radeon 300 series12:09PM EDT - AMD is starting off by talking about the R7 and R9 300 series cards.12:09PM EDT - First up, R7 36012:11PM EDT - AMD is discussing VSR. Not exclusive to the 300 series though.12:13PM EDT - First card is the R7 360, the base of the line.12:13PM EDT - R7 370 is next. $149 and up to 4GB of VRAM. Not clear if those two go together, as they said "up to"12:14PM EDT - Now moving onto cards for more intensive games, and for gaming at higher resolutions12:15PM EDT - Next is the R9 380. AMD claims it can power 1440p. Starts at $199, up to 4GB VRAM.12:15PM EDT - R9 390 and 390X start at $329 and $429. Both have 8GB of GDDR5. Meant for 4K gaming12:17PM EDT - Now talking about DX1212:17PM EDT - DX12 launching with Win10, July 29th is not far away12:18PM EDT - AMD now inviting up some game developers and a Microsoft developer12:19PM EDT - Developers seeing significant CPU usage reductions with DX1212:21PM EDT - Now talking to the developer from Lionhead about DX12 in the Fable series12:22PM EDT - Asynchronous shaders to execute shading concurrently with other rendering are an important feature12:23PM EDT - Windows 10 beta of Fable Legends coming in the near future.12:24PM EDT - Now on display: Stardock/Oxide's Ashes of the Singularity12:24PM EDT - Now talking about RTS games12:25PM EDT - Oxide is going to be one of the first companies to really use DX12. They're looking to push a lot of draw calls12:25PM EDT - RTS games have traditionally had to swap to 2D sprites when characters are too far away. Not required with DX12.12:26PM EDT - CPU power freed up by using DX12 can be put into making more complex AI12:27PM EDT - Alpha this Thursday12:27PM EDT - Now moving onto VR. AMD's Richard Huddy is back on stage12:29PM EDT - Anuj Gosalia of Oculus is talking12:30PM EDT - AMD is going to be banking hard on VR for this generation. From a business standpoint it requires lot of GPU power, and from a tech standpoint they have what should be a good solution12:31PM EDT - Low latency is the big focus in VR12:31PM EDT - Now discussing how Oculus is using AMD's LiqudVR tech, which was first announced back at GDC in March12:32PM EDT - Oculus has been working with AMD to use their LiquidVR tech12:32PM EDT - Direct hardware access for low latency, multi-GPU per-eye rendering, async shading/warping, etc12:32PM EDT - Oculus has shipped 150K dev kits (wow)12:33PM EDT - Oculus has shipped 150,000 dev kits so far12:33PM EDT - And of course, the final consumer Rift ships in Q1 of next year12:35PM EDT - For AMD GPUs, Oculus is recommending R9 290/390 and higher12:35PM EDT - Recommended GPU spec for Oculus is the R9 290 or faster12:35PM EDT - This is consistent with their earlier developer target recommendation of R9 29012:36PM EDT - Now speaking, CCP on EVE: Valkyrie12:36PM EDT - They've been one of the darling early VR demos, and will be shipping on PC and PS4 (Morpheus)12:37PM EDT - Game will be released alongside the Rift, so Q1 2016 (it)12:38PM EDT - Huddy now has the stage to himself again12:38PM EDT - Discussing how VR is being used for non-gaming applications12:38PM EDT - Now on stage, Katrina Craigwell from GE12:39PM EDT - GE is using VR for brain imaging visualization12:41PM EDT - And that's a wrap on GE12:42PM EDT - Okay, time for the high-end GPU announcement12:42PM EDT - (If you haven't already seen the leaks, well, then you'll probably be the only person surprised by this)12:42PM EDT - Now on stage, Chris Hook of AMD. Director of marketing12:43PM EDT - Leading into a discussion about small form factor PCs12:44PM EDT - Presenting Project Quantum12:44PM EDT - A custom SFF case12:45PM EDT - Contains 2 of AMD's new Fiji GPUs12:45PM EDT - Processors on the bottom, cooling on the top12:45PM EDT - Now rolling a promo video12:47PM EDT - Begun, the Closed Loop Liquid Cooler wars have12:47PM EDT - The company has clearly taken what they've learned from R9 295X212:47PM EDT - Which, though $1500 was a successful product for a dual-GPU card and a solid design12:48PM EDT - Now on stage, AMD's CEO, Dr. Lisa Su12:48PM EDT - "Most complex and highest performance GPU we have ever built"12:49PM EDT - There will be multiple products with Fiji12:49PM EDT - AMD Radeon R9 Fury X12:50PM EDT - 1.5x perf per watt of R9 290X12:50PM EDT - R9 Fury (vanilla) will be air-cooled12:50PM EDT - Cards will be in stores "very shortly"12:51PM EDT - AMD Radeon R9 Nano12:51PM EDT - Fiji in a a 6" card, half the power of 290X12:51PM EDT - (Sounds like it's significantly cut down from full Fiji)12:51PM EDT - Nano will be available later this summer12:52PM EDT - Finally, a dual-GPU card that's in the Quantum, but hasn't been named or shown12:52PM EDT - Now on stage, Joe Marci, Raja Koduri, and Chris Hook again12:53PM EDT - Raja is now explaining the human element behind designing Fiji12:55PM EDT - Focus on 4K and HBM12:55PM EDT - Specs12:55PM EDT - 4069 stream processors, 8.9B transistors12:55PM EDT - 4096 SPs, even12:55PM EDT - 8.6 TFLOPs, 1050MHz core clock12:56PM EDT - Also did some work on power management/efficiency, though now going in-depth at this time12:56PM EDT - Raja is giving special credit to the board design team12:57PM EDT - Joe now talking a bit more on HBM12:58PM EDT - AMD has over the last several years been on the cutting edge of memory tech. 2015 and Fiji is no different12:58PM EDT - HBM gets AMD more memory bandwidth, but also cuts down on memory power, giving them more power headroom for the GPU itself12:59PM EDT - AMD will be putting HBM in more devices in the future (where costs make sense, of course)01:00PM EDT - Raja has never been so excited in the last 20 years01:01PM EDT - Laying the path for the future01:01PM EDT - They see higher quality VR systems as requiring much, much more GPU performance01:02PM EDT - The Holodeck concept and Eyefinity seems to have given way to VR and the holodeck on your head01:03PM EDT - Joe is talking a bit about overclocking headroom. AMD says it should be a good overclocker01:03PM EDT - Fury X goes on sale on the 24th01:03PM EDT - $64901:03PM EDT - Fury (vanilla) for $549 on July 14th01:04PM EDT - Nano in the summer, dual-GPU card in the fall01:04PM EDT - So the fight is set: Fury X needs to meet or beat NVIDIA's GTX 980 Ti. AMD is aiming to best NVIDIA's top Maxwell GPU01:05PM EDT - To close things out, Huddy is back on stage01:05PM EDT - Will be showing the PC version of Star Wars: Battlefront01:06PM EDT - DICE's Lead Producer (whose name I couldn't type out fast enough) is now on stage01:07PM EDT - Frostbite engine game, so they already have all the tech Johan Andersson has been working on01:07PM EDT - Discussing the production of the game01:09PM EDT - Rolling PC footage01:10PM EDT - A very short clip indeed01:11PM EDT - AMD will have it playable at SDCC next month01:12PM EDT - Recap time01:13PM EDT - That's a wrap01:14PM EDT - Thanks for joining us, everyone
</article>
</item>
<item>
<title>Xbox At E3: New Controller, Game Emulation, And Plenty Of Exclusives</title>
<article>Today is the first official day of the Electronic Entertainment Expo, otherwise known as E3, and the Xbox team was on-stage to deliver plenty of news surrounding Xbox. It would be fair to say that in the past the Xbox team focused too much on the non-gaming aspects of the Xbox One, but today&rsquo;s media briefing was all about gaming. There was quite a bit announced today, so let&rsquo;s go over the highlights.
Likely the biggest news of the day was the announcement that Xbox 360 games will be playable on the Xbox One. Microsoft has built an Xbox 360 software emulator which can be launched in order to play titles from their older console. This is not vaporware either; the emulator is available starting today for people in the Xbox preview program. Currently the selection of titles is limited and more will be converted before launch, with the Xbox team targeting a holiday release for this feature. The feature will work with games that have been purchased digitally or on disc, and if you have the disc, you have to insert it, and the system will download the files to the Xbox One. I assume the disc will need to be in the device in order to play as well. So although this is an emulator, clearly Microsoft is doing a lot of work on converting the code to run quickly on the much different hardware in the Xbox One, which is x86 based, as compared to the Xbox 360&rsquo;s PowerPC architecture. Fans can vote on Xbox Feedback which games they want to see converted first, and more will be added over time.
In addition, the Microsoft has created the Xbox Elite Controller, which offers up a lot more options and customizations than the current Xbox Controller. Players can choose among six thumbsticks, with different shapes, sizes, and heights, to tailor the controller to your own gaming style. The D-Pad is also new, with a unique faceted face which should help in some games, or you can switch to the traditional D-Pad if the title prefers the accuracy of that pad.
There are four slots on the back for interchangeable paddles, giving you access to more commands with more fingers. There are also hair trigger locks which reduce the movement necessary to activate the triggers, which are valuable in a game where the full range of trigger motion is not necessary. It can quickly be deactivated to get the full range back.
There will be an Xbox Accessories App on both the Xbox One and Windows 10 to let you customize the experience and map buttons, and you can save up to 255 controller profiles for custom settings on many games. You can even load two profiles and switch between them on the fly.
The thumbsticks are stainless steel, and there are low-friction reinforced rings around the thumbsticks for a long life. The new controller will retail for $149.99 and has availability in October.
Moving on from the new controller (which it appears I need to get) Microsoft also reiterated its new partnership with Oculus for the Rift launch, but in addition they are also partnering with Valve VR. This is very interesting since Valve seems to have had a cool relationship with Microsoft over the last couple of years, and it is great to see them working together on Virtual Reality. Microsoft also showed off their own Augmented Reality device, the Hololens, with a custom version of Minecraft which players can explore and build with using Microsoft&rsquo;s own headset.
There was also plenty of software talk today as well, since any piece of hardware needs quality software. Microsoft is adding a new feature to allow players to try out games while they are being developed with the launch of the Xbox Game Preview pilot program. This is a lot like Steam&rsquo;s Early Access. Microsoft was keen to point out that they have a lot of indie developers on-board.
Last, but certainly not least, is the list of upcoming Xbox titles, some of which are exclusive to Xbox and some of which will have a limited exclusivity window. Exclusive games to Xbox include Ashen, Cuphead, Fable Legends, Forza Motorsport 6, Gears of War: Ultimate Edition, Gears of War 4, Gigantic, Halo 5: Guardians, &nbsp;Rare Replay (a collection of old Rare games), ReCore, and Sea Thieves, with Beyond Eyes, Tacoma, and the much anticipated Rise of the Tomb Raider being timed exclusives on the platform.
Microsoft has not had the success with the Xbox One, compared in relative terms at least, to the Xbox 360, and with this briefing being exclusively focused on gaming, and the introduction of a special controller for the enthusiast crowd, Microsoft is trying to win back some mindshare. Only time will tell if they are successful.
</article>
</item>
<item>
<title>A First Look At Apple&#39;s OS X El Capitan</title>
<article>Right on schedule, at last week&rsquo;s World Wide Developers Conference Apple announced the next iteration of their desktop OS, OS X. Version 10.11, dubbed El Capitan, is the latest in Apple&rsquo;s string of yearly OS updates. And with this now being the 3rd iteration of the company&rsquo;s initiative to offer free desktop operating system upgrades, it&rsquo;s safe to say that the company has settled into what is the new norm.
From an OS perspective, WWDC straddles an interesting point between announcing new OSes and priming developers for them. The sole public session of the conference is the Keynote, where Apple announces the newest versions of OS X, iOS, and whatever other major initiatives they have going on. However once the keynote is done, that&rsquo;s it for the public. The company has plenty of other activities going on at WWDC, but those are closed door events for attending developers. Ultimately there&rsquo;s no practical way for Apple to announce something major like a new OS to developers without the news leaking, so the company instead does a public announcement to get consumers and developers excited while neatly sidestepping the immediate issue with keeping such a big secret, and then closes up to get to work on preparing developers for their latest OSes.
Of course developers can only do so much without seeing the OS first-hand, so along with serving as a developers conference and keynote backdrop, WWDC also serves as the launching point for Apple&rsquo;s OS betas for developers. Over the next few months OS X and iOS 9 will go from a developer beta to a public beta, and finally to retail products. Which is to say that developers get to have all the fun of actually interacting with the OS first, as the public beta won&rsquo;t start for another month or two.
And that, at last, brings us to today. With thousands of developers and no practical way to keep the OS from leaking, Apple is seemingly trying something a little different this year when it comes to engaging consumers and the press during the developer beta period. Rather than clamming up entirely &ndash; developers are under non-disclosure agreements &ndash; Apple invited us to take a first look at the beta OS, loaning us a 2015 15-inch Retina MacBook Pro with OS X El Capitan preloaded. To that end, today we are taking our first look at El Capitan, checking out the major new features of the OS and experiencing first-hand the software Apple is putting together for later this year.
Now make no mistake, this is a beta OS and it has problems like any piece of beta software does &ndash; Apple does not withhold OS betas from the public just because they like to be secretive &ndash; but it&rsquo;s an increase in transparency from a very opaque company. More to the point, while El Capitan still has some debugging to go through and a few teething pains that come with it, it&rsquo;s already in a state where it&rsquo;s usable, where the major features are in place and working, and the bugs Apple hasn&rsquo;t already squashed can be worked around by an experienced hand. So with that in mind, let&rsquo;s take a look at what Apple is gearing up for with the 12th iteration of OS X.
First off, to address the obvious question in the room, why &ldquo;El Capitan?&rdquo; With Apple dropping the big cat codenames a couple of years back, they have now started naming OS X after California landmarks. With 10.10 Apple named the OS after Yosemite National Park, and now with 10.11 El Capitan, they have gone one step further and named the OS after the El Capitan rock formation.
El Capitan is admittedly a bit of a mouthful, especially if you&rsquo;ve never heard the accent pronounced before (it&rsquo;s pronounced el-KAP-i-TAN, by the way), however the name is fitting from a development standpoint. Whereas Yosemite was a major OS release for Apple, involving a significant UI overhaul along with numerous feature upgrades, El Capitan is a far tamer excursion for Apple. It&rsquo;s still an important release for its own reasons, but it&rsquo;s definitely an iteration on Yosemite rather than another major change; it is the Mountain Lion to Lion, the Snow Leopard to Leopard. So it&rsquo;s only fitting that with such close ties to Yosemite, it&rsquo;s named after a rock within the park.
Ultimately with the company no longer charging for OS updates, they can afford to take on a build-and-optimize approach, since no matter how many new features an OS comes with it&rsquo;s still free. At the same time it means that El Capitan gets to build off of a solid base in Yosemite, which means Apple can focus on other areas of the OS.
So what does El Capitan bring to the table? Over the next couple of pages we&rsquo;ll lay out the major features, but in a nutshell it&rsquo;s an interesting combination of some further harmonization with iOS, some new features for the OSes major applications, and even some core technology updates. Mentioning iOS and OS X in the same breath always draws a few groans, and while some of the iOS elements that have come to past OS X releases has been more tepidly received than other elements, in the case of El Capitan it feels like Apple has a better grip on harmonizing the OSes without losing what makes OS X unique, and what makes a desktop OS work best.
</article>
</item>
<item>
<title>Comparing OpenGL ES To Metal On iOS Devices With GFXBench Metal</title>
<article>In the past couple of years we&#39;ve seen the creation of a number of new low level graphics APIs. Arguably the first major initiative was AMD&#39;s Mantle API, which promised to improve performance on any GPUs that used their Graphics Core Next (GCN) architecture. Microsoft followed suit in March of 2014, with the announcement of DirectX 12 at the 2014 Game Developers Conference. While both of these APIs promise to give developers more direct access to graphics hardware in the PC space, there was still no low level graphics API for mobile devices, with the exception of future Windows tablets. That changed in the middle of 2014 at WWDC, where Apple surprised a number of people by revealing a new low level graphics and compute API that developers could use on iOS. That API is called Metal.
The need for a low level graphics API in the PC space has been fairly obvious for some time now. The level of abstraction in earlier versions of DirectX and OpenGL allows them to work with a wide variety of graphics hardware, but this comes with a significant amount of overhead. One of the biggest issues caused by this is reduced draw call throughput. A simple explanation of a draw call is that it is the command sent by the CPU which tells the GPU to render an object (or part of an object) in a frame. CPUs are already hard-pressed to keep up with high-end GPUs even with a low level API, and the increased overhead of a high level graphics API further reduces the amount that can be issued in a given period of time. This overhead mainly exists because most graphics APIs will do shader compilation and state validation (ensuring API use is valid) when a draw call is made, which takes up valuable CPU time that could be used to do other things like physics processing or drawing more objects.
Because a draw call involves the CPU preparing materials be rendered, developers can use tricks such as batching, which involves grouping together items of the same type to be rendered with a single draw call. Even this can present its own issues, such as objects not being culled when they are out of the frame. Another trick is instancing, which involves making a draw call for a single object that appears many times, and having the GPU duplicate it to various coordinates in the frame. Despite this, the overhead of the graphics API combined with the time that it takes the CPU itself to issue a draw call ultimately limits how many can be made. This reduces the number of unique objects developers can put on screen, as well as the amount of CPU time that is available to perform other tasks. Low level graphics APIs aim to address this by removing much of the overhead that exists in current graphics APIs.
The question to ask is why do Apple and iOS developers need a low level graphics API for their mobile games? The answer ends up being the same as the PC space. While the mobile space has seen tremendous improvements in both CPU and GPU processing power, the pace of CPU improvements is slowing when compared to GPU improvements. In addition, the increases GPU processing power were always of a greater magnitude than the CPU increases. You can see this in the chart above, which shows the level of the CPU and GPU performance of the iPad relative to its original model. Having CPU performance improve by a factor of twelve in less than five years is extremely impressive, yet it pales in comparison to the GPU performance which, in the case of the iPad Air 2, is 180 times faster than its original version.
Because of this widening gap between CPU and GPU speeds, it appears that even mobile devices have begun to experience the issue of the GPU being able to draw things much faster than the CPU can issue commands to do so. Metal aims to address this issue by cutting through much of the abstraction that exists in OpenGL ES, and this is possible in part because of Apple&#39;s control over their hardware and software in their devices. Apple designs their own CPU architectures, and while they don&#39;t design the GPU architecture, it&#39;s clear they&#39;re free to do what they desire to with the IP to create the GPUs they need.
The other side of the discussion is compatibility. Much of the abstraction in higher level graphics APIs is done to support a wide variety of hardware. Low level graphics APIs often are not as portable or widely compatible as high level ones, and this is also true of Metal. The iOS Metal API currently only works on devices that use GPUs based on Imagination Technologies&#39; Rogue architecture, which limits it to devices that use Apple&#39;s A7, A8, and A8X SoCs.
This can pose a dilemma for developers, as programming only for Metal limits the number of users they can target with their application. The number of older iPads and iPhones still in use, as well as Apple&#39;s insistence on selling the original iPad Mini and iPod Touch which use their A5 SoC from 2011, can limit the market for games that use Metal. If I were to make a prediction, it would be that Metal&#39;s adoption among iOS developers will grow substantially in the next year or two when devices that use the A5 and A6 chips are retired from sale.
Kishonti Informatics, the developer of the GFXBench GPU benchmarking application, have released a new version of their benchmark. The new benchmark is called GFXBench Metal, and it&#39;s essentially the same benchmark as the normal GFXBench 3.0 / 3.1. The difference is that this version of the benchmark has been built to use Apple&#39;s Metal API rather than OpenGL ES. Although it&#39;s not one of the first Metal applications, it&#39;s one of the first benchmarks that can give some insight into what improvements developers and users can see when games and other 3D applications are built using Metal rather than OpenGL ES.
Before getting into the results, I did want to address one disparity that may be noticed about the non-Metal iPad Air 2 results. It appears that Apple has been making some driver optimizations for the A8X GPU with iOS releases that have come out since our original review. Because of this, the iPad Air 2&#39;s performance in the OpenGL version of GFXBench 3.0 is noticeably improved over our original results. To avoid incorrectly characterizing the improvements that Metal brings to the table, all of the iPad tests for the OpenGL and Metal versions of the benchmark were re-run on iOS 8.3. Those are the results that are used here. Testing with the iPhone 5s and 6 revealed that there are no notable improvements to the performance of Apple A7 and A8 devices.
GFXBench 3.0&#39;s driver overhead test is one we don&#39;t normally publish, but in this circumstance it&#39;s one of the most important tests to examine. What this test does is render a large number of very simple objects. While that sounds like an easy task, the test renders each object one by one, and issues a separate draw call for each. This is essentially the most inefficient way possible to render the scene, as the GPU will be limited by the draw call throughput of the CPU and the graphics API managing them.
In this test, it&#39;s clear that Metal provides an enormous increase in performance. Even the lowest performance improvement for a device on Metal compared to OpenGL is still well over a 3x increase. While this test is obviously very artificial, it&#39;s an indication that Metal does indeed provide an enormous improvement in draw call throughput for developers to take advantage of.
While the driver overhead test is an interesting way of looking at how Metal allows for more draw call throughput, it&#39;s important to look at how it performs with actual graphics tests that simulate the type of visuals you would see in a 3D game. In both the Manhattan and T-Rex HD parts of GFXBench we do see an improvement when using Metal instead of OpenGL ES, but the gains are not enormous. The iPad Air 2 shows the greatest improvement, with an 11% increase in frame rate in T-Rex HD, and an 8.5% increase in Manhattan.
The relatively small improvements in these real world benchmarks illustrate an important point about Metal, which is that it is not a magic bullet to boost graphics performance. While there will definitely be small improvements due to general API efficiency and lower overhead, Metal&#39;s real purpose is to enable new levels of visual fidelity that were previously not possible on mobile devices. An example of this is the Epic Zen Garden application from Epic Games. The app renders at 1440x1080 with 4x MSAA on the iPad, and it displays 3500 animated butterflies on the screen at the same time. This scene has an average of 4000 draw calls per frame, which is well above what can currently be achieved with OpenGL ES on mobile hardware.
I think that Metal and other low level graphics APIs have a bright future. The introduction of Metal on OS X can simplify the process of bringing games to both Apple&#39;s desktop and mobile platforms. In the mobile space, developers of the most complicated 3D applications and games will be eager to adopt Metal as they begin to hit the limits of what visuals can be accomplished under OpenGL ES. While there are titles like Modern Combat 5 which use both Metal and OpenGL ES depending on the device, that method of development prevents you from using any of Metal&#39;s advantages effectively, as they will not scale to the OpenGL ES version. I cannot stress enough how much the continued sale of Apple A5 and A6 devices impedes the transition to using Metal only, and I hope that by the time Apple updates their product lines again those devices will be gone from sale, and eventually gone from use. Until that time, we&#39;ll probably see OpenGL ES continue to be used in most mobile game titles, with Metal serving as a glimpse of the mobile games that are yet to come.
</article>
</item>
<item>
<title>The Acer Aspire R 13 Review: Convertible Notebook With A Twist</title>
<article>The world of the convertible notebook has come a long way in just a couple of years, but we seem to have settled in on two basic types of convertible devices. There are the tablet style devices where the display can be removed from the keyboard and used separately, and there are the notebook style devices where the keyboard can be rotated around and under the display in order to act like a tablet. Acer has decided to try something different with the Aspire R 13 which features their Ezel Aero hinge.
What makes the Aspire R 13 somewhat unique is the U-Frame to which the 13.3-inch display is attached. Unlike most convertible notebooks where the display rotates around on the hinge (a la Lenovo&rsquo;s Yoga line) the U-Frame can pivot on the laptop hinge, and then the display can pivot within the U-Frame. This has a couple of advantages over a yoga-style laptop. First, the amount of bounce when using the display with touch is reduced because the display has a mounting point half way up the sides, rather than just at the bottom. Second, it allows the device to be used in a tablet mode where they keyboard is not underneath, making it easier to hold on to. And finally, the U-Frame offers up a couple of new usage modes which are not available on yoga-style devices, with Ezel mode and Stand mode on offer, in addition to notebook mode, tablet mode, tent mode, and display mode, which are available on a device which flips the keyboard around behind.
This is not the first device to try this out of course, and others like the Dell XPS 2-in-1 also had a display that flipped around inside a frame, but the frame being just half way up does allow the display to be pivoted and used at pretty much any angle.
The downside to this design is that the frame needs to be fairly sturdy, and Acer has built the U-Frame out of magnesium-aluminum, so there is no worry about strength. The one thing you cannot avoid though is that this design is going to be wider and heavier than a yoga-style device.
The R 13 comes in two configurations. The base model has a Full HD 1920x1080 display, with a Core i5-5200U processor, 8 GB of memory, and a 128 GB SSD. The upgraded model features a WQHD 2560x1440 display, a Core i7-5500U processor, 8 GB of memory, and a 256 GB SSD. Both models feature the storage provided in RAID 0, but I&rsquo;m not sure why. A single larger drive would likely do just fine. Let&rsquo;s take a look at the entire system specifications.
With the base model starting at $899, the Aspire R 13 is in a good spot. The base price is the same as the Dell XPS 13, but comes with a touch display and the convertible nature, and actually has a decent CPU upgrade over the i3 that is the base in the Dell. For $1299, you can move to the WQHD model. Both have a couple of different storage options which move the price around a few hundred dollars, with the 512 GB WQHD model having a MSRP of $1499.99. This is all right in the heart of Ultrabook pricing, and although it can get pretty pricey, it is really right around the same price as its competition. Acer will be competing on design and execution rather than price, but if anyone looked closely at the specifications you can see one issue already. The upgraded display comes in with the expected higher resolution, but moves from the IPS 1080p model to a PVA 1440p model. For a $400 increase in price, customers gain the Core i7 (which is a big chunk of the price difference), double the storage at 256 GB, and a PVA WQHD display with just 160&deg; of viewing angle. I found that contrast seemed to fall off fairly quickly off center. It is a shame Acer could not have made the upgraded display IPS as well with a wider field of view despite it being a Sharp IGZO PVA panel which is normally quite good.
For this review, Acer sent us the $1299 model with the i7 and the WQHD display.
</article>
</item>
<item>
<title>EVGA expands the SuperNOVA G2 PSU series</title>
<article>As users are becoming more and more aware of how PSUs operate and what the real energy requirements of their systems are, sales of high wattage units decrease in relevance to middle range units. Many manufacturers realize that and they began marketing high performance products of reasonable power output and pricing instead of focusing their efforts on high output units. In that light, EVGA expanded their very popular G2 PSU series downwards, adding 550W and 650W models to it.
EVGA&#39;s G2 series is synonymous with the excellent balance between cost, quality and performance. We have seen their capabilities in our review of the 850W version. After all, there is good reason why the Super Flower Leadex platform is so popular. The new 550W and 650W models are physically smaller but share the same features, so it is very likely that they are based on a Super Flower platform as well.
According to EVGA, the main features of the new 550 G2 and 650 G2 PSUs are:
We should note that both units are rated at 50&deg;C and have a ridiculous number of connectors for their power output. Even the 550W version has three PCI Express connectors (two 8 pin and one 6 pin) and nine SATA connectors. Apparently, EVGA is very confident about the capabilities of their new units - or of their OCP, at least. Nevertheless, the 550W version should be able to easily power any system with a single CPU and a single GPU, with the possible exception that the extreme combination of an AMD FX-9590 and an R9 295X2.
The new G2 series units are available as of the 12th of June.
</article>
</item>
<item>
<title>NVIDIA Acquires Game Porting Group &amp; Tech From Transgaming</title>
<article>While NVIDIA&rsquo;s core businesses and gaming have been inseparable since the start, it&rsquo;s only relatively recently that NVIDIA has become heavily involved in game creation itself, and not just supplying the hardware that games are played on. The launch of the company&rsquo;s Tegra ARM SoCs, their SHIELD product lineup, and the overall poor state of the Android gaming market has led to the company investing rather significantly in bringing higher quality games over to SHIELD and Android devices. This has culminated in NVIDIA paying for the Android ports for a number of games, some of the most famous including the Android ports of Valve&rsquo;s Half-Life 2 and Portal.
Meanwhile with the launch of the SHIELD Android TV, NVIDIA is essentially doubling-down on Android gaming as part of their efforts to become the premiere Android TV set top box. And now as part of those efforts, the company has announced that they are acquiring the Graphics &amp; Portability Group (GPG) from game tool developer Transgaming.
Transgaming is best known for their work developing Cider, a WINE-derived Windows compatibility layer used to quickly port Windows games over to OS X. With the rise of Apple&rsquo;s fortunes and the move to x86, Transgaming has been responsible for either directly porting or supplying Cider to developers to bring a number of Windows games over to OS X. However in a blink-and-you&rsquo;ll-miss-it moment, back in March of this year the company announced that they were also going to get in to using their technology and expertise to port games over to architectures, partnering with NVIDIA to bring Metal Gear Rising: Revengeance to SHIELD Android TV.
Now just 3 months later NVIDIA is acquiring the GPG outright from Transgaming. This acquisition will see the group open a new office in Toronto, while structurally they are folded into the NVIDIA GameWorks division. And although NVIDIA doesn&#39;t state what precisely they intend to do with the group and its technology beyond the fact that the &ldquo;acquisition will enrich our GameWorks effort,&rdquo; it&rsquo;s a safe bet that NVIDIA intends to do more game ports for their SHIELD devices. Given their existing (if short) relationship, the acquisition is not too surprising, however it is a bit interesting since the bulk of the group&rsquo;s experience is with porting games among different x86 OSes, not porting games to new architectures entirely.
As for Transgaming, having sold the GPG to NVIDIA, the company has retained their SwiftShader (software 3D rendering) technology and their GameTree TV business. Transgaming has indicated that they are going to focus on providing apps for the Smart TV market, which they see as a greater growth opportunity than porting games.
Finally, while this acquisition will undoubtedly be a big deal for NVIDIA&rsquo;s efforts to bring more major games to SHIELD, perhaps the more profound ramifications of this deal will be what it means for Mac gaming. Though NVIDIA doesn&rsquo;t definitively state what they will be doing with Cider, the fact that they have their own platform to worry about certainly gives pause for thought. There are a large number of games that have received native Mac ports over the years, but Cider has still been used in everything from Metal Gear Solid to EVE Online. If Cider becomes unavailable to developers, then this may cut down on the number of Windows games that get ported to OS X, especially those games where marginal sales may make a native port impractical. In either case with this acquisition NVIDIA seems to have co-opted a lot of the technology and relationships behind Mac game porting, which should be a boon for their SHIELD platform.
</article>
</item>
<item>
<title>LIFX White 800 Smart Bulb Review</title>
<article>The Internet of Things (IoT) revolution has sparked an increased interest in home automation. Lighting is one of the major home automation aspects. LIFX is one of the popular crowdfunded companies in this space to have come out with a successful product. The success of their multi-colored LED bulbs brought venture capital funding, allowing them to introduce a new product in their lineup - the White 800. The launch of the White 800 also coincided with firmware v2.00 for the LIFX bulbs.
The initial products from LIFX were multi-color LED bulbs similar to the Philips Hue. The new LIFX White 800 is a white LED bulb with tunable color temperature. Color temperature is an important aspect in the lighting environment. In addition to visual comfort, it also affects human behavioral aspects. Different color temperatures are desirable for different human activities. Therefore, tunable color temperature in a single light bulb is a good thing to have. The specifications of the White 800 indicate 890 lumens of brightness (60W-equivalent),&nbsp; 25000 hours lifetime (22.8 years @ 3 hrs/day), 11 W power consumption and tunable color temperature from 2700K to 6500K.
The first generation LIFX bulbs relied on Broadcom&#39;s WICED platform. It also had a TI chip for 802.15.4 mesh networking. However, the White 800 gets rid of the mesh networking aspect and uses the QCA 4002 low-power Wi-Fi platform. This enables a lower price point for the White 800 compared to the other bulbs in the LIFX lineup.
At the heart of the unit is the lighting control module (LCM). LIFX also seems to be targeting this board towards OEMs in addition to using it within the White 800.
THe LCM documentation gives more insight into the internal components of the board.
The unit uses a Freescale Kinetis micrcontroller (ARM Cortex-M4-based) coupled with the Qualcomm Atheros QCA 4002. The QCA 4002 is very similar to the AR9330 used in the Ubiquiti mFi devices. The integrated CPU is MIPS-based. It is tuned for low power operation and, correspondingly, lower host CPU performance. The AR933x can run full Linux, but the QCA 400x is targeted towards embedded platforms. In the LIFX, the configuration (QCA 4002) is a 1x1 802.11n 2.4 GHz connectivity platform with the RF switches integrated.
The use of the QCA 4002 software stack on the Kinetis microcontroller allows for AllSeen / AllJoyn certification (the IoT standard backed by Qualcomm). The LIFX White 800 also carries the &#39;works with nest&#39; logo, thanks to the cloud back-end.
We have looked at the internal hardware in the LIFX White 800. In the next section, we look at what the average consumer sees - the setup and usage process.
</article>
</item>
<item>
<title>JMicron SSD Controller Roadmap: JMF680 SATA 6Gbps &amp; JMF815 PCIe Controllers Next Year</title>
<article>JMicron is getting ready to ship its new JMF670H controller to its customers and we also have reference design samples in for testing, but in its suite at Computex JMicron shed light to its plans for future controllers. We stopped by JMicron last year as well and the plans have since changed a bit.
JMicron is already working on the successor of the JMF670H, which will simply be called JMF680. That&#39;s still a SATA 6Gbps design, but it will bring support for TLC NAND thanks to what JMicron calls &#39;advanced ECC&#39;.&nbsp;JMicron is confident that its ECC implementation will be competitive against the LDPC engines that its competitors have and&nbsp;ultimately I believe that LDPC is more of a marketing gimmick at this point because everyone&#39;s ECC algorithms and implementations are slightly different anyway, but the market&nbsp;is associating strong ECC and TLC enablement with LDPC.&nbsp;
Another new feature in the JMF680 is increased capacity support that will go to up to 2TB. That is thanks to the updated (and larger) DRAM controller, which can now support up to 2GB as modern drives typically need about 1MB of DRAM cache per 1GB of NAND. The four NAND channels will also get an upgrade to Toggle 3.0 and ONFi 4.0 standards to support the upcoming NAND dies with faster interfaces. The JMF680 also supports Write Booster, which is JMicron&#39;s SLC caching feature that debuts in the JMF670H (more on that in our upcoming JMF670H review).
On the PCIe side JMicron has canceled the JMF810 and JMF811 controllers, and will now be focusing solely on the JMF815. JMicron made the decision to concentrate on the value segment and thus the JMF815 is a PCIe 3.0 x2 design with four NAND channels (no NVMe, unfortunately). A four-lane design would have required moving to 28nm process node, which would have increased the cost substantially and the packaging&nbsp;would have to move away from BGA to FCBGA (used by e.g. Phison and SandForce in their upcoming PCIe controllers)&nbsp;that would further increase the cost. I think it&#39;s a good play from JMicron to focus on a segment that isn&#39;t as populated because right now everyone is focusing solely on performance with PCIe, but ultimately cost and power consumption&nbsp;will be a major factors in widespread adoption and JMicron should have an advantage there if the JMF815 is executed well.
First engineering samples of the JMF680 and JMF815 are expected to be ready in Q4&#39;15 with first retail products entering the market in early 2016.
One of the trends I saw at Computex was the move towards DRAM-less SSD controllers. The JMF608 has been relatively popular in China given its ultra-low cost and its successor, the JMF60F, will be available within the next few months. It features an improved ECC engine and a larger capacity support as well as a new, cheaper QFN packaging. Following this trend, I wouldn&#39;t be surprised if&nbsp;JMicron also has plans for DRAM-less versions of the JMF680 and JMF815.
All in all, JMicron has a pretty solid roadmap for 2016. It&#39;s not aiming to be the performance leader, but to offer cost efficient designs for the value segment. We will have to wait and see how JMicron executes its PCIe controller, but in the meantime stay tuned for our JMF670H review that will be up in the coming weeks!
</article>
</item>
<item>
<title>Oculus Rift Controllers, VR Games, And Software Features Announced</title>
<article>On the eve of E3, Oculus held a livestream to announce some more details of the upcoming Oculus Rift Virtual Reality headset. Just about a month ago, they announced that they were targeting a Q1 2016 release, and with that time fast approaching, they have given some more details on the unit itself, as well as what kind of experiences you can expect with it. Oculus has re-affirmed the Q1&#39;16 launch date, and now we finally know the specs for the retail consumer unit.
One of the key points they brought up was that the unit itself needs to be comfortable, and part of that comfort is weight. January seems like a long time ago when I got to try out the Crescent Bay version of the Rift, but at the time I was impressed with how it felt, and I don&rsquo;t recall the weight at all which I guess is the point. The final, consumer version of the Rift in turn is close to the Crescent Bay version, with further enhancements for both the electronics and the overall fit itself to bring down the weight and make it more comfortable.
Audio is also a big part of the experience, and the included headphones on Crescent Bay were quite good. For the consumer version Oculus is going in a similar direction, but today they have also confirmed that you will be able to wear your own headphones as well if you prefer that. The directional audio is a key piece to the immersion and the Oculus team has done a great job with that aspect.
Another part though is the displays. When we met with Oculus&rsquo;s CEO Brendan Iribe at CES, one of the interesting things he told us was that they have found that by interleaving a black frame in between each video frame, it can prevent ghosting. In order to do this though, the refresh rate needs to be pretty high with the unit we tested running at 90 Hz. Today they announced a tiny bit about the hardware, and the Oculus Rift will ship with two OLED panels designed for low-persistence. Oculus has previously commented that they&#39;re running at a combined 2160x1200, and while they don&#39;t list the individual panel size, 1080x1200 is a safe bet. The OLED panels are behind optical lenses which help the user focus on a screen so close to their eye without eye strain, and the inter-pupil distance is important. There will be an adjustment dial that you can tweak to make the Rift work best for you.
Tracking of your head movement is done with the help of an IR LED constellation tracking system, unlike the Hololens which does all of the tracking itself with its own cameras. This makes installation a bit more difficult but should be more precise and reduce the overall weight of the head unit.
For those that wear glasses, the company has improved the design to better allow for glasses, and they also make it easy to replace the foam surrounding the headset.
One thing that was really not known yet was what kind of control mechanism Oculus was going to employ. In the demos I did at CES, there was no interaction, and you were basically a bystander. Oculus announced today that every Rift will be shipping with an Xbox One wireless controller and the just announced wireless adapter for Windows. This is a mutually beneficial agreement to say the least, with Microsoft getting in on the VR action and Oculus getting access to a mature controller design. Oculus even stated that the controller is going to be the best way to play a lot of VR games. However they also announced their own controller for a new genre of VR games to give an even more immersive experience.
Oculus Touch is the name of new controller system that Oculus has come up with. Each controller has a traditional analog thumbstick, two buttons, an analog trigger, and a &ldquo;hand trigger&rdquo; input mechanism. The two controllers are mirror images of each other, with one for each hand. They are wireless as well, and use the IR LED tracking system as well in order to be used in space. The controllers will also offer haptic feedback so that they can be used to simulate real world touch experiences. They also detect some finger poses (not full finger tracking) in order to perform whatever task is assigned to that pose. These should be pretty cool and I can&rsquo;t wait to try them out.

Hardware is certainly part of the story, but software is going to be possibly an even bigger part. The Rift needs to launch with quality games, and it looks like Oculus has some developers on board with EVE: Valkyrie, Chronos, and Edge of Nowhere being some of the featured games.
They also showed off their 2D homescreen which they are projecting into the 3D rift world. There will be easy access to social networks and of course multiplayer gaming in virtual reality.
In addition to the Xbox controller, Oculus has also worked with Microsoft to enable the upcoming Xbox Game Streaming into the Rift, so that you can be fully immersed. This will not magically make Xbox games 3D VR worlds, but instead will project the Xbox game into a big 2D screen inside the Rift and block out all distractions.
I&rsquo;ve been a bit of a VR skeptic, but my time with the Rift was pretty cool. I can see a lot of applications for this outside of gaming, but of course gaming is going to be a big part of VR and Oculus looks to be lining up a pretty nice looking launch. A big part is going to be quality titles for the Rift and Oculus is working hard on that aspect. The hardware is now pretty polished.
Source: Oculus
</article>
</item>
<item>
<title>Nantero Exits Stealth: Using Carbon Nanotubes for Non-Volatile Memory with DRAM Performance &amp; Unlimited Endurance</title>
<article>The race for next generation non-volatile memory technology is already on at full throttle. We covered Crossbar&rsquo;s ReRAM announcement last year and last week&nbsp;a very exciting company with a different non-volatile technology exited stealth mode and shed light on&nbsp;its technology and commercialization plans. The company is called Nantero and it&rsquo;s been developing its NRAM technology for well over a decade now.

Before we talk about the technology itself, let&rsquo;s briefly discuss the company and its key persons as Nantero is probably an unfamiliar name to many (it was for me, at least).&nbsp;The company was founded by Greg Schmergel,&nbsp;Dr. Tom Rueckes and Dr. Brent M. Segal in 2001. Mr. Schmergel and Dr. Rueckes are both still with the company and serve as CEO and CTO respectively, but Dr. Segal left the company in 2008 as a part of&nbsp;Nantero&#39;s Government Business Unit acquisition&nbsp;by Lockheed Martin.&nbsp;Mr. Schmergel is a well renowned serial entrepreneur who founded ExpertCentral&nbsp;that was later acquired by About.com where Mr. Schmergel served as a Senior Vice President before co-founding Nantero. While Mr. Schmergel brings valuable business expertise to the company, the technology comes from Dr. Tom Rueckes who is a Harvard Ph.D in chemistry&nbsp;and the inventor of NRAM technology.
The Board of Directors includes several semiconductor industry veterans.&nbsp;Mr. Lai was one of the leading developers of NAND technology at Intel and also led Intel&rsquo;s Phase Change Memory (PCM) team. Dr. Makimoto is a former Chief Technologist of Sony and Hitachi and Mr. Scalise is actually the former President of Silicon Industry Association (SIA) and also served as an Executive Vice President at Apple briefly in the late 90s. Mr. Raam may too be a familiar name to some since he is the former CEO of SandForce (the SSD controller company) that is now owned by Seagate.
It goes without saying that Nantero is packed with semiconductor experience and know-how, but its technology isn&rsquo;t any less interesting. NRAM is made out of carbon nanotubes, which is the strongest material known to man and provides far better thermal and electrical conductivity than any other known material.
The way NRAM works is in fact relatively simple. Essentially there are two nanotubes, which have low resistance when in physical contact and high resistance when separated. The amount of resistance then determines whether the cell is considered to be programmed as &lsquo;0&rsquo; or &lsquo;1&rsquo;. Program operation (or &ldquo;SET&rdquo; as Nantero calls it) works by applying a voltage on one of the nanotubes, which will then attract the other nanotube and create a bond. The SET operation is very fast and takes only picoseconds, which is on par with or better than DRAM latency.&nbsp;The bond is kept in tact by Van der Waal&#39;s interactions&nbsp;and is practically immortal with&nbsp;data retention terms even in 300&deg;C is over ten years. In an erase operation (or RESET as Nantero calls it) the voltage is simply applied in the other direction, which will &ldquo;heat up&rdquo; (given the scale it&rsquo;s more like vibration) the nanotube contacts and cause them to separate. Given that carbon nanotubes are one of the strongest materials in the world, the write/erase endurance is practically infinite as independent university study has shown Nantero&rsquo;s NRAM technology to have over 1011 P/E cycles (for your information, 1011 translates to 100 billion).&nbsp;
The other great news is that carbon nanotubes are extremely small. One nanotube can have a diameter of only 2nm and the pitch between the two nanotubes in off-state can be an even tinier 1nm, so the technology has potential to scale below 5nm.&nbsp;NRAM can also scale vertically, or go 3D,&nbsp;and since the cell structure and manufacturing process are both quite simple, 3D stacking should, in theory, be much easier compared to what 3D NAND is today with&nbsp;no need for high aspect ratio etching as an example.
The process of making an NRAM wafer starts by taking a normal CMOS wafer with the normal cell select and array line circuitry, which is then spin coated with carbon nanotubes. Carbon nanotubes are grown from iron that would normally contaminate a clean room, thus Nantero had to develop a patented process that creates &lsquo;pure&rsquo; carbon nanotubes with less than one out of billion particles being foreign (the standard for the highest quality clean rooms). Nantero has worked hard in the past two years to bring the cost of carbon nanotubes down and currently the company says that the nanotubes have a negligible impact on chip cost, meaning making NRAM isn&#39;t inherently more expensive than any other semiconductor.&nbsp;
With the nanotubes on the wafer, the top electrode is deposited on top of the nanotubes, followed by the&nbsp;photoresist, which is then&nbsp;patterned using a single mask. Finally the wafer is etched to cut the nanotubes into smaller pieces (i.e. more memory cells) and that&rsquo;s it in a nutshell. Obviously there are other general semiconductor processing steps involved, but those are the same for all memory technologies, so the fundamental process of manufacturing NRAM isn&rsquo;t that complex. All that is needed is a normal CMOS fab because the NRAM process requires no special or additional tools.
Fortunately, NRAM isn&rsquo;t just a technology that exists on paper. Nantero&rsquo;s NRAM process has already been installed in seven production CMOS fabs ranging from 20nm to 250nm and limited production&nbsp;has been taking place for several years now,&nbsp;although only in small few megabit capacities. As a matter of fact, Nantero completed a successful space test with NASA on Space Shuttle Atlantis back in 2009 where NRAM operated without any shielding throughout the trip without any errors despite the intense radiation, because&nbsp;as I mentioned earlier, the nanotube bonds are practically unbreakable and are not affected by heat, magnetism, radiation and the like.
Because Nantero is an IP licensing company, it relies solely on its partners for production. It&#39;s a logical strategy because a decent sized fab requires an investment in the order of billions of dollars and in the end the company would have to compete against Intel, Samsung and the rest of the semiconductor giants. Actual end products will be sold under the manufacturer&#39;s brand (e.g. Intel), so you won&#39;t see any Nantero branded products on the market.&nbsp;
Nantero isn&#39;t disclosing any of its partners at this point as most of them are still developing products that have the potential for higher volume production. While Nantero has its own chip team that is developing high capacity (several gigabits) dies, every partner is also doing its own work to implement NRAM at a larger scale, which makes sense given that the big semiconductor companies have far more resources and are familiar with high capacity memory devices.
Aside from semiconductor companies, Nantero has also partnered with several more consumer-facing companies to develop concepts and products around NRAM technology. Since NRAM provides the same level of performance as DRAM but is&nbsp;non-volatile, NRAM could open the doors for products that aren&#39;t achievable (at least properly) with today&#39;s NAND and DRAM technology. As examples Nantero mentions 3D smartphones and commercial 3D printers (although to be frank both already exist to some extent), but practically anything that&#39;s handicapped by IO performance and volatility can be fixed with NRAM in the future.&nbsp;
Since it will take several years before NRAM is even close to modern NAND capacities, Nantero has a three step strategy of bringing NRAM to the market. In the first step Nantero is simply offering a class&nbsp;of&nbsp;memory (both standalone and embedded)&nbsp;that has DRAM&#39;s performance characteristics and NAND&#39;s non-volatility. Technically that means NRAM is competing against current MRAM and ReRAM products for a specialized niche market that really needs high performance and non-volatility. The consumer market is obviously not one of those and even for the enterprise NRAM is likely too small capacity and expensive, but the industrial and especially space/military applications should benefit from NRAM despite the high initial cost.&nbsp;
The next step is to grow NRAM to gigabit-class capacities&nbsp;and offer a non-volatile alternative to DRAM. Going to gigabit-class certainly opens the doors for NRAM as a mainstream memory because it could be used for a variety of caching applications that benefit from non-volatility (SSDs with their DRAM caches for NAND mapping table are a prime example). Tape out of first gigabit NRAM wafers is still about 18 months away, so I would expect to see something shipping perhaps in late 2017 or 2018.
The final step, of course, is a terabit-class die to replace NAND (FYI, Samsung is projecting 1Tbit NAND die in 2017). Achieving that requires work on both lithography scaling and 3D integration technologies because such a high capacity die is only economical with either multiple layers or&nbsp;advanced lithography, or both.
NRAM also has the potential to operate in MLC mode for further density improvements, but for now Nantero is focusing on scaling NRAM down and adding layers through 3D to increase density. Once the work on those two is done and has been implemented to a production fab, Nantero will start commercializing NRAM MLC technology, but that is likely at least several years away.
The announcement is intriguing to say the least. From a technology standpoint NRAM sounds very exciting because it&#39;s effectively bringing us non-volatile DRAM performance, and better yet the cell design is scalable whereas DRAM has major struggles going below 20nm. I like the fact that Nantero has decided to go with IP licensing model because it means that NRAM is a technology available to everyone. The reason why DRAM and NAND are&nbsp;where they are&nbsp;today is because there are multiple companies producing them, resulting in competition with billions of R&amp;D dollars.&nbsp;&nbsp;
I wonder if any of the big semiconductor companies has partnered with Nantero yet.&nbsp;Most of them have been tight-lipped about their post-NAND plans, but maybe Nantero&#39;s announcement will sooner than later force the companies to talk about their strategies. Obviously a lot depends on how far 3D NAND can efficiently scale, but from what I have heard the transition to next generation memory technologies should begin around 2020. The future of memory isn&#39;t here yet, but it&#39;s certainly getting closer and it will be interesting to see what technology ends up taking the crown.
</article>
</item>
<item>
<title>Plextor M7e PCIe SSD to Ship in Q3, M7V TLC SSD in 2016 &amp; New Software Features</title>
<article>Plextor first showed off the M7e at CES earlier this year and at Computex we got an update on the release schedule. Plextor is now aiming for Q3 release, meaning that we will likely hear the final release at Flash Memory Summit in August. Specifications have not really changed since the M7e still utilizes the same Marvell PCIe 2.0 x4 AHCI controller with performance rated at up to 1.4GB/s read and 1GB/s write as well as up to 125K random&nbsp;read and 140K write IOPS. M7e will be available in both M.2 and PCIe card form factors with capacities range from 256GB to 1TB, so the M7e may very well be the first M.2 2280 drive to break the 1TB barrier.&nbsp;
Regarding the TLC drive M6V (or M7V as Plextor now calls it), Plextor is taking its time to fine tune the firmware to squeeze every megabyte of performance out of the drive and more importantly ensure high reliability and endurance. Plextor told me that its firmware can boost the endurance to 2,000 P/E cycles with 15nm TLC, so it the claim holds true then I&#39;m fine with Plextor taking a little longer and pushing the release to 2016.
On the software side, Plextor actually had three new items to show. The first one is updated PlexTurbo, which now carries version number 3 and increases the maximum cache size to 16GB. The cache size is also now user adjustable and supports multiple disks, so one can decide what Plextor SSD to speed up with PlexTurbo.
The first new addition to Plextor&#39;s software suite is PlexVault, which creates a hidden partition for storing sensitive data. The partition is completely hidden and isn&#39;t even visible in Disk Management, so other users won&#39;t even know that such hidden partition exists. Accessing the partition works through a hot key, although a password can also be entered to protect the hidden partition from accidental access. I&#39;m not sure how useful the feature really is, but I guess it creates another layer of security for NSFW (not safe for the wife) content for those who may need it.&nbsp;
The final&nbsp;piece of new software is PlexCompressor, which is an automated compression utility. If a file is not accessed for 30 days, PlexCompressor will automatically compress the file to increase free space. The file will then be uncompressed when accessed, which obviously takes a bit of the free space since the file will now be stored in uncompressed format for another 30 days.&nbsp;The compression is transparent to the user and is done fully in software (i.e. by the CPU), so it&#39;s not&nbsp;SandForce-like hardware compression. There is no impact on SSD performance, although as compression consumes some CPU cycles there may be impact on CPU heavy workloads and especially battery life. Out of the three pieces of software Plextor has, I think PlexCompressor is the most potent because it results in concrete extra free space for the end-user and with SSD prices still being relatively high (compared to HDDs) it makes sense to get the most out of the storage one has.
</article>
</item>
<item>
<title>Synology Launches RC18015xs+ / RXD1215sas High-Availability Cluster Solution</title>
<article>Synology is no stranger to high-availability (HA) systems. Synology High Availability is touted as one of the features that differentiate Synology&#39;s NAS units from other vendors&#39; for small business and enterprise usage. Put simply, Synology HA allows two NAS units (same model) to be connected to each other directly through their LAN ports, while also being connected to the main network through their other LAN ports. One of the NAS units is designated as the active unit, while the other passively tracks updates made to that unit. In case of any failure in the active unit, the other one can seamless take over without any downtime.
Synology is now extending this concept to a high-availability cluster. The products being introduced today are the RackStation RC18015xs+ compute node and the 12-bay RXD1215sas expansion unit.
Unlike Synology&#39;s traditional RackStation products, the compute node doesn&#39;t come with storage bays. They are just 1U servers sporting a Xeon E3-1230 v2 (4C / 8T Ivy Bridge running at 3.3 GHz) CPU. The specifications of the RC18015xs+ are provided below.
The PCIe 3.0 x8 slot allows for installation of 10 GbE adapters, if required. The compute node is priced at $4000. The expansion unit comes with the following specifications, and it is priced at $3500.
In order to set up a high-availability cluster, two compute nodes and at least one expansion unit is needed (as shown in the diagram on top). The operation of the cluster and high-availability features are similar to Synology HA. Performance numbers are of the order of 2,300 MBps and 330K IOPS using dual 10G adapters. All DSM (v5.2) features such as SSD caching and virtualization certifications are available. High-availability is also ensured with redundancy of hardware components (PSUs / SAS connectors / fans etc.).
The other important aspect of today&#39;s announcement is the usage of btrfs for the file system. As of now, the only COTS NAS units with btrfs support in this market segment have been those from Netgear and Thecus. So, it is heartening to see Synology also adopting it. btrfs brings along many advantages, including snapshots with minimal overhead and protection against bit-rot. The unfortunate aspect is that it is currently only available in this high-availability cluster solution. We hope it becomes an option for other NAS models soon.
Coming to the pricing aspect, we see that consumers need to buy two compute nodes and one expansion unit at the minimum, bringing the cost of a diskless configuration to $11500. This is pretty steep, considering that Quanta&#39;s cluster-in-a-box solutions (with similar computing performance) can be had along with Windows Server licenses for around half the price. Synology&#39;s products have always carried a premium (deservedly so for the ease of setup and maintenance), so it is not a surprise to see the pricing strategy here.
</article>
</item>
<item>
<title>Phanteks Computex 2015 Booth Tour</title>
<article>Phanteks had its new Enthoo EVOLV series cases in both mini-ITX and full ATX form factors&nbsp;on display in its suite at Computex.&nbsp;The mini-ITX version is made out of steel and available in two color schemes (white-black &amp; red-black). There&#39;s a single 200mm fan installed in the front with room for two 120/140mm fans at the top and one at the back. It can take a 330mm GPU and a 200mm CPU cooler, so you can build a fairly powerful system. One of the more special aspects of Phanteks&#39; cases is the PSU cover, which essentially hides the PSU cables to create the clean look that many desire.
The ATX version is fully made out of 3mm thick aluminum (despite the side window). For some reason the design and overall build remind me of the original Mac Pro, which isn&#39;t a bad thing at all.&nbsp;
One of the unique aspects in the case are fully modular hard drive bays. I have to say I really like the concept because typically many ATX cases easily have +5 irremovable bays, but in reality most users probably won&#39;t use more than one or two.&nbsp;Phanteks includes three with the case, but obviously the user can buy extra ones if needed.&nbsp;
There are actually two hard drive bays and SSD brackets behind the main chamber, so in most cases the user won&#39;t even need the modular HDD bays and can thus maximize airflow by not having anything between the fans and motherboard.&nbsp;
Phanteks also had a prototype of a dual-system case that can take a full ATX motherboard and a mini-ITX one. The interesting part is that Phanteks is working on a power splitter, so the two systems could be powered by a single PSU to save on space and cost. As you can see, the concept isn&#39;t really final yet because Phanteks needs to some custom cabling in order to be able to close the case since right now the cables come off too much. It&#39;s a niche product for sure, but the idea of running two full systems inside a decent size case is definitely alluring. See the gallery for more shots of the prototype and other cases Phanteks had to show!
Gallery: Phanteks Computex 2015 Booth Tour
</article>
</item>
<item>
<title>Lian Li Computex 2015 Booth Tour</title>
<article>Lian Li had close to a dozen new or prototype-level cases on display at Computex. I&#39;ve added most in the gallery at the end of this post, but I&#39;ll go through a few of the highlights here as well.
The first one is the PC-V33A, which is a box-like case in which the motherboard is mounted horizontally. The top cover is made out of single piece of aluminum, but it opens up for easy installation.&nbsp;
The case above is more of a conceptual prototype where Lian Li is playing around with a taller&nbsp;case design. Instead of having hard drive bays next to the motherboard, there&#39;s room for four hard drives in the top chamber, which allows for better airflow in the main chamber.&nbsp;
One of the more down to earth designs is the PC-K621, which is also Lian Li&#39;s first non-aluminum case. Traditionally Lian Li has kept the Lancool brand for value cases, but it seems that the company is trying to consolidate everything under a single brand now. The PC-K621 is made out of steel and plastic, but it does feel very sturdy and despite the fact that the front panel is made out of plastic, it has a metal-like look in it. Pricing will be about $70, so while it&#39;s not exactly a value case it&#39;s still considerably cheaper than the rest of the Lian Li cases.
One minor change Lian Li has made to its cases is changing the power button material from plastic to aluminum. The company received many complaints of the power button not having the same feel as the rest of the case, so as any respectable company Lian Li listened to its customers and made the change.&nbsp;
And obviously no Lian Li booth tour is complete without the computer desk case. Lian Li has modified the design a bit so that one can now easily sit with legs under the table, which was one of the issues the earlier cases had (note: that&#39;s Kip Hartwell, Lian Li&#39;s marketing rep, in the photo, not me). The desk is still expensive, though, and Lian Li doesn&#39;t really have any plans of making a value model, but it&#39;s a relatively small niche anyway.
Gallery: Lian Li Computex 2015 Booth Tour
Check out the gallery above if you&#39;re interested in seeing what else Lian Li had to offer!
</article>
</item>
<item>
<title>Microsoft Surface Hub Availability And Specifications Announced</title>
<article>Today Microsoft announced more news regarding the Surface Hub, which is their large-screen collaboration device. Built specifically for the business conference room, the Surface Hub packs some impressive features inside.
The first bit of news though is that the Surface Hub will be available for pre-order starting on July 1st, and shipments will begin in September. It will initially be available in twenty four markets, with the United States, Canada, Australia, Austria, Belgium, Denmark, France, Finland, Germany, Ireland, Italy, Japan, Luxembourg, the Netherlands, New Zealand, Norway, Portugal, Qatar, Singapore, Spain, Sweden, Switzerland, United Arab Emirates and the United Kingdom all getting first dibs on this new device. The prices will seem steep to anyone who does not often furnish a conference room, with the smaller 55-inch model being offered for $6999, and the 84-inch model will sell for $19,999.
That is not inexpensive at all, but it should actually be less expensive that some of the other conference room solutions, and yet pack in technology that they can&rsquo;t offer. Here is a table of the listed specifications:
The first notable aspect is the displays, which both feature optically bonded projective capacitance sensors to minimize reflections. Both models also can support an insane 100 points of concurrent multi-touch, and three simultaneous pen inputs. The 55-inch model is a 1920x1080 panel, and the 84-inch is a 3840x2160 resolution, and both have a refresh rate of 120Hz. The touch digitizer is also 120Hz, and according to Microsoft it makes the experience much more akin to an analog counterpart.
These will not be just displays to project to either. Powering the Surface Hub is a custom version of Windows 10, which is run on a Haswell Core i5 on the smaller model and a Core i7 on the larger one. The 84-inch model also jumps from integrated graphics to the NVIDIA Quadro K2200, which is a Maxwell based GM107 GPU with 640 CUDA Cores. Both Hubs come with 128 GB of SSD storage and 8 GB of RAM, as well as Gigabit Ethernet and 802.11ac, with Bluetooth 4.0 LE, NFC, and Miracast available.
They will also feature two front-facing stereo speakers, a four element microphone, two 1080p cameras each, as well as passive infrared presence sensors and ambient light sensors.
I think the steep price is going to keep these devices closely locked to their target audience of conference and meeting rooms. It would be very cool to have an 84-inch Windows 10 powered Smart TV, but for $20,000 it would be a tough sell.
I&rsquo;ve asked a couple of questions to Microsoft to get some more details about this device though, including its ability to handle HDMI 2.0 inputs and a few other things, so once I hear back I&rsquo;ll update the post.
Source: Microsoft
</article>
</item>
<item>
<title>Futuremark Announces VRMark: A Benchmark For Virtual Reality Systems</title>
<article>Today Futuremark&nbsp;revealed that they are in the process of developing a benchmark for virtual reality hardware and displays. In the same naming style as&nbsp;PCMark and 3DMark, this new virtual reality benchmark will be called VRMark. According to Futuremark, VRMark will use a combination of software and hardware to evaluate a system&#39;s ability to provide a high quality virtual reality experience.
Because virtual reality systems have many aspects that all need to be functioning properly to provide a good experience, the process of benchmarking them is different from how you would test a computer or a smartphone. VRMark will evaluate a system&#39;s ability to provide a high and consistent frame rate, as with virtual reality it&#39;s important to both have a high frame rate, as well as to ensure that the timing between those frames is consistent.
In addition to measurements of the hardware&#39;s ability to render and display frames in a timely and smooth manner, VRMark will evaluate the sensors located&nbsp;in a VR head-mounted display. Lowering sensor latency has been a big part of the development process for VR headsets, and VRMark will help companies and reviewers evaluate this aspect of VR system performance.
There&#39;s currently no word on when VRMark will be released, apart from the promise that it will launch in 2015. Virtual Reality benchmarks like VRMark will certainly be a useful tool&nbsp;to see how the various VR headsets currently being developed&nbsp;compare to one&nbsp;another.
</article>
</item>
<item>
<title>Apple’s Metal API Comes to OS X Desktops</title>
<article>At last year&rsquo;s WWDC, Apple introduced their Metal API for iOS 8. A low-level graphics API, Metal was originally designed to bring the benefits of low-level graphics programming to Apple&rsquo;s mobile operating system. And while we typically don&rsquo;t think of mobile devices as being GPU-bound, in reality Apple has been packing some relatively powerful GPUs like GXA6850 with what are relatively speaking still fairly weak CPUs, which means Apple has ended up in a situation where they can be CPU-bottlenecked on draw calls.
Metal, despite being the 3rd such low-level API to be introduced, was the first to reach production status. Microsoft&rsquo;s DirectX 12 is arguably not there yet (Windows 10 is still in testing), and Khronos&rsquo;s Vulkan was still in its primordial Mantle form at this point in 2014. What this means is that out of all of the vendors, it&rsquo;s arguably Apple who has the lead time advantage in low-level API development. Which is why for the last year we have been wondering if Metal would stay on iOS, or make the jump to OS X.
Yesterday we got our answer, with the announcement from Apple that Metal would be coming to OS X &ldquo;10.11&rdquo; El Capitan, and that it would be part of a larger investment into Metal for the company. Along with bringing Metal to OS X, Apple is going to be releasing new API kits that interface with Metal to simplify development, and internally Apple is now using Metal (when available) for parts of the desktop composition rendering chain. At this point it&rsquo;s fair to say that Apple has gone all-in on Metal.
Consequently the fact that Metal is now over on OS X is not unexpected, but whether it has been planned for or not, it means that we now have 3 low-level APIs on the desktop as well as mobile. OS X&rsquo;s Metal will be going head-to-head with Microsoft&rsquo;s DirectX 12 and Khronos&rsquo;s Vulkan, and this is the first time in a very long that we have seen a viable and competitive 3rd graphics API on the desktop, as DirectX and OpenGL have been the reigning APIs since the turn of the millennium.
As for what this means for Mac users, in the short run it&rsquo;s a good thing. With Vulkan still in development, had Apple not implemented Metal on OS X, OS X would have needed to stick with classic OpenGL for another year until OS X 10.12. Going with their own API, as was the case with mobile, gets a low-level API on OS X sooner. Furthermore because it&rsquo;s been on iOS for the last year, Apple gets to leverage all of the developer experience and code already written for Metal, and bring that over to OS X. Which is why developers like Epic are able to show off engines using Metal on OS X so early.
In the long run however there are some big unknowns left to answer, which could have a big impact on how things play out. Apple has not yet released the complete documentation for the newest version of Metal &ndash; specifically, we don&rsquo;t have feature lists &ndash; so how the Mac and iOS versions compare feature-wise remains to be seen. My biggest concern here is that Apple will put OS X and iOS at parity, essentially limiting the features available to the lowest common denominator of iOS, leading to Macs in general being behind the curve in graphics features. The other big question is whether Apple will support Vulkan next year once it&rsquo;s done, or whether they will stick with Metal, essentially turning OS X&rsquo;s graphics stack proprietary. Which for users could lead to a reduction in game ports to the Mac if developers have to go write against a Mac-specific graphics API.
One thing that is a pretty sure bet at this point is the GPUs that will support Metal. In short, don&rsquo;t expect to see anything that can&rsquo;t support Vulkan support Metal due to a lack of necessary features. So I&rsquo;m expecting Metal compatibility to start with Intel&rsquo;s Haswell (Gen 7.5) iGPUs, AMD&rsquo;s GCN dGPUs, and NVIDIA&rsquo;s Fermi/Kepler dGPUs. El Capitan works on a much wider range of machines of course, so this means only a fraction of those machines get to experience Metal. Though this was the same situation on mobile as well.
As for developers, things will be interesting. As I mentioned before Apple seems to be going all-in on Metal, starting with the fact that they will have Metal back-ends for their Core Graphics and Core Animation frameworks. And actually I&rsquo;m a bit surprised by this, as basic compositing is not something that is draw call limited. Apple is claiming upwards of 50% performance increases here, so I&rsquo;m curious just how this works out, but I suspect these are based on low-level benchmarks. Draw call performance is not the only benefit of Metal, but it is the most immediate, so Apple may be leveraging the harder to get GPU benefits here, or just wringing every last Joule of power out by getting to an API that isn&#39;t doing high-level state checking.
In any case, by building Core frameworks off of Metal, Apple is in a position where they have to ensure Metal drivers are working well, which is to the benefit of developers. Meanwhile Apple is going one step past Metal on iOS 8 with the release of MetalKit, which is a set of utility functions for Metal to help speed Metal development. As we&rsquo;ve mentioned before one of the few real pitfalls of low-level APIs is that to best utilize them you need guru-level programmers &ndash; after all, the API doesn&rsquo;t have high-level safety nets to keep developers out of trouble &ndash; and with MetalKit Apple is at least partially resolving this issue by providing some base functionality for programmers.
Wrapping things up, though not an unexpected move from Apple, it will none the less be interesting to see how their efforts with Metal go. As a tightly integrated vendor they have the advantage of being able to move quickly when they choose to, which is why we&rsquo;re seeing Metal come to OS X so soon and to get used by Core OS components so soon. Metal is just a graphics API, but due to Apple&rsquo;s timing OS X will be the real test for low-level APIs on the desktop, and not just for gaming. Apple is in an interesting position to take advantage of these new APIs like no one else can, so in several ways they are going to be the pathfinder on just what can be done with these APIs.
</article>
</item>
<item>
<title>be quiet! Showcases New Silent Base 600 Case &amp; SilentWings 3 Fans</title>
<article>be quiet! had two new products to show in its suite at Computex. The first one is Silent Base 600, which is a smaller and more affordable version of the Silent Base 800 that was first showed at last year&#39;s Computex. Typical to be quiet! brand, the company focused on building a quiet case, yet still offering excellent cooling performance. be quiet! will be offering models with and without the side window, and the case&nbsp;comes with two pre-installed Pure Wings 2 fans (140mm in the front and 120mm in the back) with support for up to seven fans (six in the windowed model). GPUs and CPU coolers of up to 400mm and 170mm are supported respectively and the Silent Base 600 will be available in September in three colors (orange, black &amp; silver) with MSRPs of $100 (no window) and $115 (with side window).
Gallery: be quiet! Silent Base 600 at Computex 2015
The other new product be quiet! showcased is the third generation SilentWings fan. be quiet! modified the shape of the blades to produce higher airflow without increasing the noise level and SilentWings 3 is also the only consumer-oriented fan with a 6-pole motor. Both PWM and non-PWM models will be available in October with the Euro MSRPs being about 20&euro;.&nbsp;
Gallery: be quiet! SilentWings 3 at Computex 2015
</article>
</item>
<item>
<title>IBM Pairs Xilinx FPGAs to POWER8 to Create an Education Cloud Service</title>
<article>Today IBM has announced &quot;SuperVessel&quot;, an OpenStack based&nbsp;cloud service&nbsp;that enables students and&nbsp;developers to develop applications on a POWER 8 based infrastructure. What makes this cloud service interesting is the announcement that&nbsp;Hemant Dhulla, Vice President of Data Center and Wired Communications for Xilinx made:
FPGAs, or field-programmable gate arrays are traditionally used to perform a specific algorithm in hardware. The result is a&nbsp;bulky and&nbsp;expensive chip (produced in low quantities) that runs a certain algorithms at very high speed and low latency.&nbsp;
Offloading some processing tasks to a specialized chip is certainly nothing new. APUs are CPUs that offload some of their tasks to integrated GPUs. But quite a few parallel algorithms run fast but pretty inefficiently on GPUs. In many cases, an FPGA uses a lot less power.&nbsp;
Intel has been delivering &quot;customized&quot; Xeons to large customers such as Amazon en Facebook, and has been promising that it will integrate&nbsp;Altera&nbsp;FPGAs&nbsp;inside certain&nbsp;Xeons.&nbsp;&nbsp;Intel recently bought Altera for $16.7&nbsp;Billion.&nbsp;
But IBM seems to have beaten Intel to the FPGA&nbsp;punch with CAPI, the POWER8&#39;s Coherent Accelerator Processor Interface. IBM does not integrate FPGA inside the POWER8 package (yet), but communicates coherently over the PCI express interface.&nbsp;
The most interesting fact about &quot;Supervessel&quot;&nbsp;that is IBM has managed to make a cloud service that makes ample us of - traditionally expensive - FPGAs, and that the necessary software is in place to make it relatively easy to make use of those FPGAs. &nbsp;What software did IBM implement to make offload some of the processing work to&nbsp;the Xilinx FPGAs? Unfortunately, so far we only saw the press release and it is very light on technical details. Nevertheless, it is interesting to note that the OpenPOWER Foundation is making a lot of progress in very little time - it was founded only at the end of 2013.
</article>
</item>
<item>
<title>SFF-8639 Connector Renamed as U.2</title>
<article>As SATA Express never took off because of the two-lane limitation, the SSD and motherboard industries have been looking for an alternative connector for connecting 2.5&quot; SSDs over PCIe. SFF-8639, which is essentially SATA Express on steroids with support for four PCIe lanes, has been viewed as the most potent connector because it already has industry support in the enterprise space and with the SSD 750 Intel brought the SFF-8639 connector to the client side. Given that SFF-8639 isn&#39;t a very consumer facing name (even I&#39;ve had trouble remembering the numbers), the SSD Form Factor Working Group has decided to rename the connector as U.2 to make it more marketable. That coincides well with M.2 that has already been used in the industry for a couple of years and more importantly both connectors now carry alike naming.
Whether U.2 and 2.5&quot; PCIe SSDs take off in the client space remains to be seen, though. The biggest hurdle is the expensive cabling&nbsp;because unlike normal SATA and SATA Express cables, the U.2 cable consists of several small, shielded cables that increase the cost. From what I have heard that is the reason why the industry came up with SATA Express in the first place because the OEMs wanted to keep the cabling cost equivalent to existing SATA cables.
I believe&nbsp;M.2 will be the main connector / form factor in the client space, but there is still a market for high performance and capacity 2.5&quot; PCIe SSDs as M.2 has more physical limitations that restrict the capacities and thermals. I can see U.2 in high-end motherboards where the connector and cable costs aren&#39;t that big of an issue, but we&#39;ll see what happens over the next year or two.
Source: Hardwarezone via PC Perspective
</article>
</item>
<item>
<title>Seagate Announces SandForce SF3500 SSD Controller Series: Mass Production Expected in Q4&#39;15</title>
<article>The SandForce SF3000 series has become the unicorn of the SSD industry. For the past two years there has been a lot of hype about the new controller, but Seagate/SandForce has kept missing deadlines one after the other. Initially the third generation SandForce controller was supposed to ship in mid-2014, but obviously that didn&#39;t happen. Next we heard Q4&#39;15, which was then changed to mid-2015 and the latest word I got at Computex is expected mass production in Q4&#39;15, meaning that we could see first products shipping in early 2016.&nbsp;
The original paper launch back in late 2013 only included the SF3700 lineup, which was first supposed to cater all markets from entry-level client to enterprise. Last week Seagate announced that the SF3000 series has now been split into two: SF3500 for the client and SF3700 for the enterprise market. The reasoning behind the differentiation doesn&#39;t only lie in marketing as the SF3500 and SF3700 are separate dies with the SF3500 having four NAND channels whereas the SF3700 keeps the 9-channel design as announced previously. It makes a lot of sense to build a separate die for the client space and honestly&nbsp;I was a little skeptical about cost efficiency because it&#39;s practically impossible to build a silicon that is feature rich enough for the enterprise with a consumer friendly price tag. We&#39;ve seen the client space moving towards 4-channel controller designs anyway, so I suspect introducing a smaller 4-channel controller was the only way SandForce could remain price competitive against Silicon Motion, which has taken a large share of SandForce&#39;s old clients.&nbsp;
The SF3500 series includes three SKUs and similar to the SF3700 silicon the controller supports both SATA 6Gbps and PCIe 2.0 (although only two lanes, whereas the SF3700 features four). The SF3514 and SF3504 are SATA and PCIe respectively, but the SF3524 has a switch that allows it to operate in both SATA and PCIe modes similar to the SF3700 SKUs. Unfortunately the switch isn&#39;t user-accessible as it has to be toggled by the manufacturer, so it&#39;s merely a feature that helps the OEMs with inventory management. The SF3524, being the high-end model, also has more supported firmware features, but Seagate isn&#39;t disclosing any at this stage, although I was told&nbsp;they are more back-end related rather than concrete features that are visible to the end-user.&nbsp;
Inherently the SF3500 is just a 4-channel version of the SF3700 and supports all SandForce technologies (DuraWrite, SHIELD, RAISE etc), but with one twist. The SF3500 is the first time SandForce is relying on external DRAM for caching the NAND mapping table, whereas the SF3700 and all older SandForce controllers only use the integrated SRAM. No user data is stored in the DRAM, so its function is purely to increase performance as well as reduce power up time when waking the system from sleep.&nbsp;
Another separating feature is the RAISE support. Because the SF3500 is client-oriented, it only supports level-1 RAISE, which uses one NAND die for&nbsp;protection against single page/block failures (in small capacity drives fractional RAISE can be used as it requires less than a full die). The SF3700 has full RAISE support and can operate in either level 1 or 2 mode&nbsp;with level-2 protecting against a full die failure (the 9th channel is there for that reason).&nbsp;
SandForce&#39;s focus in the SF3000 series has been mixed performance from day one. While most modern drives boast excellent peak read and write performance, nearly every drive experiences notable loss in performance under mixed read/write workloads. We&#39;ve been testing mixed performance as a part of our 2015 Client SSD Suite and I certainly agree with SandForce that it&#39;s an area where improvement is needed, which is what the SF3000 series is promised to do. Seagate&#39;s/SandForce&#39;s internal tests put the SF3700 at much higher performance efficiency than the competing NVMe drive that Seagate couldn&#39;t officially disclose (hint: the manufacturer starts with an I).&nbsp;
Gallery: Seagate SandForce SF3000 Series SSD Controllers at Computex 2015
All in all, Seagate seems to be making progress with the SF3000 series. It&#39;s inevitably late from the original launch schedule, but on the other hand there are only a handful of client PCIe drives on the market right now, so if the new schedule sticks and the SF3000 is as good as Seagate is showing it to be, SandForce will definitely be back in the game. The Computex&nbsp;announcement was relatively high-level, but&nbsp;Seagate hinted that they will have a truckload of new details to share within the next two months, so we will stay tuned for more.&nbsp;
</article>
</item>
<item>
<title>The Corsair CS450M PSU Review</title>
<article>Rightfully, there have been many requests for us to review medium-to-low wattage power supply units. This is more than reasonable, as the average home PC almost never requires a PSU with a maximum output greater than 550-600 Watts. On our end, it is a little difficult to source such units, both because there are few worthwhile models and because manufacturers are more eager to supply samples of their high-end/flagship models than they are their lower-end models. There are a number of assumptions one could make about why the manufacturers prefer to have only their top models reviewed, but we would rather stick to the facts.
One of the very few manufacturers that responded to our call for sub-500 Watt units and immediately dispatched a sample is Corsair. Corsair provided us with a CS450M, the modular 450W version of the CS series. The CS series is a low-to-mid tier power supply &ndash; not the cheapest series that Corsair currently offers, but still value-minded &ndash; aiming to combine good performance and a high value for money. On paper, the 80Plus Gold certified CS450M appears to be a good deal for the retail price of $80 including shipping. The specifications however rarely ever say anything about the true quality and performance of a PSU, which we will examine in the following pages.
Corsair supplies the CS450M in a relatively simple, serious cardboard box. It is smaller than the boxes of the higher end models and that is because there are no polystyrene foam pieces protecting the unit, only a bubble bag. The CS450M however is much lighter than a &gt;1kW PSU and the box is sturdy, therefore it should provide enough protection during shipping. The sides and the back of the box are littered with the specifications and the features of the PSU.
The bundle is exactly as we expected it to be - basic but not overly so. Corsair supplies a simple manual, the typical AC power cable, four black screws, and a few cable ties with the CS450M. This is nothing special but it is not that bad, considering that some companies even skip the AC power cable with their low cost models.
The CS450M is a semi-modular PSU, with the ATX and the CPU EPS cables hardwired to the unit while the rest of the cables are modular. There are only four modular cables, two with SATA connectors, one with Molex connectors and one with a single PCI Express connector.&nbsp; With the exception of the sleeved ATX cable, of the cables are &quot;flat&quot;, ribbon-like, with black wires.&nbsp;
</article>
</item>
<item>
<title>BitFenix Shows Massive Atlas Case &amp; LED Light Stripes</title>
<article>BitFenix had one new case&nbsp;on the show&nbsp;floor called the Atlas. It&#39;s a massive &quot;dual-chamber&quot; design that aims to build a very clean look since the PSU, hard drives and basically all the not-so-pretty components are placed behind the motherboard.&nbsp;
Gallery: BitFenix Computex 2015 Booth Tour
I&#39;ve added more shots of the Atlas in the gallery above. One interesting detail is the screw-less hard drive bays, although in a case this huge only four bays is a little limiting. Personally I feel that Atlas mostly wastes a lot of space by focusing solely on creating a cleaner look. Nowadays there are many cases that hide the PSU fairly well and with good cable management you can achieve a clean look without twice the size. I guess the Atlas could still have a niche for users who really need a lot of space for e.g. radiators, but I feel like it&#39;s more of a concept at this point rather than a final product.
While nothing new, BitFenix also had its Pandora mATX and ATX cases on display.
BitFenix had its own LED light stripes inside every case, which light the system up and give&nbsp;the user the ability to properly see the components inside. You only need two stripes for lighting the innards and obviously the stripes are available in multiple colors to fit everyone&#39;s preference. I asked about an RGB version, but I was told that and&nbsp;RGB version couldn&#39;t produce as pure colors and producing white light (which is one of the most popular models) wouldn&#39;t be possible at all. In addition, RGB would add unnecessary cost due to a controller requirement, so at least for now BitFenix is only shipping single-color stripes.&nbsp;
</article>
</item>
<item>
<title>HTC Pushing A Small Update To The HTC One M9 Today</title>
<article>Barring any unforeseen complications, HTC One M9 users in Europe, the Middle East, and Africa should be receiving&nbsp;an OTA update today which&nbsp;will bring&nbsp;the software&nbsp;to version&nbsp;1.40.401.5. This update rolled out in Taiwan a couple of weeks ago, and will be sent to North American units later this month. You can find the&nbsp;list of changes for this update below.
While this update is still Android Lollipop 5.0.2 instead of&nbsp;Lollipop 5.1, the&nbsp;improvements to battery life and camera processing will definitely be appreciated by users.&nbsp;
</article>
</item>
<item>
<title>Microsoft Launches Updated Xbox One, Controller, and PC Adapter</title>
<article>News comes from Redmond today that Microsoft will be offering a new model of Xbox One. It will ship with a larger 1 TB hard drive, which is double the storage capacity of the original Xbox One. It will also feature a new matte black finish, with the original Xbox One being outfitted in a glossy exterior. This is a small upgrade, but with the size of games getting larger and larger, it makes a lot of sense to offer the larger internal storage, even though the Xbox One did add the ability to use USB storage some time ago. As for the finish, I think the matte should be a good look and should help with dust and fingerprints.
Also being announced today, and being bundled with the new Xbox, is an updated version of the wireless controller. The big change is that Microsoft has finally ditched the proprietary headset connector, and has now moved to the normal 3.5mm jack. More subtle, but certainly notable, is that the bumpers have been tweaked, and the new controllers can now be updated wirelessly. The first gen controllers could also be updated, however they had to be connected to the Xbox One with a USB cable, so this should make it a lot easier to keep the controller firmware up to date. I have only updated my controllers one time because it&rsquo;s a bit of a pain to do so, but when Microsoft drastically reduced the connection time when powering on the controllers, it was worth my time to find a cable and do that. With the new ones, even less hassle is necessary which is always a good thing. There is also a new camouflage design in silver and black available for those that want something different.
The final bit of Xbox news today is that Microsoft is finally releasing a wireless adapter for Windows PCs, letting you use the Xbox One controllers on your PC with no cable necessary. The adapter is just $25, or bundled with a controller for $80, and should come in very handy for anyone who wants to do PC gaming with a controller, or more importantly for those that want to use the upcoming game streaming from their Xbox One to their PC, which is something I got to try first hand at Build. The wireless adapter is coming this fall.
The new 1 TB model of the Xbox One will be bundled with the Halo Master Chief Collection and available starting on June 16th for $399, with the 500 GB model being permanently moved to $349.
Source: Major Nelson Blog
</article>
</item>
<item>
<title>The Web is getting its bytecode: WebAssembly</title>
<article>In the quest for ever faster JavaScript, there has been a recurring refrain: why use JavaScript at all?
JavaScript engines have been a major focus of browser developers for some years, and the result has been substantial performance improvements from every vendor. JIT ("just-in-time") compilation that turns JavaScript code into instructions that can be directly executed on the processor brought huge speed gains. New data types have been added to the language to reduce the overheads when crunching numbers, and combined with asm.js, a high performance limited subset of JavaScript, applications running in the browser can achieve performance that's comparable with that of native code.
In spite of these improvements, the question of "why JavaScript?" remains. This is not without reason. The use of JavaScript incurs certain overheads: browsers have to read and interpret a text-based language that was designed for human authors, not for machines. The design of JavaScript itself has features that are suboptimal from a performance perspective; the way a single JavaScript variable may at different times represent a number, a string, or a fragment of HTML means that a JIT compiler may not be able to optimize as aggressively as it would like. The ability to modify the behavior of even built-in objects such as arrays can be similarly problematic.
JavaScript does have certain important advantages: it's a memory safe, sandboxed environment, meaning that (browser bugs excepted) JavaScript programs can't escape beyond the confines of the browser to access sensitive data or install persistent malware. JavaScript is also processor independent, so scripts will run just as well on an x86 PC as they will on an ARM smartphone.
However, there are well-known ways of providing the advantages of JavaScript without those perceived downsides: bytecode runtimes like Java and .NET. Unlike script files, the bytecode represents a low-level, fairly compact representation of a program. Bytecodes also tend to be much easier for computers to read and JIT compile. Bytecode systems tend to map cleanly to the underlying arithmetic capabilities of the processor, too; they tend to operate on simple integers and floating point numbers, thereby avoiding the complexity of JavaScript's object system.
As such, there has long been a degree of pressure to use a bytecode system for the browser. Both Microsoft and Sun (now Oracle) did this with .NET and Java, respectively, but these systems both depended on plugins, rather than being integrated into the browser's rendering engine the way JavaScript is. JavaScript programs could directly manipulate HTML objects; the plugins were instead off in their own world, separate from the HTML pages they resided in.
Google built a couple of systems to try to extend the browser to go beyond JavaScript. Native Client (NaCl) ran x86 (or ARM) programs in a secure sandbox, and Portable Native Client (PNaCl) did the same but using a kind of bytecode instead of x86 or ARM code. However, while Google championed these approaches other browser vendors largely rebuffed them. JavaScript was the lowest common denominator; it was the one thing that all browsers had to implement, so it was felt that it was better to make JavaScript better than to try to invent a whole new system.
As JavaScript became faster, the browser also became more capable. WebGL, for example, exposed hardware accelerated 3D graphics to the JavaScript developer. New APIs, to give access to, for example, games controllers, webcams, and microphones, have all been developed, extending the scope of what JavaScript in the browser can do. Simultaneously, a range of JavaScript-based-but-not-actually-JavaScript were devised. Microsoft's TypeScript, for example, offers various language features that Microsoft thinks are useful for the development of large programs by large teams. But browsers don't have to support TypeScript: the TypeScript compiler produces regular JavaScript that can run in any browser.
This kind of wide-ranging usage led Microsoft's Scott Hanselman to dub JavaScript the "assembly language for the Web," a sentiment largely shared by people such as Brendan Eich, who invented JavaScript, and Douglas Crockford, who invented JSON, widely used for JavaScript-based data interchange.
But the people calling for a bytecode for the browser never went away, and they were never entirely wrong about the perceived advantages. And now they're going to get their wish. WebAssembly is a new project being worked on by people from Mozilla, Microsoft, Google, and Apple, to produce a bytecode for the Web.
WebAssembly, or wasm for short, is intended to be a portable bytecode that will be efficient for browsers to download and load, providing a more efficient target for compilers than plain JavaScript or even asm.js. Like, for example, .NET bytecode, wasm instructions operate on native machine types such as 32-bit integers, enabling efficient compilation. It's also designed to be extensible, to make it easy to add, say, support for SIMD instruction sets like SSE and AVX.
WebAssembly will include both a binary notation, that compilers will produce, and a corresponding text notation, suitable for display in debuggers or development environments. Early prototypes are already showing some of the expected advantages; the binary representation is 20 times faster to parse than the equivalent asm.js.
The people behind wasm have not forgotten that JavaScript is supported everywhere and wasm is currently not supported anywhere. Their plan is to fill the gap with a polyfill; a JavaScript script that will convert wasm to asm.js for those browsers that don't have native wasm support. Either the browser will interpret the wasm directly, or it will load the polyfill and execute the resulting asm.js. Native handling should be faster, but the polyfill means that a developer can be sure that a wasm program will always work.
wasm is still in the early stages of development. There's no formal standards body behind it, just an informal community group. The specifications aren't complete and the high level design is still being decided. But with all four major browser engine makers working together, the future of wasm should be bright. And the JavaScript skeptics who have been crying out for a bytecode for so long will finally get their wish.
Listing image by Pablo BD
Expand full story
Peter Bright  / Peter is Technology Editor at Ars. He covers Microsoft, programming and software development, Web technology and browsers, and security. He is based in Houston, TX.
</article>
</item>
<item>
<title>Judge says Kleiner Perkins should get $276K from Ellen Pao</title>
<article>Judge Harold Kahn issued a tentative ruling (PDF) today awarding Kleiner Perkins $275,996.63 in costs after it won a high-profile gender discrimination case against former employee Ellen Pao. That's far less than the nearly $1 million that Kleiner had sought, although still more than Pao wanted to pay.
In 2012, Pao sued her former employer for gender discrimination, saying that she had been passed up on promotions because of her gender. She also alleged that when she began to complain about her treatment, she was retaliated against and eventually fired. A jury disagreed with Pao in March and ruled in favor of Kleiner Perkins on all counts. In April, the venture capital firm requested $972,814 from the plaintiff, but Pao contested the bill, saying it was “improper under the law” and “grossly excessive and unreasonable.”
Pao's lawyers argued in April that a recent California Supreme Court ruling suggested that expert witness costs could not be recovered from plaintiffs unless they had acted in bad faith—and $864,680 of Kleiner's fees request was expert witness costs. Pao also argued that Kleiner's original settlement offer was not made in good faith. Kleiner, for its part, argued that Pao was out of line in not accepting its settlement offer.
Judge Kahn disagreed with Pao that Kleiner's settlement offer was made in bad faith, but he wrote that he was inclined to “scale” the bill of costs down:
There is no doubt that KPCB has “vastly” greater economic resources than Ms. Pao. Nor is there any doubt that Ms. Pao is not indigent. While both her current employment and the likely continuing remuneration in the form of carried interests from her former employment at KPCB show that Ms. Pao has significant economic resources, it is also undoubtedly true that the $864,680.25 that KPCB seeks in expert fees is a material amount in the context of Ms. Pao’s resources. On the other hand, this amount is not a material amount in the context of KPCB’s resources.
“We’re pleased the court has reached a fair result,” a Kleiner spokeswoman told Ars. ”This tentative ruling recognizes that our settlement offer was reasonable and made in good faith. It also recognizes the cost rules still apply when a plaintiff refuses a reasonable settlement offer and forces the parties to go through an expensive trial.”
Pao's spokeswoman said she had no comment.
The two sides will meet in San Francisco Superior Court tomorrow morning to hash out the details on who owes what. It's unclear if Pao will appeal the jury's decision, but earlier this month Pao's attorneys filed a notice of appeal, giving them 40 days to explains their grounds for an appeal.
Expand full story
Megan Geuss  / Megan is a staff editor at Ars Technica. She writes breaking news and has a background in fact-checking and research.
</article>
</item>
<item>
<title>Quantum bouncer keeps light off the dance floor</title>
<article>My favorite experiments are not necessarily the groundbreaking ones. I love those too, don't get me wrong; but I like the ones that make me take a good hard look at the way in which I picture the physics.
One critical skill in physics is knowing what to leave out. For instance, if I can predict and describe a physical system with classical physics, why add quantum mechanics? But a recent paper highlights that it is always important to bear in mind that every classical picture has a quantum background. You may be able to neglect that background, but should never forget that it is there.
The experiment involves thinking about optical cavities, which are sort of on the border between quantum and classical worlds. Normally, we think about optical cavities in terms of the color, or wavelength of light that an optical cavity will accept. The distance between the mirrors must be commensurate with the wavelength. This can be described by both classical and quantum physics. The amount of light in the cavity, however, is almost always thought of in terms of classical physics.
Even when the light intensity drops to the point where we are considering individual photons, the optical cavity doesn't care. Shine the light on the cavity and light will keep building up within it until the rate of incoming photons equals the rate at which they leak out. Again, this is well described by both quantum and classical physics. Although the graininess of intensity jumps at the level of individual photons, in most cases, this can be ignored.
But the optical cavity is still quantum. It accepts one photon, then another, and another, up until the point where the mirrors are destroyed by the light intensity. But, on accepting a photon, the light in the cavity enters a new quantum state. So the infinite acceptance of photons has a caveat: the appropriate quantum state to receive it must exist.
What should happen, though, if there were missing quantum states? Say that a cavity could accept one photon, or two photons, but not three photons because the three photon state was missing? How would the light behave?
A similar experiment gets performed very often, but it's usually done at much higher optical intensities. Say we have an optical cavity that has a block of glass in it. If we shine a laser with the right color on it, the light starts to enter the cavity. As the intensity of the light in the cavity builds up, the glass responds to the intensity by changing its optical properties—typically the refractive index changes slightly.
Once this happens, the light suddenly finds that it has the wrong color and can no longer resonate in the cavity. The laser is also the wrong color, so no new light enters the cavity. The intensity of the light drops sharply, and the refractive index of the glass returns to its normal value. Under the right conditions, you can end up with a stable situation where the intensity of light in the cavity reaches a constant value. But, under other circumstances, the intensity of the light in the cavity fluctuates periodically because the light intensity overshoots and undershoots.
But, that is really a classical physics picture, which works because there are a lot of photons involved. What happens when the quantum nature of the cavity enters the picture?
This is exactly what researchers from France set out to investigate. The researchers did not use optical cavities, but switched to microwave cavities in the form of an aluminum box, which is like a microwave oven. Attached to this was a superconducting qubit (a ring of superconducting material interrupted by a gap of non-superconducting material). The aluminum box is entirely passive and just accepts whatever frequencies its dimensions allow. By itself, it behaves just like an ordinary resonator, so light can just build up to an intensity that depends on the intensity of the driving radiation and how long light stays in the resonator.
The qubit is slightly different. Although it is also a resonator, it can be tuned to resonate different colors of light. Like our block of glass above, that tuning depends on the intensity of the light it experiences, including light from the aluminum cavity. This, in turn, changes the resonant frequency of the aluminum cavity, because the light is influenced by both objects under the right circumstances.
The picture is like this: the researchers decide that they only want a maximum of two photons in the aluminum cavity, so they set the qubit to drive the cavity out of resonance for a light intensity corresponding to three photons. (They actually tried a number of different photon counts in different experiments.) This was done by choosing the qubit drive frequency appropriately.
The researchers found that the number of photons in the cavity was always below the blocking level. If the cavity is set such that it will accept a single photon, then the number of photons in the cavity always varies between zero and one in a periodic fashion. That seems just like the classical behavior, but with a twist. Classically, the resonant frequency of the cavity should only shift if the second photon enters the cavity. But, as far as the measurements show, that never happens. Somehow, the very possibility of a frequency shift prevents the second photon from entering the cavity.
You might be thinking, well, maybe there is a mistake here and, actually, the first photon shifts the frequency enough to prevent the second photon from entering. But we know this isn't true. If the first photon induced a frequency shift large enough to prevent the second from entering, the first photon would itself be quickly lost.
In other words, we would expect that the oscillation between zero and one photon would be much more rapid, because the light intensity in the cavity would decay much more rapidly. That is, the coupling between the microwave source and the cavity, and the reflectivity of the aluminum walls tells us how long we expect a photon to remain in the cavity. If the first photon shifted the resonance frequency, this value would be expected to fall. However, it does not appear to do so.
The researchers tried the experiment for limits of two, three, four, and five photons. In each case, they show that the intensity build up to a photon number that is just one below the set level, and that the intensity then oscillates between that number and zero.
The nice thing is that the whole experiment behaves a bit like an artificial atom being driven through excited states. Being artificial, it can be tuned and used for different experiments. It won't tell us anything much about real atoms, but maybe it can take the place of real atoms for various applications in quantum physics.
Even if it can't be used for other experiments, I don't really care. This is the sort of experiment that just blows my mind: it shows the clear and stark difference between our classical experience and the quantum reality that underlies it.
Science, 2015, DOI: 10.1126/science.1259345
Listing image by Flickr user woodleywonderworks
Expand full story
Chris Lee  / Chris writes for Ars Technica's science section. A physicist by day and science writer by night, he specializes in quantum physics and optics. He lives and works in Eindhoven, the Netherlands. 
</article>
</item>
<item>
<title>Review: iRig UA finally brings amp modelling to Android but lacks killer tone</title>
<article>ars.AD.queue.push(["xrailTop", {sz:"300x250", kws:[], collapse: true}]);If, like me, you're an Android-toting guitar player or musician, options for getting high-quality audio in or out of your smartphone or tablet have been rather limited. Class-compliant support for USB audio interfaces (essentially an external sound card) has long been baked into iOS via the USB camera connection kit for iPad, meaning that a wide range of audio devices can be used with that platform's plethora of musical apps. There are limitations of course—needing an iPad being a large one—but multiple third-party vendors now built audio interfaces specifically for use with iPhones as well.
Thankfully, IK Multimedia—a company known for its AmpliTube modelling software on the desktop and iOS—has stepped in to fill this gap on Android. The iRig UA is a £80 ($99) audio interface and software solution for Android 4.2 and higher devices. It lets you plug in a guitar and start bashing out riffs without the need for a hulking Marshall stack or expensive dedicated modelling hardware like an Axe-Fx. It's the sort of thing that, when I first started playing guitar many moons ago, I'd have absolutely gone nuts over.
Child-like excitement aside though, I'm not entirely convinced by the sounds on offer. While guitar amplifier modelling has come a long way over the last few years, unfortunately the iRig UA is still a few steps behind.
Part of the reason it's taken so long for devices like the iRig UA to hit Android is due to a lack of interest from Google itself; high-quality audio simply hasn't been a priority. Only towards the tail end of last year did Google introduce class-compliant support for USB audio interfaces in Android 5.0, but even that had some hefty stipulations attached to it to dramatically limit compatibility. The folks over at ExtremeSD, which makes its own custom audio driver for Android, have a good list of what does and doesn't work with Android's class-compliant driver. Suffice it to say, most audio interfaces have some issues working with the OS.
Limited or not, class-compliant support means that third parties can finally build audio interfaces for Android that work with a wide range of devices. The emphasis there is very much on "a wide range." Samsung did develop the Professional Audio SDK, which added high-performance audio processing and a USB MIDI driver to Android, but it only works on Samsung phones and tablets. One of the few devices to take advantage of the SDK was IK Multimedia's iRig HD-A, released towards the tail end of last year along with a special version of the company's AmpliTube software on the Samsung Galaxy Store.
Clearly, a more widely accessible solution was needed. Enter the iRig UA. The device itself is a small, lightweight, and unassuming bus-powered silver box that hooks up to any device sporting Android 4.2 or higher and supporting host mode/USB OTG (if you don't know if your phone supports host mode, you'll probably need an app to find out). The iRig UA works in conjunction with the AmpliTube UA app to deliver some decent guitar tones by emulating the effects of valve overdrive and distortion in software, manipulating the guitar signal into something that resembles what you might hear from a classic guitar amp.
"But wait a second, Mark!" I hear you cry. "I can just plug my guitar straight into my phone's headphone jack using an adaptor, why do I need this?" Well, yes, you can just hook your guitar up to your device's headphone jack using something like IK's own iRig adaptor. That comes with some compromises. The microphone preamp—the device that amplifies the weak signal of a mic to a higher line-level signal suitable for processing—on the vast majority of phones and tablets isn't really designed for instruments or high-quality audio. Instead, it's meant and optimised for speech during phone calls.
The thing about amplifying any signal is that, unless you've got a good preamp, the process of amplification tends to add noise. This is a particular problem for guitars, where the signal is pushed through further stages of amplification (digitally or otherwise) in order to produce the overdriven guitar tone that most electric guitar players crave. Using your device's headphone jack is fine if you're just jamming, but the additional signal noise it produces makes it a poor choice for recording, not to mention that the noise becomes irritating during a long session.
Because the iRig UA acts as a separate audio interface, it bypasses the device's preamp and opts for its own. Inside there's a 24-bit digital-to-analogue converter (DAC) that supports 44.1/48kHz sampling rate along with a low-noise instrument preamp.
Connectivity consists of a 1/4-inch (TRS) guitar input jack, a 3.5mm headphone jack, a headphone volume control, and a 3.5mm auxiliary input for plugging in an external audio device like an iPod. There's also a micro USB port for hooking up the iRig UA to your device (it's entirely bus-powered) and an input LED to show you when then input signal is too high and the audio starts clipping.
Using the iRig UA is as simple as plugging it into your device, after which it takes just a few seconds to power up and take over audio input and output duties. If you're running Android 5.0 or higher, the iRig functions as a straight up audio interface, letting you use it as an external DAC and recording solution. It works with other guitar modelling apps too. Sadly, given the dearth of good quality ones on the Google Play store, that's not much of a bonus just yet.
Still, the iRig's audio interface capabilities mean you can use it as DAC with its built-in headphone amplifier putting out a lot more volume than the Nexus 5, Sony Xperia Z3 Compact, and Nvidia Shield Tablet that we tested it with. The iRig UA also takes over microphone duties system-wide. That means if you get a phone call, you have to unplug the iRig UA or activate speakerphone to talk, unless of course you'd rather the person on the other end of the line hear some sweet licks.
The iRig UA's other headline feature—and where it differs from the similar iRig HD-A—is that it contains some (sadly unspecified) 32-bit digital signal processing hardware to take the load off of your device's CPU. Most Android devices have pretty powerful hardware in them these days, but depending on your device's configuration, lag can become an issue. This is something of a death knell for any modelling software. If there's a noticeable delay between striking a note and hearing it played back, it's incredibly hard to play in time. Modelling is particularly susceptible to this thanks to the amount of signal processing the takes place on the audio from your guitar.
The iRig UA software offloads the signal processing to the iRig UA hardware, resulting in a round-trip latency of just 2ms. In our own testing, there was no noticeable latency.
Expand full story
Mark Walton  / Mark is Gaming and Hardware Editor at Ars Technica UK by day, and keen musician by night. He hails from the UK, the home of ARM, heavy metal, and superior chocolate.
</article>
</item>
<item>
<title>OneWeb’s constellation of 700 low-altitude satellites will be built by Airbus</title>
<article>Airbus, the European aerospace giant, has won a contract to build 900 communications satellites for OneWeb's global high-speed satellite Internet service. OneWeb appears to be on target to launch 700 of the satellites in 2018. Both Richard Branson's Virgin Group and Qualcomm are still on board as major investors, and the project is expected to cost somewhere between £1.3 and £1.9 billion ($2-3 billion). SpaceX (with funding from Google) is also looking to compete in this same space, but its constellation isn't due until at least 2020.
If OneWeb doesn't sound familiar, don't worry, you're not going crazy. This company used to be called WorldVu, and it was founded by Greg Wyler, who previously founded the satellite Internet company O3b.
Where O3b provides access via eight satellites in medium Earth orbit (an altitude of 5000mi/8000km), OneWeb will use 700 satellites in low Earth orbit (500mi/800km altitude). The primary advantage of low-altitude satellites is that round-trip latency is much lower: O3b round-trip latency is about 240ms, while OneWeb could be as low as 50ms (just about comparable to your home ADSL connection).
Another advantage of using more satellites is that there's more total throughput available. A single O3b satellite can manage about 12Gbps over the 26-40GHz Ka band, which means there's a total network capacity of just 100Gbps split across the entire world. We don't know the exact specifications of the OneWeb satellites yet, but the target is 6Gbps per satellite over the 12-18GHz Ku band. Multiply that by 700 satellites and we could be looking at a theoretical total capacity of 4200Gbps or 4.2Tbps—much more respectable.
The disadvantage of having a constellation of 700 satellites, of course, is cost. While the OneWeb satellites are physically much, much smaller—about 150kg (330lbs) per OneWeb satellite vs. 4000kg (9000lbs) for a geosynchronous comms satellite—it costs a lot to build 900 of them and launch 700 of them into space. (The extra 200 will be held back on Earth until they're needed.) Airbus hopes that, because it's producing so many OneWeb satellites, it will be able to make them in a production line-like fashion and reduce costs significantly.
Airbus beat out Thales Alenia Space, Lockheed, OHB (a German space company), and SSL (Space Systems/Loral) for the contract. The big question now, however, is who is going to launch the satellites. According to Reuters, Virgin Galactic will launch "some" of the satellites with its still-being-designed LauncherOne rocket. And SpaceX, despite the fact it's working on its own satellite constellation, may be the launch partner for the majority of the OneWeb constellation.
The main engines sprout wings and propellers, and fly back to HQ like a drone.
The current plan is to have the OneWeb constellation in operation by 2018. The satellites will be slotted into 20 different orbital planes, providing consistent coverage of most of the globe. The ostensible purpose of the system is to provide high-speed, low-cost Internet access to parts of the world that don't have adequate last-mile copper or fibre infrastructure (i.e. a few billion people in Africa and Asia). Westerners stand to gain as well. For airplanes, ships, rural Europeans, and Americans out of range of ADSL... low-altitude satellite Internet could be very tasty indeed.
Ultimately, the success or failure of OneWeb's service will come down to cost. With an expected build/launch/insure cost of $2-3 billion, OneWeb will need to recoup its outlay somehow. Hopefully the plan is to offer really cheap access (say, a few dollars per month) and then to multiply that by tens or hundreds of millions of users. Low-altitude satellite Internet access has been attempted a few times—most notably by Teledesic, partly funded by Bill Gates—but so far no one has successfully pulled it off.
This post originated on Ars Technica UK
Expand full story
Sebastian Anthony  / Sebastian is the editor of Ars Technica UK. He writes about any and all aspects of science and technology. Read more about the launch of Ars Technica UK.
</article>
</item>
<item>
<title>The Raspberry Pi finally has an official case, priced at just $9 (£6)</title>
<article>More than three years after launch, there is now an official Raspberry Pi case. In keeping with Raspberry Pi's aspirational remit of bringing affordable computing to the masses, the new case costs just £6 (or $8.60 in the US). Rather fittingly, the new item features a dashing white-and-raspberry colour scheme.
Since the release of the Model A and B in 2012 and through the follow-up releases of the Model B+ and Raspberry Pi 2, an official case has always been one rather obvious omission from the product stack. Because the Raspberry Pi is designed for a range of uses—DIY maker machinations to low-cost educational computing—it never really made sense to provide one. Instead, the Foundation encouraged people and third-party vendors to make and/or sell their own. As such we've seen some wonderful cases over the years, including offerings made of 3D-printed plastic, Lego bricks, and even hand-crafted wood.
Now, the Foundation has stepped in. The official case for the Raspberry Pi 2 (and the Model B+) was designed in partnership with Kinneir Dufort. It's made of injection moulded plastic and comes in four parts: a raspberry-coloured main chassis and three clip-on panels (two for the sides and one that goes on top). You can remove some or all of the white panels depending on how much of the underlying electronics you want to display. Only the panel nearest the GPIO pins is solid and will need to be removed if you want to use it.
The official Raspberry Pi case is priced at £6 (RS Electronics, Swag Store, Element14) or $8.60 (MCM Electronics, Newark, Allied), which might sound impressive, but it's in-line with some of the cheaper third-party offerings already on the market. However, we'd be inclined to say the official case is a bit more attractive at first glance than some of the other cheaper cases.
This post originated on Ars Technica UK
Expand full story
Sebastian Anthony  / Sebastian is the editor of Ars Technica UK. He writes about any and all aspects of science and technology. Read more about the launch of Ars Technica UK.
</article>
</item>
<item>
<title>Time Warner Cable may face net neutrality complaint from webcam hoster</title>
<article>The first company to file a complaint against an Internet service provider under new net neutrality rules could be a webcam hoster that is demanding a free connection to Time Warner Cable's (TWC) network.
The company, Commercial Network Services, "plans to file a peering complaint against Time Warner Cable under the Federal Communications Commission's new network-neutrality rules unless the company strikes a free peering deal ASAP," according to a report yesterday at Multichannel News.Further ReadingNet neutrality takes effect Friday; ISPs scramble to avoid complaintsAT&amp;T settles with Cogent two days before interconnection complaints can begin.
But the company will have a hard time winning its complaint because the FCC does not ban payments for peering, also known as interconnection. The net neutrality rules that took effect last week prevent ISPs from accepting payments to prioritize traffic above other services over the so-called "last mile" of the network. But there's no ban on payments for interconnection, which is an exchange of traffic that happens at the edge of the provider's network. Interconnection allows traffic to flow into the provider's network without delay but doesn't speed it up thereafter.
The FCC's rules do let companies file complaints if they believe ISPs' payment demands are unreasonable, with each complaint judged on a case-by-case basis rather than against a specific standard. That threat alone has spurred ISPs to strike new peering deals with companies they've warred with, such as Level 3 and Cogent. Threatening to file a complaint may help Commercial Network Services win a better rate in negotiations, but an actual complaint isn't likely to result in no payment at all.
Commercial Network Services offers a number of managed IT services, but its webcam hosting business is the one involved in the dispute.
"CNS operates a number of Webcams that stream live video over the Internet," The Washington Post wrote. "It Web-casts one of the largest Fourth of July fireworks displays on the West Coast, and has amassed a large military audience that logs in from afar whenever its cameras show the comings and goings of U.S. Navy vessels based in San Diego. But TWC's actions are resulting in degraded video quality for those viewers, said CNS chief executive Barry Bahrami."
Bahrami said he's gotten free interconnection from companies such as Google and Cox Communications, and he claims that TWC's demands are a "blatant violation" of the FCC's rules.
TWC believes it is on solid ground. "Time Warner Cable enters into settlement-free peering arrangements with network operators that exchange large amounts of traffic at multiple locations and where there is a mutual exchange of value. Under TWC’s longstanding and industry-standard peering policy, Commercial Network Services does not qualify for settlement-free peering," the company said in a statement published by Multichannel News.
"There are many ways for a webcam operator like Commercial Network Services to exchange traffic with TWC and reach our broadband subscribers," TWC continued, noting that the company could buy access through third-party transit providers that connect directly to TWC's network. "TWC’s interconnection practices are not only 'just and reasonable' as required by the FCC, but consistent with the practices of all major ISPs and well-established industry standards. We are confident that the FCC will reject any complaint that is premised on the notion that every edge provider around the globe is entitled to enter into a settlement-free peering arrangement."
TWC is also involved in a dispute with Cogent, one of the third-party transit providers. Cogent has threatened to file complaints against ISPs that it can't come to terms with, but it hasn't done so yet. Cogent has been able to strike new deals with AT&amp;T and Verizon, while Level 3 has signed with AT&amp;T, Verizon, and Comcast.
Netflix has also fought with ISPs over peering payments, claiming it should be given free interconnection. But Netflix agreed to pay Comcast, TWC, Verizon, and AT&amp;T before the FCC approved its net neutrality rules.
Expand full story
Jon Brodkin  / Jon is Ars Technica's senior IT reporter, covering business technology,  the FCC and broadband, telecommunications, supercomputing, data centers, and wireless technology.
</article>
</item>
<item>
<title>Serious OS X and iOS flaws let hackers steal keychain, 1Password contents</title>
<article>Researchers have uncovered huge holes in the application sandboxes protecting Apple's OS X and iOS operating systems, a discovery that allows them to create apps that pilfer iCloud, Gmail, and banking passwords and can also siphon data from 1Password, Evernote, and other apps.
The malicious proof-of-concept apps were approved by the Apple Store, which requires all qualifying submissions to treat every other app as untrusted. Despite the supposed vetting by Apple engineers, the researchers' apps were able to bypass sandboxing protections that are supposed to prevent one app from accessing the credentials, contacts, and other resources belonging to another app. Like Linux, Android, Windows, and most other mainstream OSes, OS X and iOS strictly limit app access for the purpose of protecting them against malware. The success of the researchers' cross-app resource access—or XARA—attacks, raises troubling doubts about those assurances on the widely used Apple platforms.
"The consequences are dire," they wrote in a research paper titled Unauthorized Cross-App Resource Access on MAC OS X and iOS. "For example, on the latest Mac OS X 10.10.3, our sandboxed app successfully retrieved from the system's keychain the passwords and secret tokens of iCloud, email and all kinds of social networks stored there by the system app Internet Accounts, and bank and Gmail passwords from Google Chrome." Referring to interprocess communication, which is the tightly controlled and Apple-approved mechanism for one app to interact with another and the Bundle ID token used to enforce sandbox policies, the researchers continued:
From various IPC channels, we intercepted user passwords maintained by the popular 1Password app (ranked 3rd by the MAC App Store) and the secret token of Evernote (ranked 3rd in the free “Productivity” apps); also, through exploiting the BID vulnerability, our app collected all the private notes under Evernote and all the photos under WeChat. We reported our findings to Apple and other software vendors, who all acknowledged their importance. We also built an app that captures the attempts to exploit the weaknesses.
The Apple sandbox made its debut in OS X and uses the mandatory access control framework from the TrustedBSD project to enforce security policies at the system-call level. Since version 10.7.5, most apps submitted to Apple's Mac App Store are required to adhere to the sandboxing scheme. By default, the OS X Gatekeeper prevents users from installing apps unless they come from the store or come from a trusted developer that adheres to sandboxing requirements. iOS apps, meanwhile, have always adhered to strict sandboxing.
Despite the strict controls, the researchers found several ways a malicious app can surreptitiously access data from another app that's supposed to be off-limits. They included:
IPC interception: Browsers and other Internet-connected apps often use the WebSocket protocol to interact with extensions or other apps. Malicious apps can capitalize on this usage by preemptively taking control of the Internet port a trusted app uses to send or receive data through the WebSocket channel. The researchers wrote:
The security risks of intercepting the IPC communication through these vulnerable channels are realistic and serious. As an example, here we just elaborate our end-to-end attacks on three popular apps. We analyzed the 1Password app for OS X, which is one of the most popular password management apps and ranked 3rd by the MAC App Store. The app comes with a browser extension for each major browser that collects the passwords from the user’s web account and passes them to the app through a WebSocket connection. In our research, our sandboxed app created a local WebSocket server that took over the port 6263, before the 1Password app did, and was successfully connected to the password extension and downloaded the password whenever the user logged into her web account. We reported our findings to the 1Password security team, which acknowledged the gravity of this problem. This attack succeeded on OS X 10.10 (latest version when we reported the problem), against Chrome, Firefox and Safari. Our attack code passed the vetting process of the MAC Store.
A video demonstration of the attack is below:
In a detailed blog post published in response to the new proof-of-concept exploit, AgileBits "Chief Defender Against the Dark Arts" Jeffrey Goldberg wrote: "The fact of the matter is that specialized malware can capture some of the information sent by the 1Password browser extension and 1Password mini on the Mac under certain circumstances. But roughly speaking, such malware can do no more (and actually considerably less) than what a malicious browser extension could do in your browser."
Goldberg, whose company develops and markets 1Password, went on to advise users how to make themselves less susceptible to the attack and to thank the researchers for exposing the problem.
Password stealing is when a malicious app exploits a subtle design weakness in the OS X keychain, which stores passwords and other credentials used by browsers and other apps. By design, an individual keychain item is supposed to be available only to the app that originally added it, unless exceptions are explicitly approved. A malicious app can secretly allocate keychain attributes for apps that have not yet been installed in hopes the user will install them later. If the bet pans out, the malicious app will have full access to the credentials. In the case the malicious app is installed after a targeted trusted app is already in place, the malicious app can simply delete the password. Once the end user enters it again, the malicious app will be able to retrieve it.
A video demonstration of the attack follows:
Container cracking: Containers are designed to store the contacts, passwords, and other contents in highly restricted directories and subdirectories named after the BID assigned to the sandboxed app. The Mac App Store, however, fails to verify whether the subdirectory of a legitimate app is the same as one claimed by a malicious app.
"Once the attack app is launched, whenever the OS finds out that the container directory bearing the sub-target's BID (as its name) already exists, the sub-target is automatically added onto the directory's ACL [access control list]," the researchers wrote. "As a result, the malicious app gains the full access to other apps' containers, which completely breaks its sandbox confinement."
Scheme hijacking: OS X and iOS automatically associate certain URL schemes with a particular app so it can be opened when associated data is received from an Internet-connected server. URLs beginning with wunderlist://, for instance, will open the to-do list management app by that name and pass a variety of parameters to it. The researchers' malicious app was able to take ownership of the Wunderlist URL scheme and then steal the Google Single Sign On login credential transmitted by the browser. To prevent the targeted user from discovering the attack, the app then delivered the pilfered token to Wunderlist, causing the app to behave normally.
It's not the first time researchers have found flaws in application sandboxes. The attack exploiting WebSocket weaknesses, for instance, can also succeed in Windows under certain conditions, the researchers said. Interestingly, they said application sandboxing in Google's Android OS was much better at withstanding XARA threats.
For the time being, the researchers told Ars, there isn't much end users can do except wait for Apple to fix the vulnerabilities. At the request of Apple, the researchers delayed disclosing their findings for six months to give developers a head start in hardening OS X and iOS against the attacks. Since reporting the keychain vulnerabilities to Apple, company engineers started using a random username to patch some of its apps, a countermeasure the researchers said is ultimately "futile."
In light of the vulnerabilities, users of all OSes should limit the apps they install to those that are truly needed and explicitly trusted.
"The consequences of such attacks are devastating, leading to complete disclosure of the most sensitive user information (e.g., passwords) to a malicious app even when it is sandboxed," the researchers warned. "Such findings, which we believe are just a tip of the iceberg, will certainly inspire the follow-up research on other XARA hazards across platforms."
Listing image: Simon Greig
Post updated to add comment from AgileBits.
Expand full story
Dan Goodin  / Dan is the Security Editor at Ars Technica, which he joined in 2012 after working for The Register, the Associated Press, Bloomberg News, and other publications.
</article>
</item>
<item>
<title>Uber drivers are employees, California Labor Commission ruling suggests</title>
<article>The California Labor Commission has issued a ruling in favor (PDF) of a former Uber employee, ordering the company to reimburse her for costs incurred while driving for Uber. The Commission's decision says that Uber is liable for these costs because its drivers are employees of the company, something that Uber has been battling to disprove in several courts around the country. As employees, Uber drivers in the state would qualify for minimum wage, overtime, and worker’s compensation. Uber has maintained that its drivers are independent contractors.
Further ReadingStartup workers sue to be recognized as employees, not mere contractorsSlew of lawsuits target tech startups, and they just might succeed.The Labor Commission issued its ruling in favor of Barbara Ann Berwick, a former Uber driver who sued Uber in San Francisco Superior Court. Uber and other companies like Lyft and Homejoy, which seek to connect people offering services with people seeking services through an app platform, have seen an influx of lawsuits from employees and former employees, accusing the companies of keeping their margins low by passing the costs of running their business onto “contractors” who often pay for car insurance, gas, and maintenance out-of-pocket.
The Commission ordered Uber to pay Berwick $4,152 in reimbursable business expenses and interest. While that’s not a lot in this particular instance, the award sets a precedent that could threaten Uber's bottom line. Uber, a company with a multi-billion-dollar valuation, is appealing the decision.
The Labor Commission wrote, ”Product Manager Brian Tolkin testified that Defendant Uber is a technological platform, a smart phone application that private vehicle drivers (“Transportation Providers”) and passengers use to facilitate private transactions.” However, “By obtaining the clients in need of the service and providing workers to conduct it, Defendants retained all necessary control over the operation as a whole,” the commission found.
That Uber drivers perform work with their own cars doesn’t make them independent contractors, the commission also noted, citing as an example pizza deliverers, who often use their own cars to conduct a separate company's business, but who are still considered employees.
”Without Drivers such as Plaintiff, Defendant’s business would not exist,” the commission wrote.
In a statement posted to its website, Uber wrote:
“The California Labor Commission’s ruling is non-binding and applies to a single driver. Indeed it is contrary to a previous ruling by the same commission, which concluded in 2012 that the driver ‘performed services as an independent contractor, and not as a bona fide employee.’ Five other states have also come to the same conclusion. It’s important to remember that the number one reason drivers choose to use Uber is because they have complete flexibility and control. The majority of them can and do choose to earn their living from multiple sources, including other ride sharing companies.
Uber has no lack of antagonists in competing taxi and limo industries, and the president of the National Limousine Associations (NLA), Gary Buffo, said in a statement that his association "is very pleased with the fair and balanced ruling issued by the California Labor Commission,” adding that the NLA "is committed to upholding universal standards and best practices for all companies that employ drivers for private transportation.”
Ars wrote in March that recent legal action against companies that operate in the “1099 economy”—in other words, whose workforce is primarily made up of independent contractors—end up challenging documents that people sign before they begin working with the company. Still, plaintiff's lawyers maintain that those documents are often not ironclad.
[Correction: Ars changed the headline from "Uber drivers are employees, California Labor Commission rules" to "Uber drivers are employees, California Labor Commission ruling suggests" to reflect that this particular ruling only applies to one driver.]
Expand full story
Megan Geuss  / Megan is a staff editor at Ars Technica. She writes breaking news and has a background in fact-checking and research.
</article>
</item>
<item>
<title>Nest launches 2nd-gen smoke detector for $99, kills off Dropcam brand</title>
<article>Nest's Maxime Veron said that Nest has become better at reducing "nuisance alarms."
Nest's Maxime Veron said that Nest has become better at reducing "nuisance alarms."
The second generation Nest Protect will go for $99.
Dropcam has become Nestcam, and this is its final form (for now).
The camera is thinner and can be manipulated to fit in hard-to-reach areas.
Senior Product Manager Greg Hu came on stage to talk about how Nest products integrate into a customer's life.
The Nest.
Nest products all live together on the company's app.
The Next Protect controls.
Nest's in-app settings screen.
ars.AD.queue.push(["xrailTop", {sz:"300x250", kws:[], collapse: true}]);SAN FRANCISCO—Nest, the Google subsidiary devoted to home automation, announced a refresh to its product line Wednesday, which includes a second-generation smoke detector, a unified Nest app, and the rebranded Dropcam, now called "Nest Cam."
In a press event held in an art gallery, company reps touted the new Nest Protect smoke detector ($99/£89/€119), which features new dual-wavelength sensors that can detect "fast and slow moving fires" and can also distinguish between steam and smoke.
"We're really good at reducing nuisance alarms," Nest head of marketing Maxime Veron told assembled media, pointing out the new ability to disable the alarm from a smartphone. "When you burn toast, the last thing you want is the smoke alarm to yell at you."
The new Protect lacks the company's "Nest Wave" feature, which would allow users to silence an errant alarm by waving their hand under the smoke detector. Nest found that under "a unique combination of circumstances" the alarm could be disabled unitentionally. Now, if you burn the toast, you'll have to whip out your smartphone.
According to Nest's press release, the new Protect has been redesigned to be 11 percent smaller and now sits "more flush against the ceiling or wall." The light, used not only for alarms but also as a motion detecting nightlight in the dark, is now brighter. The new Protect is also equipped with a microphone, which the company says is used for a once-a-month sound check—the Nest beeps, and the microphone listens and confirms the beeping still works. We get the feeling that privacy advocates will be uneasy about the addition of a microphone.
With the Nest Cam, the Dropcam brand seems to be dead—besides the hardware rebranding, the cloud video subscription service is now called "Nest Aware."
Veron confirmed to Ars after the event that the Nest Cam's hardware (still $199) is "mainly the same" as the Dropcam Pro. The big addition over the Dropcam Pro is 1080p support. In the past, Dropcam reps have said the Pro is capable of 1080p, but the feature was disabled due to bandwidth issues. The Nest Cam isn't yet available in the UK or Europe; the website says it's "coming soon."
Further ReadingWhat Google can really do with Nest, or really, Nest’s dataHint: It's not home automation. The stand for the camera has been redesigned and now has a rare-earth magnet in the bottom, allowing it to easily be stuck to a metal object. Night vision has also been improved thanks to new algorithms that can tell the difference between the sun (for day/night mode) and a temporary light like a flashlight or headlight.
Nest is also launching a unified app that can control the Nest Thermostat, Nest Cam, and Nest Protect from a single app. We asked if Nest was finally adding geofencing to the new app, which would automatically turn the thermostat on and off based on the user's location, but sadly, the answer was no.
The company also announced a new program called the Nest Safety Awards to incentivize further Nest ownership. Under the plan, homeowners who have Liberty Mutual or American Family homeowners' insurance can get a free Nest Protect and up to five percent off if they agree to regularly transmit the fact that their Nest is on and operational to their insurance company.
"What's also important to us is privacy," Senior Product Manager Greg Hu said. "Personal info is never shared with insurance companies."
The Nest Cam and Protect can both be ordered now. The Nest Cam ships this month, and the Protect ships next month in the US. The app should be hitting the iOS and Android app stores today.
Expand full story
</article>
</item>
<item>
<title>Elite: Dangerous developers talk to Ars about Planet Coaster</title>
<article>Ars was in the front row for last night’s PC Gamer show, which lasted more than two hours and contained a whole mess of announcements and trailers (fun fact: I was on photo duty for the liveblog and took 1,256 photos!). Elite: Dangerous lead designer David Braben took the stage at one point for an announcement from his company, Frontier Developments—and the new title they revealed was just about as far away from Elite: Dangerous as possible. Frontier’s next game will be Planet Coaster, a sort-of-reboot, sort-of-fresh-start on the venerable Roller Coaster Tycoon series of sandbox games.
Frontier’s PR privately teased the announcement to Ars before the show, telling us only that they were unveiling a game they’d been wanting to make for more than a decade that contained “no space, no guns.” We suspected that the project might be roller coaster park-related due to Frontier’s January announcement of a similarly named project, which Frontier’s PR confirmed has now been renamed Planet Coaster.
After the PC Gamer Show, Frontier Development studio head David Braben was kind enough to take a few minutes to talk to Ars about the new title (we’d already talked to him earlier in the day about Elite: Dangerous, but we held our questions on the new game until this second meeting).
Why Planet Coaster instead of continuing on with the “Tycoon” branding? Frontier, after all, developed the third game in the Roller Coaster Tycoon series. But after so many “Tycoon” games—from roller coasters to dinosaurs to toilets—the brand just didn’t carry the cachet anymore. “We’re revisiting everything,” said Braben. “The whole ‘tycoon’ thing—we looked at it a lot, and we did plan and consider that for quite a while, and decided actually it comes with too much baggage. There were too many tycoon games that weren’t very good.
“Let’s do something fresh, is the intention,” he continued. “We’re doing that with each aspect—to revisit the genre properly and do it really well, and do every aspect of it really well… because we love it, and we’ve done so many coaster games over the years—we know how to do it.”
The new game will share an underlying engine with Elite: Dangerous—Frontier’s proprietary COBRA engine. In spite of its Elite-sounding name, the COBRA engine actually has a theme park heritage, being first developed to power Frontier’s 2004 Roller Coaster Tycoon 3. It also powered Frontier’s 2010 Xbox 360 game Kinectimals.
Braben was excited about the engine commonality, too, talking for a couple of minutes about the advantages of having an in-house engine for all games and actually bringing up a lot of the same points that Bioware hit on when they discussed unifying development on Frostbite with us on Monday. “Compare Kinectimals to Elite: Dangerous! It’s a broad range, but actually the engineering problems you have to solve are remarkably similar—weather systems on planets or fluffy fur on the head of a baby tiger.”
On the subject of gameplay and features—well, it’s becoming rather a refrain at this point, but Braben didn’t have anything solid to announce. The “planet” in the game does imply that there are some MMO-style components. While Braben dodged our questions with a smile (“We’re going to talk about that over the next weeks and months!” he said), Frontier’s PR confirmed to us that players will be able to build connected “villages” of parks and share coaster designs with each other.
To preemptively address the fears of Elite: Dangerous players concerned that a new game will sap developer resources away from Elite and slow that game’s continued development, Braben reiterated that Planet Coaster was being run by a different development team within Frontier. “This is absolutely a separate team,” he told us. “We’ve actually got more people on Elite than we had three or four months ago… If you look at the cadence of what we’re doing—we’re doing Xbox, and that’s not contracting what we’re doing on the PC, and we have great announcements for the PC to come.”
The game can be preordered directly from Frontier right now, and the company is also offering additional perks that can be purchased, all the way from early beta access to having your name indelibly printed on in-game paving bricks. We'll have to wait until 2016 for the release (the exact release date hasn't been specified), but we'll try to get some impressions and hands-on time with the beta once it's available.
Expand full story
Lee Hutchinson  / Lee is the Senior Reviews Editor at Ars and is responsible for the product news and reviews section. He also knows stuff about enterprise storage, security, and manned space flight. Lee is based in Houston, TX.
</article>
</item>
<item>
<title>FBI aerial surveillance revelations prompt backlash from US lawmakers</title>
<article>Revelations that the Federal Bureau of Investigation was operating a secret fleet of small aircraft spying on the public below has prompted a backlash of sorts.
Lawmakers in the US Senate introduced legislation Wednesday that would require federal authorities to get a probable-cause warrant from a judge to surveil the public from above with manned aircraft or drones.
Further ReadingCongress mulls law requiring warrant for e-mail data—yet againProposals to expand Americans' cloud privacy rights have gone nowhere.
"Americans' privacy rights don't stop at the treetops," Sen. Ron Wyden (D-Ore.) said of his proposal.
The Protecting Individuals from Mass Aerial Surveillance Act (PDF), also sponsored by Sen. Dean Heller (R-Nev.), comes two weeks after the Associated Press "traced at least 50 aircraft back to the FBI, and identified more than 100 flights in 11 states over a 30-day period since late April, orbiting both major cities and rural areas." What's more, the FBI obscured the planes' ownership through fake companies.
Ars' Sean Gallagher also described what he called the "FBI's secret surveillance air force—small planes with sensors perfected for battlefield intelligence in Iraq and Afghanistan that have quietly seen service all over the country." The planes are equipped with high-definition day and night surveillance systems.
Under Wednesday's proposed measure, information collected without a warrant during these surveillance flights would be inadmissible in court. Also, the US government could not contract with commercial or private plane operators to perform unauthorized surveillance.
Heller said the measure, which is being backed by civil rights groups, protects the public "from being trampled by the government's intrusion from above and provides much-needed clarity on what authority the federal government has related to aerial surveillance."
Further ReadingHow I tracked FBI aerial surveillanceThose mysterious planes overhead are actually government surveillance aircraft.
The proposal, however, is riddled with loopholes.
For starters, it does not apply to local and state law enforcement agencies. Further, the measure does not apply to immigration patrol within 25 miles of a land border, and all bets are off during "exigent" or emergency situations. Wildlife management and searching for illegal marijuana-growing operations are also excluded.
Even with the loopholes, the measure has slim chances of landing on President Barack Obama's desk for signature, as lawmakers are loath to demand probable-cause warrants to protect Americans' privacy.
The most recent example was the passage of the USA Freedom Act two weeks ago. That measure Obama signed alters the bulk phone metadata spying program Edward Snowden disclosed two years ago. Here's how Ars described the act:
Under the new legislation, however, the bulk phone metadata stays with the telecoms and is removed from the hands of the NSA. It can still be accessed with the FISA Court's blessing as long as the government asserts that it has a reasonable suspicion that the phone data of a target is relevant to a terror investigation and that at least one party to the call is overseas. As we've repeatedly stated, the Constitution's Fourth Amendment standard of probable cause does not apply. The metadata includes phone numbers of all parties in a call, numbers of calling cards, time and length of calls, and the international mobile subscriber identity (ISMI) of mobile calls.
If that isn't enough evidence that the measure is likely doomed, consider that current law does not require the authorities to get a probable-cause warrant from a judge to demand ISPs fork over customers' e-mail if that e-mail has been stored on servers at least 180 days. Lawmakers often talk about changing that President Ronald Reagan-era law, but there's no real political will to actually get it done.
Expand full story
David Kravets  / The senior editor for Ars Technica. Founder of TYDN fake news site. Technologist. Political scientist. Humorist. Dad of two boys. Been doing journalism for so long I remember manual typewriters with real paper.
</article>
</item>
<item>
<title>Caffeine could limit damage of chronic stress</title>
<article>During periods of chronic stress, we often up our caffeine consumption. This works better than you might expect—the increase can reduce some of the negative effects of long-term stress, including depression and memory deterioration. In a new study published in PNAS, researchers dug further into this finding, examining the signaling networks that caffeine influences within the brain. One of the proteins they identify is a potential treatment target for the symptoms of long-term stress.
Chronic unpredictable stress alters neural circuits in the hippocampus. It dampens mood, reduces memory performance, and increases an individual’s susceptibility to depression. The researchers studied this phenomenon in mice by exposing them to chronic, unpredictable, long-term stress in a variety of forms: cage-tilting, damp sawdust, predator sounds, placement in an empty cage, switching cages, and inversion of day/night light cycles. Just like humans experiencing chronic stress, the mice showed weight loss and memory deterioration. The mice also demonstrated helplessness and loss of interest in stimuli, which are markers of depression in mice.
After being chronically stressed, the mice were exposed to caffeine in their drinking water. As expected, caffeine reduced the mice’s depressive symptoms. Additionally, it improved the memory impairment in these mice, measured via recall of maze-based problem solving and object displacement.
Caffeine acts on proteins called adenosine receptors in the brain, so the researchers examined changes in adenosine A1 and adenosine A2A receptor behavior after exposure to chronic stress. In neuroscience, a chemical antagonist is a molecule that interferes with the chemical binding of another molecule to its receptor. In exploring chronic stress-induced changes to neurotransmitter function based on antagonists, they found that A1 antagonist binding was reduced by chronic stress, whereas A2A antagonist binding was enhanced. This indicated that the AzA receptors were up-regulated in response to chronic stress, which made it a strong potential target for a drug that could mimic the effects of caffeine.
In exploring the possibility of a drug therapy for the effects of chronic stress, they found that a chemical that selectively blocked the adenosine A2A receptor mimicked the protective effects of caffeine for chronically stressed mice. This therapy was effective immediately after the exposure to chronic stress had ended, and it was even effective several weeks after chronic stress exposure had concluded, indicating that it may have therapeutic value even if it can’t be administered during or directly after a stressful time period.
Similarly, genetic manipulation of mice to remove their A2A receptors was also protective against the negative effects of chronic stress. However, this finding is mainly used to support the data showing that A2A receptors were an effective target for chronic stress drug therapies—gene therapy to knock out a specific neuroreceptor in humans isn’t currently feasible, and it could come with severe side effects.
As a result of these two findings, the researchers conclude that A2A receptors play an important role in chronic-stress-related physiological changes. They suggest that adenosine receptors may have a general role in controlling mood disorders, which may make them appealing targets for therapies going forward.
PNAS, 2015. DOI: 10.1073/pnas.1423088112 (About DOIs).
Expand full story
Roheeni Saxena  / Roheeni is a Science Correspondent writing for Ars Technica. She holds an MPH from Columbia, and has worked as a bench researcher for Harvard Medical School and the NIMH. She currently serves as Associate Director of Educational Programs at Columbia’s School of Public Health. 
</article>
</item>
<item>
<title>AT&amp;T’s unlimited data throttling to be punished with $100 million fine</title>
<article>The Federal Communications Commission today said it plans to fine AT&amp;T $100 million for throttling the wireless Internet connections of customers with unlimited data plans without adequately notifying the customers about the reduced speeds.Further ReadingAT&#038;T still throttles “unlimited data”—even when network not congestedHalf-megabit speeds force customers to abandon unlimited data.
"The Commission charges AT&amp;T with violating the 2010 Open Internet Transparency Rule by falsely labeling these plans as 'unlimited' and by failing to sufficiently inform customers of the maximum speed they would receive under the Maximum Bit Rate policy," the announcement said.
The action isn't yet final. The FCC issued a Notice of Apparent Liability against AT&amp;T that includes the proposed fine and provisions designed to bring AT&amp;T into compliance with the commission's rules about making proper disclosures to customers.
AT&amp;T can ask the commission to reduce or eliminate the fine, which would be deposited into the US Treasury. But even if AT&amp;T opposes the fine, the commission says the company must file a report within 30 days detailing steps it is taking to correct misleading and inaccurate statements to consumers and to make more specific disclosures about data speeds. AT&amp;T was ordered to notify unlimited data plan customers that its "disclosures were in violation of the Transparency Rule, and that AT&amp;T is correcting, or has corrected, its violation of the rule with a revised disclosure statement."
AT&amp;T was also ordered to let affected customers cancel their plans without penalty.
"Consumers deserve to get what they pay for,” FCC Chairman Tom Wheeler said in the announcement. “Broadband providers must be upfront and transparent about the services they provide. The FCC will not stand idly by while consumers are deceived by misleading marketing materials and insufficient disclosure.”
The Open Internet Transparency Rule is the only major portion of the 2010 net neutrality order that survived a court review. The FCC could have tried to penalize AT&amp;T using the 2015 version of its net neutrality rules, but it chose the safer course of using a rule that has already been upheld. The FCC's latest rules are being challenged in court by broadband providers.
AT&amp;T started offering unlimited data in 2007 when it partnered with Apple to sell the iPhone. The company no longer sells unlimited data to new customers but allows existing ones to keep the plans.
"Although the company no longer offers unlimited plans to new customers, it allows current unlimited customers to renew their plans and has sold millions of existing unlimited customers new... contracts for data plans that continue to be labeled as 'unlimited,'" the FCC said. "In 2011, AT&amp;T implemented a 'Maximum Bit Rate' policy and capped the maximum data speeds for unlimited customers after they used a set amount of data within a billing cycle. The capped speeds were much slower than the normal network speeds AT&amp;T advertised and significantly impaired the ability of AT&amp;T customers to access the Internet or use data applications for the remainder of the billing cycle."
AT&amp;T had been throttling all LTE customers with unlimited data plans once they hit 5GB of usage each month, but the company changed the policy last month so that the customers are now only throttled when the network is congested.
In addition to the FCC fine, AT&amp;T is being sued by the Federal Trade Commission, which is seeking a court judgment that would bring millions of dollars of refunds to consumers.
The FCC said it believes millions of customers have been affected by AT&amp;T's throttling, with speed reductions that "imped[ed] their ability to use common data applications such as GPS mapping or streaming video." On average, customers' speeds were slowed for 12 days per monthly billing cycle, the FCC said.
We interviewed one of those customers last December; speed tests showed the customer's cellular speeds were reduced from 23.51Mbps to just 0.11Mbps.
AT&amp;T is gearing up for a fight. The company provided this statement: "We will vigorously dispute the FCC’s assertions. The FCC has specifically identified this practice as a legitimate and reasonable way to manage network resources for the benefit of all customers and has known for years that all of the major carriers use it. We have been fully transparent with our customers, providing notice in multiple ways and going well beyond the FCC’s disclosure requirements."
While it's true that other major carriers throttle unlimited data in certain instances, AT&amp;T was alone among the biggest providers in throttling speeds even when its network wasn't congested.
The proposed fine was approved by Wheeler's Democratic majority, while Republican Commissioners Ajit Pai and Michael O'Rielly opposed the punishment of AT&amp;T.
Pai said that "the Commission simply ignores many of the disclosures AT&amp;T made" and that "instead of coming up with a common sense metric, the Commission threw a dart and came up with a $100 million forfeiture."
The FCC said its rules allow for a fine of $16,000 per violation, but actually imposing that amount "in this case would lead to an astronomical figure" because so many customers were affected.
The commission chose $100 million because, even though it "represents a small fraction" of the revenue AT&amp;T has made from unlimited data plans, it is still large enough "to account for the gravity of AT&amp;T's violations" and "deter future violations by imposing a penalty that will not be viewed by AT&amp;T as a mere 'cost of doing business.'"
AT&amp;T reported revenue of $32.6 billion and operating income of $5.5 billion in the most recent quarter. Wireless revenue accounted for $18.2 billion of the total, with a wireless operating income margin of 24.5 percent.
Expand full story
Jon Brodkin  / Jon is Ars Technica's senior IT reporter, covering business technology,  the FCC and broadband, telecommunications, supercomputing, data centers, and wireless technology.
</article>
</item>
<item>
<title>Even former NSA chief thinks USA Freedom Act was a pointless change</title>
<article>The former director of the National Security Agency isn’t particularly concerned about the loss of the government’s bulk metadata collection under Section 215 of the Patriot Act.
As Gen. Michael Hayden pointed out in an interview at a Wall Street Journal conference on Monday, the only change that has happened is that data has moved to being held by phone companies, and the government can get it under a court order.
Hayden said:
If somebody would come up to me and say, “Look, Hayden, here’s the thing: This Snowden thing is going to be a nightmare for you guys for about two years. And when we get all done with it, what you’re going to be required to do is that little 215 program about American telephony metadata—and by the way, you can still have access to it, but you got to go to the court and get access to it from the companies, rather than keep it to yourself”—I go: “And this is it after two years? Cool!”
The NSA and the intelligence community as a whole still have many other technical and legal tools at their disposal, including the little-understood Executive Order 12333, among others.
That document, known in government circles as "twelve triple three," gives incredible leeway to intelligence agencies sweeping up vast quantities of Americans' data. That data ranges from e-mail content to Facebook messages, from Skype chats to practically anything that passes over the Internet on an incidental basis. In other words, EO 12333 protects the tangential collection of Americans' data even when Americans aren't specifically targeted—otherwise it would be forbidden under the Foreign Intelligence Surveillance Act (FISA) of 1978.
Expand full story
Cyrus Farivar  / Cyrus is the Senior Business Editor at Ars Technica, and is also a radio producer and author. His first book, The Internet of Elsewhere, was published in April 2011.
</article>
</item>
<item>
<title>Hands-on: Reaching out and touching someone with Oculus’ Touch controllers</title>
<article>Look ma... hands!
Look ma... hands!
This is it... the consumer Oculus Rift.
&nbsp;
&nbsp;
The prototype Oculus Touch controllers. Oculus says the final controllers will be quite similar.
&nbsp;
The demo room for sit-down, Xbox One controller demos
&nbsp;
CEO Brendan Iribe shows off the consumer Rift.
Sliding the Rift on.
Adjusting the fit using velcro straps on the sides.
Getting acclimated to the Oculus Touch.
Punch, punch, it's all in  the mind.
&nbsp;
&nbsp;
I'll just reach over here...
With the rise of the new virtual reality spurred on by Oculus these last three years or so, many have worried that playing behind an opaque headset will be more isolating and withdrawn than the experience of playing on a couch or even a monitor at your desk. After fiddling with a virtual room full of toys alongside another Rift user playing from the next room—using real hand and arm movements made virtual by the Oculus Touch controller—I have to say those fears are probably overblown. If anything, sharing a virtual space with someone else in this way made me feel more engaged and connected with another player than simply playing online or on the same TV.
Further ReadingEyes-on: Oculus’ Crescent Bay prototype is a new high-water markImprovements in comfort, resolution, and tracking make it hard to go back.The E3 demo was the first time those outside Oculus have been able to use the finalized consumer version of the Oculus Rift after years of using dev kits and prototypes. As far as audio and sound go, the experience seemed quite similar to the Crescent Bay prototype I first tried last September. The resolution is high enough to avoid the "screen door effect" of the Oculus dev kits, and the head tracking is rock-solid enough to make you feel like you're simply looking at a complete space that fully surrounds you. Oculus isn't talking exact specs, but the company did say the field of view has been expanded somewhat from the 100 degrees on the development kits.
The biggest changes in the consumer version are to the form factor. Oculus isn't discussing the specific weight, but the headset is incredibly light—perhaps about a pound—and easy to pick up with one hand. Unlike the dev kits, which are practically welded to your face with these elastic straps, the consumer Rift slides over your head like a baseball cap. A rigid spine wraps around the top of your skull and hovers the display just in front of your eyes.
A couple of velcro straps on this spine can be easily adjusted for a tight fit, a process that feels much more natural than the knob twisting and sliding of Sony's Morpheus. And the thick cable that weighed down the dev kits is now a single, unobtrusive cable that dangles out from the side of the display. Two small headphones flip down to provide the positional audio for the experience. The sound quality on these isn't great, but they can be replaced with a high-end set of ear cups if you want something with better fidelity (and more weight).
If you're just familiar with the cumbersome dev kits, you'll probably be amazed at how much the consumer Rift simply gets out of the way and lets you enjoy the feeling of being in a virtual space. You never quite forget it's on your head, but the distractions of what's going on in your eyes and ears are enough to make it a minor part of your perception, like the feeling of wearing a slightly heavy hat and pair of sunglasses. It's hard to know for sure after just a few minutes of demos, but it's easy to envision wearing this headset comfortably for hour-long play sessions.
Our Oculus demo started with a sit-down experience using the included Xbox One controller. I tried out a hockey game that put me behind a goalie mask, tasked with following the puck and tapping a trigger to block shots as they came in. It was a decent introduction to the concept of being in a first-person VR experience but nothing that special.
Insomniac's Edge of Nowhere felt more like a full game, with a third-person, over-the-shoulder perspective that you control simply by looking around rather than tilting an analog stick. This felt completely natural, but it was inherently a bit limiting. There was no way to turn the camera around a full 360 degrees, so the action mainly moved forward in a straight line with occasional short trips down diagonal paths. The VR environment surrounding me was suitably spooky, though, especially when descending a deep cavern with huge spiders chittering about the darkness on the glittering walls.
The highlight of our time with the Rift came when we were taken to a soundproofed, heavily air-conditioned room to try out the Oculus Touch controller prototypes. Holding them feels a bit like holding a Wii Nunchuk controller—complete with a thumbstick and trigger buttons—but with an added ring encircling the finger area.
That ring allows cameras to track your hands and sensors to track your fingers, but it also allows you to simply let go of the controller. If you open up your palms and spread your fingers, the controller stays in place, balanced against your palm and resting on the crook of your thumb and pointer finger. This means you don't have to maintain a constant fist-like grip, as you do when holding the wands that provide hand tracking on the PlayStation Move or VR controllers.
Sensors in the ring are what the Oculus Touch controller uses to track finger movements, though this feature was a little touchy in our demo. While the position tracking of our hands in virtual space was very fine and precise, finger tracking seemed to snap between an "open" and "closed" position without much refinement. What's more, aside from the thumb and pointer, the three other fingers only moved as a single unit in virtual space. The reaction to finger movements often felt just slightly delayed in the virtual world as well.
This is it... the consumer Oculus Rift.
&nbsp;
These are minor quibbles, though, for a controller that finally provides Oculus' solution to the "where are my hands" problem. In my demo, I was able to punch at a tetherball, bounce a ping-pong ball on a paddle, juggle blocks in my hands, aim and fire a slingshot by pulling it back and letting go, control an RC car with the thumbstick, and even throw and catch a boomerang. I did it all naturally, just by moving my hands and opening and closing my grip as I would in the real world.
I've been able to do similar things in other VR demos from the likes of Sony and Valve, but Oculus might just take the cake as far as sheer comfort and natural feel. Using the triggers and sensors to grip and point took a few minutes of getting used to, but after that it's easy to forget the light controller is really there. I felt like I was just using my hands rather than poking at the world with magical wands as in those other demos.
The only real problem with the Touch controllers, as it stands now, is that they're not the default for the Oculus Rift. CEO Brendan Iribe told us the Touch controllers would be added as an optional accessory slightly after the launch of the Rift, available by the end of the first half of 2016. The Touch controllers also currently require a second tracking camera to work, though Iribe says they are working to get that down to one by launch. And while Iribe insisted that the included Xbox One controller was the best interface for many VR games, he also suggested that developers that have just gotten their hands on the Touch prototypes would need more time to make compelling software that uses them. Let's hope they do so quickly, because being able to use your hands fills in an important missing piece in the VR experience.
While seeing and using my hands in the latest Oculus demo was impressive enough, the presence of another person in the virtual space was even more revelatory. Though my guide was standing in the next room, in virtual space his avatar was right there with me, appearing as a floating wireframe head and pair of hands across the table. We could hear each other talk through the Rift's built-in microphone and flipdown headphones. Every time he spoke, his virtual mouth showed a robotic distortion pattern like something out of an old sci-fi film.
More than anything else I've seen yet, sharing a virtual space like this seems like the killer app for Oculus. Even without any specific game to play, just being able to play with a virtual table full of toys with another person provided plenty of fun.
Everything about this shared play felt completely natural. When my guide handed me a ray gun or a slingshot to fire at some targets, I simply grabbed it out of his hand. When we wanted to play some table tennis, we cleared off the table with a sweep of our hands and hit the ball back and forth a few times. When he waved at me or offered a fist bump, I waved and fist-bumped back without really thinking about it. When he pushed a button and made my avatar minuscule, I cowered a bit at his ability to throw gigantic toys my way.
Coming out of the headset and returning to an empty, isolated room was a bit shocking after sharing such an intimate space. The only real limitation was the inability to walk around the space together—the Rift's camera-based tracking system is still limited to a tracking area that's only a few square feet. Still, the demo had me really intrigued to see what game developers do with the idea of having multiple people get together and manipulate virtual spaces.
Listing image by Sam Machkovech
Expand full story
Kyle Orland  / Kyle is the Senior Gaming Editor at Ars Technica, specializing in video game hardware and software. He has journalism and computer science degrees from University of Maryland. He is based in Pittsburgh, PA. 
</article>
</item>
<item>
<title>White House to Congress: Don’t use budget to kill net neutrality</title>
<article>The White House told Republicans in Congress yesterday that the nation's budget should not be used to enact "unrelated ideological provisions," including a proposal to prevent the Federal Communications Commission from enforcing its net neutrality rules.Further ReadingNet neutrality takes effect Friday; ISPs scramble to avoid complaintsAT&amp;T settles with Cogent two days before interconnection complaints can begin.
A letter to House Appropriations Committee Chairman Hal Rogers (R-Ky.) from Shaun Donovan, director of President Obama's Office of Management and Budget, detailed several provisions that the White House objects to, including the net neutrality prohibition. The Republican-controlled Appropriations Committee released the budget proposal last week.
The 2016 budget proposal "contains provisions aimed at delaying or preventing implementation of the FCC's net neutrality order, which creates a level playing field for innovation and provides important consumer protections on broadband service, and prohibits certain direct or indirect regulations that could independently prevent the order from being implemented," Donovan wrote to Rogers. "The Administration believes that the Congress should consider appropriations bills free of unrelated ideological provisions. The inclusion of these provisions threatens to undermine an orderly appropriations process."
The Republican proposal would prevent the FCC from enforcing the net neutrality regulations until courts rule on lawsuits filed by broadband providers that want the net neutrality order overturned. The budget provision is just one of numerous attempts that Republicans in Congress have made to limit or overturn the FCC's net neutrality rules, which took effect Friday.
The White House letter also objected to "the inadequate overall funding levels" in the Republican budget proposal.
"The bill cuts funding for the FCC by $98 million, or 24 percent, compared with the President's Budget," the letter said. "Because the FCC is funded by regulatory fees and auction proceeds, its funding level has no impact on the deficit, nor does it impact the amount of funding available for other agencies. Thus, these cuts unnecessarily force the FCC to scale back important work on public safety, wireless spectrum, and universal service, while increasing overall costs for taxpayers."
The White House also said inadequate funding for the IRS could prevent the tax collection agency from implementing technology upgrades that would protect taxpayer information. Besides that, the Obama administration "strongly opposes sections of the bill that limit IRS funding and transfers to carry out implementation of the Affordable Care Act, through which millions of individuals have signed up for coverage through the Health Insurance Marketplaces, and to provisions that unnecessarily encumber IRS operations with reporting requirements."
Expand full story
Jon Brodkin  / Jon is Ars Technica's senior IT reporter, covering business technology,  the FCC and broadband, telecommunications, supercomputing, data centers, and wireless technology.
</article>
</item>
<item>
<title>Call of Duty: Black Ops 3 multiplayer relinquishes realism in favour of fun</title>
<article>Say what you will about Call of Duty: Black Ops III's rather muted reception at Sony's E3 press conference this year, but judging by the multiplayer alone, it seems that there's life in the old girl yet. Updating COD's multiplayer is no easy task: aside from the obvious development resources needed, just how do you add new features while also pleasing—and above all, not alienating—one of the biggest and most passionate audiences around? The answer, it seems, is one of gentle refinement rather than all out reinvention.
At first glance, you might think otherwise of Black Ops III's multiplayer, especially when compared to Black Ops II. There's double jumping, wall running, swimming, automatic traversal, and even some pretty epic running slides for landing those KillCam winning moments. But these are now familiar ideas thanks to the likes of Titanfall and Advanced Warfare. What Black Ops III does is push them that little bit further, to the point where it's hard to imagine that the game ever had realistic(ish) warfare roots.
Black Ops III is the most game-like game in the series: a frantic heap of twitchy mechanics and seamless movement all designed to make mowing down opponents in multiplayer as satisfying and as skillful as possible. Smooth traversal is the first big change. Where previous games asked you to press a button to jump over a box or through a window, Black Ops III removes that button prompt. Instead, the game lets you sprint over it in one seamless motion. It does this in all directions; whether you're running forward, backwards, or sideways, the environment is there for you to glide through or across without interference.
Sprinting is now limitless too; augmented super soldiers of the future don't get tired after all. Removing this previous limitation opens up the game in a way that you wouldn't expect from such a small tweak. An already fast game is made even faster, almost verging into the likes of Unreal Tournament and Quake III. In such titles, movement speeds were ludicrously fast and unrealistic, but the games are perhaps more fun as a result. You can now slide along the ground while sprinting too (replacing dive-to-prone), removing yet another barrier to near limitless movement.
Then there's the new wall running ability, an extension of the wall climbing ability of Advanced Warfare and basically the same thing that we saw in Titanfall last year. Like all of Black Ops III's other tweaks, it's there to keep you moving. Walls, windows, and other objects that were once barriers designed to slow you down or keep you enclosed are now just ways of escaping gunfire or sneaking up on an unsuspecting opponent. Jumping itself has been tweaked as well. Now as you leap into the air, you can use a boost to extend your jump with little taps letting you float over huge distances. An extended button press sends you flying into the air like a rocket.
Coupled with the fact that you can aim and shoot during all these movements—and without your aim being interrupted by your gun pointing up in the air as you clamber over something—the game has been turned into a kind of bullet-filled ballet. Playing it is one thing, but seeing it being played is quite the sight, particularly when there are skilled, fast-paced players involved. No matter what, your iron (or red dot) sights can always be pointed at a foe whether you're galloping along a wall or doing a spot of swimming (yes, you can do that now too).
Perhaps the biggest and most disruptive change to the COD's multiplayer comes in the form of its Specialists, the multiplayer super-soldiers you play that each come equipped with a special ability. Developer Treyarch hasn't taken the wraps off all the Specialists yet, but it has at least detailed some—and boy are the purists going to hate these guys. Each character has both a special high-powered weapon and special ability, but they're mutually exclusive and the player has to choose which to use before the start of the match. They're also limited by a charge meter that fills as you run around (or more quickly as you get kills).
Despite all that, it's easy to see why players are going to be divided on the Specialists: their weapons are undeniably cool, but they're also extremely powerful. Take the large-calibre revolver of the Seraph called the Annihilator, which—as its name suggests—completely obliterates an opponent in a messy pile of bodily parts with just a single shot. The same goes for the Outrider's combat bow, its explosive-tipped arrows having the satisfying tendency to explode opponents from the inside out.
Plus, Ubisoft introduces the Siege Generator for constantly changing challenges.
Other Specialists include the Reaper, a combat robot whose arm turns into a minigun; Prophet, who's equipped with a lightning gun that can chain-kill other opponents in the vicinity; and Nomad, who can fire a gun that deploys a swarm of nanodrones that surrounds and kills opponents. How well these characters work in a real-life multiplayer scenario, especially after the community has figured out ways to exploit them, remains to be seen. But in the demo room at E3 at least, the Specialist powers were a hell of a lot of fun.
Sure, there may have been a few frustrating moments when I was on the tail end of a shot from the Annihilator, but when I was the one wielding the gun, exploding opponents into bloody bits was oh so satisfying. To anyone who still lusts after the olden days (uhh, a few years ago) of realistic weapons and quick deaths, these changes might seem like the end of days for the series. But I'd urge you to give them a try. I had an absolute blast leaping around the map like a lunatic and exploding bad guys before being mowed down by a robot with a minigun for an arm.
Yes, we're now at the point where we're taking about a COD game that features a robot with a minigun for an arm as a playable character in multiplayer. But at its core, somehow, Black Ops III still feels very much like a COD game. This is a series that has been chopped and changed and tweaked many times over without somehow losing its core identity. Love it or hate it, that's an amazing achievement.
This post originated on Ars Technica UK
Expand full story
Mark Walton  / Mark is Gaming and Hardware Editor at Ars Technica UK by day, and keen musician by night. He hails from the UK, the home of ARM, heavy metal, and superior chocolate.
</article>
</item>
<item>
<title>Amazon undercuts its high-end e-reader with new 300 PPI Kindle Paperwhite</title>
<article>At $199, even a great reading experience has limited appeal beyond its core audience.
We liked Amazon's high-end Kindle Voyage e-reader when we reviewed it last year, but our biggest concern was that the $199 dedicated reader didn't do enough to distance itself from the $119 Kindle Paperwhite. Today that extra purchase gets even harder to justify, since Amazon just added the Voyage's 1448×1072, 300 PPI E-Ink Carta screen to the Paperwhite without raising its price. The new version is still $119 with Special Offers advertising and $139 without. Adding 3G boosts the price to $189 with ads and $209 without.
The Kindle Voyage still offers a handful of benefits over the Paperwhite—it has an automatic brightness sensor, a screen that sits flush with the surrounding bezel, and capacitive buttons in the bezel for page turning. But none of those features affected the reading experience as much as the screen, which used its extra sharpness to make E-Ink text look as much like actual printed text as possible. Putting that same excellent screen in something that costs $80 less makes the Voyage difficult to recommend.
Otherwise the new Paperwhite doesn't change much. The Wi-Fi version weighs 7.2 ounces (205 g) and the e-reader is 0.36 inches (9.1 mm) thick. 2.4GHz 802.11n Wi-Fi is used to download books to its 4GB of internal storage. The touchscreen is the only way to turn pages and type, since it lacks both the physical buttons of older models and the capacitive buttons of the Voyage. And the E-Ink Carta screen technology, while not new to the Kindle lineup, still offers better contrast and less ghosting than the E-Ink Pearl screens used in older and cheaper Kindles.
The new Kindle Paperwhite can be pre-ordered today, and it will begin shipping June 30.
Expand full story
Andrew Cunningham  / Andrew has a B.A. in Classics from Kenyon College and has over five years of experience in IT. His work has appeared on Charge Shot!!! and AnandTech, and he records a weekly book podcast called Overdue. 
</article>
</item>
<item>
<title>Stephen Elop out as Microsoft merges Windows and devices groups</title>
<article>Stephen Elop, the one-time Microsoft exec who left the company to become CEO of Nokia and then returned after overseeing the sale of Nokia's devices division to Redmond, is to leave Microsoft as a result of a reorganization.
On his return to the software giant, Elop took on the role of Vice President of the Microsoft Devices Group. That group is being merged into Operating Systems Group, forming a new Windows and Devices Group. This will be led by Terry Myerson, who previously led the OSG.
The Cloud and Enterprise (C+E) group, led by Scott Guthrie, is similarly expanding. The Business Services Division, which owns the Dynamics range of products, is being folded into C+E, and as such its leader, Kirill Tatarinov, is also leaving the company.
Eric Rudder, whose Microsoft bio indecisively describes him as both Vice President of Advanced Technology and Education, and Vice President of Advanced Strategy, is also leaving. The Education responsibility is moving to Qi Lu, as part of his Application and Services Group.
Mark Penn, the "Chief Insights Officer," will also be moving on from Microsoft in September, though the company says that this last departure is unrelated to the engineering group restructuring. The Wall Street Journal reports that he's starting a company that will invest in "digital marketing services." The new company, Stagwell Group, has raised $250 million in funding, with former Microsoft CEO Steve Ballmer a core investor.
Expand full story
Peter Bright  / Peter is Technology Editor at Ars. He covers Microsoft, programming and software development, Web technology and browsers, and security. He is based in Houston, TX.
</article>
</item>
<item>
<title>Comcast source: Company not interested in buying T-Mobile [Updated]</title>
<article>Updated @ 10:47 EST, 15:47 BST: A source familiar with Comcast's thinking told Ars after this article published that it is not interested in buying T-Mobile. The source requested anonymity because the company tries to avoid making official comments on rumored mergers.
Since 2012, Comcast has has a commercial partnership with Verizon Wireless that allows it to sell Verizon Wireless products in bundles with Comcast cable services. This partnership would also allow Comcast to resell Verizon Wireless service under its own name beginning next year.
"Four years from signing, Comcast could become a reseller of Verizon Wireless' service through a Mobile Virtual Network Operator (MVNO) agreement. Comcast could purchase Verizon Wireless' service at wholesale rates and then market and sell its own, branded wireless service in connection with our bundled offerings, creating more choice for consumers," Comcast's December 2011 announcement of the agreement said. The deal was approved by the FCC with some conditions in 2012.
Comcast is reportedly talking to the owner of T-Mobile US, Deutsche Telekom, about buying the wireless carrier.
The report was published today by Manager Magazin in Germany (where Deutsche Telekom is based) and summarized by Reuters:
Deutsche Telekom is in talks with U.S. cable company Comcast about a potential sale of T-Mobile US, German Manager Magazin reported on Wednesday, citing sources.
Deutsche Telekom is in talks with several parties, including satellite provider Dish, according to the magazine, but Comcast is viewed as a more attractive buyer by the German telecoms provider's management.
Comcast would be a better candidate as it is financially stronger and would be able to make an offer to buy all shares in T-Mobile US, Manager Magazine reported.
Deutsche Telekom and T-Mobile shares each rose about 3 percent upon the report of a possible sale to Comcast, according to Bloomberg. Further ReadingPossible Dish/T-Mobile merger could be trouble for AT&amp;T and VerizonT-Mobile's problem is spectrum—Dish owns a ton of spectrum and hasn't used it.
The German telecommunications company has been trying to sell T-Mobile for years. Deals with AT&amp;T and Sprint were both abandoned because the US government opposed a merger that would leave the US with three major nationwide carriers instead of the current four.
Rumors of Dish Network trying to buy T-Mobile surfaced a couple of weeks ago.
Deutsche Telekom told Ars it "does not comment on rumors and speculation." We also contacted Comcast and T-Mobile this morning but have not heard back.
Comcast, the largest cable and broadband company in the US, tried to buy Time Warner Cable but admitted defeat on the merger in April because of opposition from the Federal Communications Commission and Department of Justice. US regulators were concerned Comcast could use its increased size to stifle the competition that online video streaming services pose to cable TV.
T-Mobile US CEO John Legere has said that he is interested in partnering with a cable company.
Expand full story
Jon Brodkin  / Jon is Ars Technica's senior IT reporter, covering business technology,  the FCC and broadband, telecommunications, supercomputing, data centers, and wireless technology.
</article>
</item>
<item>
<title>Elite: Dangerous at E3: Xbox exclusives and Q&amp;A with David Braben</title>
<article>E3 is in full swing, and while Kyle and Sam are off taking their meetings, I stormed the Los Angeles Convention Center and made immediately toward Frontier Developments’ meeting room on the upper tier of the convention center. There, amid dim mood lighting and soft carpets, I got a chance to see what Frontier had to show off for E3—namely, Elite: Dangerous on the Xbox One.
Of course, as a card-carrying PC gamer, this didn’t excite me as much as it does Ars Automotive Editor Jonathan Gitlin (who is positively foaming to plug in his controller and blast off), but it is a big deal for the game—opening things up to an entirely new audience with entirely different expectations. Although the game just passed 500,000 active players (after its 1.0 release in December 2014), series creator David Braben recognizes that the console market represents a huge swath of potential new players.
We spent several minutes on the Xbox version of Elite, and after getting my bearings it was a pretty easy transition to go from the Warthog HOTAS I usually play on to the Xbox’s controller. The controls have been collapsed so that every major flight system is available from the controller, either through direct button press or from holding down modifier buttons.
Unfortunately, the most immersive gameplay element of Elite—head tracking—won’t be available on the Xbox. We asked the Frontier devs if they had any intention of utilizing the Xbox’s Kinect to do facial recognition and head tracking (which on the PC enables players to open and close control panels simply by looking at them), and at least for now, there are no plans to do so.
Head tracking aside, the game played great. The graphics appeared to lack some of the high detail touches visible with maximum settings on a PC, but that’s to be expected—Elite: Dangerous is designed to look good even at 4k resolutions, and it’s going to be impossible to equal how good the game looks on a high-end PC. Still, at 1080p the game looked sharp and appeared to our eye to be running at a smooth 60 frames per second even in areas that on the PC are prone to some stuttering and lag (namely, within space stations).
Frontier has announced an arena combat-style game mode called “Close Quarters Combat,” or CQC—which will be a timed exclusive on the XBox One version of Elite: Dangerous. Braben and his team had no release dates for us—saying that CQC is coming “soon” for Xbox One and will be on PC “this year”—but we got to spend some time with the new game mode.
CQC takes place separate from the main game universe and is intended to be a quick join, quick action PvP matchmaking mode where players can jump and get immediately to action. To that end, players don’t play CQC with their own ships, but rather with a limited selection of four small ships: the Eagle, the Sidewinder, the Viper, and the newly playable Federal Fighter (which executive producer Ben Dowie said was his favorite).
CQC matches are arena style, and we witnessed players fighting in and around a large space station. My first impression is that CQC shares some elements with Descent—the ships flew around and through the space station, zipping through cramped station utility conduits almost like the Millennium Falcon storming the Death Star. We saw a single capture-the-flag style game mode, with players flying around fighting over a “data sphere.” To match with the quick-twitch gameplay, the basic targeting mechanism has been changed so that losing line-of-sight on enemy ships breaks your targeting lock, as well.
Playing more CQC matches unlocks CQC-specific rewards like new equipment, and there will be a CQC leaderboard, but Frontier is very adamant that CQC won’t impinge on the main game—you won’t be able, for example, to earn monster weapons in CQC that are usable in the main game. They might offer non-gameplay-affecting rewards, like ship decals based on CQC skill, but some of the design details are still up in the air.
The timed exclusivity of CQC is going to leave a bad taste in some PC players’ mouths, especially those who have been with the project for years and contributed to Elite: Dangerous’ Kickstarter or alpha and beta phases. But designer David Braben sees CQC as an essential outreach to new console players who might at first be overwhelmed with the substantial Elite: Dangerous learning curve. “That kind of game play is not really intended for the PC player,” said Braben. “You want to switch on, play immediately, and switch off again… it’s a good way to bring the game to console, because we get a lot of attention for it. You’ve got to balance the fact that that audience wants something, and we will bring this to PC later.”
We then took the opportunity to hit David up with our entire list of questions about what’s coming next for Elite: Dangerous, and first on that list was whether there would be more free updates, or if next up are paid updates like the much-discussed planetary landings.
“There is more free stuff to come, but I don’t want to pre-announce what they are!” laughed Braben. “But, yes, there are paid things to come as well. I’m very, very excited about where we are.” Braben was unfortunately cagey on actual upcoming features we might see, but he did re-confirm that both the planetary landings and walk-around-the-ship mode were coming—and, importantly, that those features would come to PC first.
On that last point, Producer Ben Dowie reiterated that Xbox One and PC players won’t be playing head-to-head—although they’ll be playing in the same simulated universe, they’ll never encounter each other in space, likely because Microsoft’s Xbox patch cycle adds complexity to Frontier’s game update procedure. This means that PC players and Xbox players will often wind up on different clients, which means no head-to-head play. To that end, anticipated PC-centric features will likely land on PC first.
Speaking of anticipated features: we asked Braben where the rest of the promised ships were, since there are still multiple announced-but-unreleased ships like the Panther Clipper and the Federal Corvette. “There are more to come,” Braben said slowly, “and we haven’t announced when those will be, but they are in plan.”
“Are there any ships that haven’t been announced that are still coming?” I asked.
Braben smiled, then nodded and said, “There’s more.”
The latest Elite: Dangerous update adds a strategic element to the game that not every player has been excited about, and we asked Braben about how he sees Power Play affecting the general player base—and if he sees it as an optional or a mandatory component of the game.
“It’s not mandatory,” Braben said. “What Power Play is there for is for people who have really seen a lot of the game and who want to interact on a higher level, and that was lacking. Actually, personally,” he continued, “I would rather a new player not participate in Power Play… it’s already quite complicated! But then once you’ve gotten to that level and you’re really into the game, you can go, ‘oh wow, that’s really cool, I can take part in this!’”
“What we’re putting in is a feature with which people can choose to engage with, but you don’t have to,” he said. “The game is very similar to what it was before… but it doesn’t really impinge on you unless you choose to support a particular power or not.”
Several of the main occupations in Elite went through a re-balance with the latest major patch—mining and bounty hunting, in particular, with one becoming more profitable and one significantly less. We asked Braben about this rebalancing. “The trouble with any sort of balancing is that there will always be some people who will say, oh that’s terrible! When we launched the game, I think we got it slightly wrong, and the trading was the standard profession to do to get money—and it felt quite grind-y. So, bounty hunting was made better, and people found ways to make it very, very lucrative—more lucrative than we intended. We’re trying to make it with all of these things that you can do the profession that you most enjoy.”
I pointed out that there’s frequent contention online about the “right” way to play, be it casual or hard-core, and Braben agreed. “But there shouldn’t be a ‘right’ way,” he said. “You should do what makes you excited. I don’t want there to be a ‘right’ way, because then you’re not necessarily playing the way you want to play. And people have come up with lots of suggestions, some of them very constructive and sensible, and we do listen, and people hopefully have seen that we’ve changed things and adjusted things, but not in a way—we hope!—to upset people. We’re doing it to make the game better!”
We then ran down sort of a grab-bag of other questions. More ship decorations are coming, but there’s no word yet on custom paint jobs or decals—nor is there any word yet on being able to name your ship. There are no changes planned to separate solo and online saves, and players will continue to inhabit the same shared galaxy whether they’re in solo or multiplayer—again, continuing with Braben’s contention that there’s no ‘right’ way to play.
More standardized online gaming conventions like clans or formalized player organizations aren’t in the cards, at least not for the foreseeable future. VR support for headsets other than the Oculus Rift will be handled on a case-by-case basis, but Frontier believes very strongly in VR and definitely wants to go in that direction. And there aren’t any plans at this moment to expand beyond 32 players in each instance—but it’s always a possibility in the future.
As we wrapped, Braben wanted to reiterate that the CQC timed exclusive shouldn’t be considered a sign of a shift in priorities from Frontier, and that, in his words, “There are more exciting things coming to PC this year.”
Are one of those things the Elite universe’s big bad guys, the Thargoids? Braben gave us the same answer to that as he’s given us in the past: he smiled, paused a moment, and said, “They are coming.”
Correction: This piece originally said that "third-person walk-around-the-ship mode" was one of the upcoming features. This was a mistake by the author, who is suffering from E3-induced psychosis. No point-of-view was specified and what we know of the walk-around-the-ship mode remains consistent with the descriptions in the Elite: Dangerous design discussion forum. The piece has been updated and we regret the error.
Expand full story
Lee Hutchinson  / Lee is the Senior Reviews Editor at Ars and is responsible for the product news and reviews section. He also knows stuff about enterprise storage, security, and manned space flight. Lee is based in Houston, TX.
</article>
</item>
<item>
<title>Gallery: E3’s gone toy crazy! From Star Wars to Atari frisbees</title>
<article>Corporate synergy at its finest. Olaf doesn't seem too stressed about the force-choke he's suffering, though.
Corporate synergy at its finest. Olaf doesn't seem too stressed about the force-choke he's suffering, though.
Sup, Chewie.
&nbsp;
&nbsp;
Darth Maul's figurine got a special little section.
This wooden Maul was pretty large, maybe 30 inches tall.
&nbsp;
Sup, Boba.
The rest of the Disney universe was in full effect at the Infinity booth. We wonder what Sam Flynn and Minnie Mouse will get up to as a duo.
And of course, Marvel characters were hanging out, as well. This oversized Iron Man is the only one we hadn't seen released yet.
You could pose as a figurine and have your photo taken. We didn't understand it, either, but we almost swiped one of those faux lightsabers.
ars.AD.queue.push(["xrailTop", {sz:"300x250", kws:[], collapse: true}]);LOS ANGELES—If you've ever winced when people describe video games as "toys," this year's Electronics Entertainment Expo is not the video game conference for you. Thanks to the skyrocketing sales of game-compatible toy lines like Skylanders, Disney Infinity, and Nintendo amiibo, this year's E3, more than any before it, has put eBay-loving figurine collectors in its sights.
Anybody who's seen photo galleries in our product reviews has probably spied a few detailed figurines on our desktops, so we can't help but stick a camera lens right up on toys that have our favorite Star Wars (above) and Nintendo (below) characters. To counterbalance this overload of new toys, of course, we've also included a gallery of historic Atari stuff, including prototype systems, sick belt buckles, and, of course, some toys. We'll have more impressions on the E3 games that they work with later this week.
Happy 30th anniversary of Super Mario! Celebrate with an amiibo that makes the famed plumber bigger in Super Mario Maker.
Happy 30th anniversary of Super Mario! Celebrate with an amiibo that makes the famed plumber bigger in Super Mario Maker.
Original color palette will be available if you prefer.
We finally got up close and yarnable with the amiibo from Yoshi's Wooly World. They're pretty big.
DK and Bowser will be compatible with the next Skylanders game for Wii U, which marks the first time a Nintendo character has been sold as a toy that works in a video game. (We think.)
The new 3DS Animal Crossing game will support these "amiibo cards."
...but Nintendo would never miss an opportunity to sell full-on Animal Crossing amiibo toys.
Chibi-Robo's getting a 3DS game, so, it's getting a toy.
Old-school.
In a first for the amiibo line, one of the characters' poses will be switchable. We suppose this adds to the value proposition of the otherwise flat Mr. Game &amp; Watch character.
The Famicom-colored ROB will only be sold in Japan, meaning it'll probably sell for $40 bazillion on eBay. Why does Japan get all of the best stuff?
The rest of these amiibo round out the next wave of Smash Bros.-specific amiibo.
We seriously implore you to click through this whole thing. There's some cool, rare Atari stuff in here.
Old pals!
Old pals!
The rest of this gallery was brought to you by the National Videogame Museum, which will soon open in Frisco, Texas, (just north of Dallas). Visit their site to learn more.
Some serious paperweights.
So many badges, stickers, and pins.
'Wanna play with my joystick?"
A relic from the NES's limited New York launch in 1985.
You probably didn't want to be caught dead wearing this button after the dismal launch of Atari's E.T.
Man, they really did slap Mario on everything, didn't they.
Find the PCjr pin!
I, SAM, WANT THIS BELT BUCKLE. BADLY.
Atari's is bigger, of course.
Go on, win yourself a McRib.
Join the Atari Game Club today!
Until today, we didn't know Columbia House had a "video game club." Wonder if that ever offered 11 games for a penny...
These catalogs are amazing.
Amazing, we say.
Make sure you have alllll of the compatible accessories!
"There is no, 'it's only a game' attitude at Atari."
"The dragon is waiting to eat you."
Edutainment!
We're pretty sure the box for III was eaten by a grue.
If you had one of these signed, you were a proven Pac-Master.
Virtual reality, man.
The National Videogame Museum exhibit was also peppered with a few free-play arcade machines and a line of old consoles.
Not sure we've ever seen such a meta Pac-Man T-shirt.
If this wasn't your favorite 2600 game, well... we don't blame you. But dat title.
"Just another high-strung prima donna from Atari." Wow.
You can't be an Activision Decathlete without the shiniest of official gear.
If you're about to lose at the Q*Bert board game, just grab an Atari-branded Louisville Slugger and "persuade" your rivals to quit.
Me too, man. Me too.
LUNCHBOXES!
A mobile!
Some particularly rare hardware at this exhibit.
Seriously, seriously rare stuff.
An Atari phone? Huh.
The museum exhibit was apparently sponsored by the upcoming movie Pixels, and they proved that by bringing the Mini Coopers used to represent the Pac-Man ghosts in a chase scene in the film.
Yep, Clyde was there, too.
Listing image by Sam Machkovech
Expand full story
Sam Machkovech  / Sam has written about video games on-and-off since his first syndicated column launched in 1996. He can regularly be found playing pinball at Add-A-Ball in Seattle, WA.
</article>
</item>
<item>
<title>Land Rover shows off tech to drive an SUV via smartphone app</title>
<article>In the years since Jaguar and Land Rover passed from Ford to the Indian car company Tata Motors in 2008, the two storied British companies have developed quite a reputation for forward thinking and investing in R&amp;D. Jaguar's XJ sedans and Land Rover's mighty Range Rover SUVs have been on crash diets, swapping the steel of their predecessors for aluminum; The Jaguar XJ is actually lighter than the smaller (and slightly older) XF, and the most recent Range Rover lost an impressive 881lbs (400kg). But the companies' R&amp;D centers have much more up their sleeves. On Tuesday, Land Rover showed off a Range Rover Sport that lets the driver control the car—at a low speed—with a smartphone app.
The remote control SUV is part of Land Rover's basket of technologies the company is developing for future autonomous vehicles. To use it, the driver (and key) must be within 32 feet (10m) of the vehicle, and there's a maximum speed of 4mph (6.4km/h). Land Rover suggests the tech could be useful for getting one's Range Rover out of a tight parking spot or when rock crawling, freeing the driver to act as their own spotter. The company also revealed a system that enables a Range Rover Sport to make 180 degree turns, which seems a relatively natural evolution of the auto parking feature (Park Assist in Land Rover-speak) that more and more car makers are offering now.
These systems come on the heels of some rather cool tech Jaguar Land Rover has been showing off recently, including a 'transparent' hood that uses cameras to project a view of the surface underneath the front wheels (to be used off-roading), as well as a full-windscreen heads-up display that owes quite a lot to video games like Forza and Gran Turismo (although sadly this is still quite a few years from the showroom).
Expand full story
Jonathan M. Gitlin  / Jonathan is the automotive editor at Ars Technica, covering all things car-related. Jonathan lives and works in Washington, D.C. 
</article>
</item>
<item>
<title>Just Cause 3 gives you a million and one ways to watch the world burn</title>
<article>&nbsp;
Even if you've played previous games in the series, understanding what's possible in Just Cause 3 isn’t easy. This is very much one of those instances in which the sheer wealth of content comes across as something entirely intimidating. My first thoughts when taking control of Rico and setting out across the fictional Mediterranean island chain of Medici were not “how can I put what's on offer to best use?" but “how am I ever going to see even half of what's possible here?”
Just Cause 3 is a game of systems, which, in isolation, are not all that interesting. But when intersected, the potential impact of your actions is mind-boggling... so long as you understand the outcome of any attempted cross-pollination. The education, the learning of the rules, those are the intimidating bits, the bits that you know are essential for enjoyment, despite the long and drawn-out process required to get there.
The good news is that you needn't (and shouldn't) attempt to learn everything in one swoop before diving into the game proper. Taking things slow by learning single systems one at a time before combining them together is better for both your sanity and enjoyment. Take the wingsuit, the shiny new piece of gear that’s been adorning trailers and adverts since Just Cause 3's announcement. As another option for traversing the environment by air, it isn't designed to be immediately accessible. The suit has a very distinct feel to it that comes across, upon first flights, as distinctly lethargic and disappointingly conservative. However, after some practise and various experiments, you begin to understand developer Avalanche Studios' intentions and marvel at the possibilities they bring.
It turns out that the way to use the wingsuit properly is to employ it in conjunction with both your parachute and grapple hook: the former used to provide dramatic extra lift, the latter lots of speed. Gliding through a canyon at a canter while gradually moving closer to the gravel of terra firma is not much fun. But grappling to a distant rock and reeling yourself in to create momentum, followed by opening your chute to thrust yourself into the air, before closing it instantly and spreading your wings... there’s the fun.
Each of those three rules of engagement are separate and complete systems in and of themselves, all of them fully formed to the extent that they are worthwhile and essential gimmicks to get into your head in isolation. But it’s not until they’re made into a single unit that their potential is realised. Indeed, it’s this philosophy that underpins the entire game. Avalanche is not so much making a game that directs you between Item A, Mission B and Point of Interest C, as it is delivering you a variety of ideas that you’re free to interpret as you wish.
Flying through the world is not the only thing that requires the intelligent combination of individual systems. Explosions and general mayhem have always been core pillars of Just Cause, and that dedication to destruction lives on here, perhaps being more of a feature now than it ever was thanks to the extra oomph provided by the latest hardware. The scale and force of explosions has been dramatically heightened, driven by complex chain reactions, with one fire or explosion harbouring the potential to trigger knock-on effects across anything flammable, electrical, or merely fragile within its vicinity. Before you know it, that innocent oil drum you ignited has caused an entire military outpost to crumble to the ground while you grappled your way to higher ground and safety.
These dramatic events trickle into one another in a wonderfully charming and sadistic way. There's an unpredictability to proceedings that means you can achieve great success through sheer luck or, alternatively, kill yourself tragically due to a spot of bad luck. It all adds to the chaos, and it's this that Just Cause 3 uses to carve out a niche compared to other open-world games.
Your best friend in destruction is the tethering system, which allows you to connect multiple wires between entities. Those entities might be the aforementioned oil drum, a vehicle, a person, a solid building or perhaps less solid building. Pretty much anything in the game can be used with the tether. Once attached, the tether can be tightened to create a strain on whatever it’s attached to. Humans are relatively weak, causing them to buckle under the pressure instantly and fly into whatever they're attached to. If that's a wall then they slam against it and die. If that's an oil drum, they'll slam against it, explode, and die. If that's another human, they'll slam against each other, bleed and die. So, uhh, lots of death then.
Listing image by Square Enix
Expand full story
</article>
</item>
<item>
<title>The first PC Gaming Show gets buried in a slow roll of announcements</title>
<article>The first ever PC Gaming Show at E3 dragged a bit at points during its two-and-a-half hour run time. Still, the eclectic mix of on-stage interviews, trailers, and live demos offered its fair share of announcements for PC players who stuck with it until the end (like our steely nerved livebloggers). Here's a quick (but still lengthy) summary of the major news:
Microsoft head of Xbox Phil Spencer made an appearance to announce that Killer Instinct will be making the jump from Xbox One to PC. He also talked up Microsoft's renewed commitment to PC gaming and stressed how offering players a free upgrade to Windows 10 would allow PC developers to target a single OS as much as possible, rather than having to test on multiple versions of Windows. "We want to respect when developers choose to target TV screens, or when developers choose to go with mouse-and-keyboard control," he said.
Rod Fergusson of The Coalition announced another major Xbox title would be coming to the PC: Gears of War: Ultimate Edition. The series' first PC release since the original game was released in 2007 is actually a remake of that original game, but now with support for 4K resolution, an unlimited refresh rate, DirectX 12 graphics, and mouse/keyboard or wireless controller support on Windows 10.
Speaking of Gears of War, designer Cliff Bleszinski was on hand to talk about how bored he got reading books by the pool all day during his recent break from game development. He's coming back to work on Project Bluestreak with Nexon, his return to the first-person shooter genre after Gears' over-the-shoulder action. Some sterile, in-engine environments were shown as the 40-year-old Bleszinski admitted to being less good at precise aiming these days, and talked of designing area-of-effect weapons to compensate. Still, he hopes Project Bluestreak will allow skillful players to utterly dominate in the same way they do in games like Counter Strike.
DayZ creator Dean Hall showed up ostensibly to talk about his new game, Ion, but spent a significant chunk of time copping to the development problems that have plagued the long Early Access tenure of DayZ. Hall said he never expected the game to get so big—"It was a mod I thought maybe 50 people would play"—and said the increased size and scope of the game's quick expansion has been hard to maintain without a good roadmap, even with the support of a publisher like Bohemia Interactive.
Later in the show, Bohemia's Brian Hicks reconfirmed that they are still aiming for a 2015 release for the "full feature beta" of DayZ, in line with the two to three years of development they originally expected. That beta will feature a single player mode, use Steam Workshop for modding, and allow people to host their own servers in whatever configurations they want, he said.
Bohemia Interactive also showed off Tanoa, a new expansion area for Arma III. This 100 square kilometer theater of war is primarily a set of jungle filled islands arranged in an archipelago, making water traversal and underwater exploration more important than ever, Hicks said.
Show sponsor AMD used its time on stage to talk up the 300-series graphics cards it had announced just that morning. AMD Radeon CEO Lisa Su also showed off Project Quantum—a 16-teraflop gaming system featuring two Fiji GPUs—and unveiled a single card featuring two Fiji-level GPU chips. AMD stressed a few times that all this graphics power would be needed to drive experiences like the 90 frame-per-second refresh rates of virtual reality headsets.
Cloud Imperium's Chris Roberts wasn't able to be at the show live, but he attended via video conference from England. From there, he showed some of the work he and his team were doing on motion capture, which he said would help make Star Citizen comparable to major AAA releases on an emotional animation level. "It's financed by all of you out there, and that speaks to the power of PC gamers," he said.
Frontier's David Braben (of Elite fame) came to the stage and got a little tongue-tied as he showed off a new game that, surprisingly, didn't feature outer space or spaceships. Instead, a pre-rendered trailer for Planet Coaster showed a lot of heart. In it, roller coasters, concession stands, and happy patrons popped up all around as a man got chased down for a bear hug by a darling mascot in a T-Rex suit. Braben called it a "revival of another long-dead genre" that the team at Frontier has "wanted to make for a long time." Look for it in 2016.
ArenNet's Colin Johanson was on hand to showcase the "Heart of Thorns" expansion for Guild Wars 2 and its new "Guild Halls" feature, which lets players "own a part of the world." Players will be able to add buildings and custom decorations to these halls, but also fight to capture them from each other using tools like airships. Teams of players can fight to claim top marks on leaderboards in this battle for control of the halls.
Blizzard was on hand to announce "Whispers of Oblivion," a three-mission prologue for Starcraft 2 that will bridge the gap between the "Heart of the Swarm" expansion and the upcoming "Legacy of the Void." Those missions will be made available for free before the release of "Legacy of the Void," but those that pre-purchase the game will get early access.
Blizzard also showed new footage of Heroes of the Storm's upcoming "Eternal Conflict" expansion. The new content pack will feature a bevy of Diablo characters, including the skeletal King Leoric, who wields a powerful hammer to bash through enemies. Diablo's Monk was also confirmed to be in development as the first support character from Diablo to appear in Blizzard's MOBA.
Wrapping up the show, Hello Games' Sean Murray announced that the highly anticipated No Man's Sky will be coming to PC the same day as the PS4, though he gave no indication of when that release would be, precisely. Murray talked a bit about the AI bots his team had to design to go and test the game's procedurally generated universe, creating technical reports and GIFs automatically for evaluation. "We grew up playing Elite. This was the game I imagined at the time," Murray said of his ambitious creation.
Outside of the major announcements, the PC Gaming Show was simply packed with showcases of games coming to computers. Here's a round up of a few other highlights
All in all, the first PC Gaming Show used its ample time to show off the breadth and depth of the PC gaming space in an effective, if occasionally long-winded, manner. The event seems primed to be a regular part of the E3 calendar along the console-focused showcases run by Sony, Microsoft, and Nintendo.
Expand full story
Kyle Orland  / Kyle is the Senior Gaming Editor at Ars Technica, specializing in video game hardware and software. He has journalism and computer science degrees from University of Maryland. He is based in Pittsburgh, PA. 
</article>
</item>
<item>
<title>Liveblog: The PC Gaming Show at E3 2015</title>
<article>As part of this year's E3 festivities, PC Gamer is hosting a night of press announcements devoted solely to PC gaming. Ars will be on hand to liveblog the event starting at 6pm PDT (9pm EDT, 2am Wednesday UK time) on Tuesday, June 16.
PC Gamer has lined up a wide array of companies and developers to present at this first-of-its-kind press conference, including Microsoft, AMD, Blizzard, Square Enix, Cliff Bleszkinski (Gears of War), Dean Hall (Day Z), Fullbright (Gone Home), Cloud Imperium (Star Citizen), and many more. Exactly what they'll be discussing/announcing is largely a mystery, but we'll be on hand to cover it as they do, in any case.
Expand full story
Kyle Orland  / Kyle is the Senior Gaming Editor at Ars Technica, specializing in video game hardware and software. He has journalism and computer science degrees from University of Maryland. He is based in Pittsburgh, PA. 
</article>
</item>
<item>
<title>AMD unveils R9 Fury X, Fury, and Nano graphics cards</title>
<article>There's not one, not two, but three brand new high-end graphics cards on the way from AMD. As rumoured, AMD is dropping the numerical branding and is instead grouping its top cards under the "Fury" banner. All are based on its new Fiji chip, which is a tweaked version of the company's long-standing GCN architecture, and—as expected—all will come equipped with 4GB of stacked, on-package high bandwidth memory (HBM).
The flagship is the $649 R9 Fury X, which launches on June 24. At that price, it is pitched directly against Nvidia's GTX 980 Ti. It features 4096 stream processors—a huge jump over the 2816 stream processors found in the R9 290X—"up to" 1050MHz core clock, 256 texture units, 64 ROPs, HBM memory with 512 GB/s of bandwidth, a 67.2 GP/s pixel fill rate, and a six-phase VRM (voltage regulator module), which AMD claims is ideal for overclocking the card. We don't yet have UK pricing, but it'll probably be around £550.
Despite using two 8-pin power connectors, the Fury X's power consumption isn't as high as some feared: the TDP is 275W, just a tad higher than the R9 290X's, although it's worth bearing in mind that in real-world usage, the R9 290X was much closer to 300W. The Fury X supports up to 375W of power for overclocking.
AMD's reference cooler has gotten a makeover, with the company finally ditching the less-than-ideal plastic blower design of older cards. The Fury X's cooler is made of die cast aluminium and finished in a black nickel gloss and a soft touch black. It's quite a handsome thing, and thanks to the use of HBM memory AMD has been able to shrink the size of the card down to just a hair over 19cm (7.5in), which is all the more impressive given the huge die made up of 8.9 billion transistors. However, that doesn't take into account the watercooling tubes, which poke out of the rear of the card. Speaking of watercooling, the Fury X is cooled via an extremely thick 120mm radiator, rated for up to 500W of cooling.
Bling comes in the form of a light-up Radeon logo on the top of the card, as well an LED light strip that indicates the current level of GPU operation. Users can choose between either red or blue, while a separate green LED lights up when the card is idling and not making use of any GPU cores. For hooking up monitors, there are three full-size DisplayPort outputs, along with a single HDMI 1.4 port.
Updated: Other sites are reporting that it's HDMI 2.0. We're trying to confirm one way or the other with AMD.
Next up is the $549 R9 Fury, an air-cooled version of the Fury X. Specs are thin on the ground for regular Fury, but it appears to feature the same aluminium construction as the Fury X along with a blower-style fan for venting hot air directly out of the case. It's likely that the Fury will drop some stream processors and ROPs compared to the Fury X, but we'll have to wait until we hear something official from AMD to get confirmation.
The third card is an odd one. Called the R9 Nano, it too is based on the Fiji chip with HBM, but comes in at just 15cm/6in, a size usually reserved for low-end cards. There's a large fan on the side for venting air inside a PC case, and AMD claims the R9 Nano offer ups to two times the performance-per-watt over the R9 290X; higher than the improvement offered by the Fury and Fury X. No pricing was given for the R9 Nano, but it is supposed to launch "this summer."
There's also supposed to be a dual-GPU configuration of the Fury X on the way, AMD gave no additional details.
No, it's not cheap, but the 980 Ti is still an outstanding graphics card for the price.
AMD did, however, unveil a rather mad-looking PC equipped with dual Fury X graphics cards. Called Project Quantum, it's a unique small form factor PC split into two sections, with the middle lit up by a set of glowing red LEDs. Other than the dual Fury X GPUs, AMD isn't talking about what else lies inside the PC, but we did spot a rather huge external PSU being used to power it. Sadly, you won't actually be able to go and buy the PC, at least for now: AMD is currently pitching the design to partners in order to bring it to market, but has no time frame for release.
For those who don't need quite so much graphics horsepower, AMD also revealed some updates to the R9 and R7 graphics card ranges. Sadly, the new 300-series is a rebrand of the older 200-series cards, and sport almost identical specs, bar an increased amount of memory. At the bottom is the R7 360, which comes equipped with 2GB of GDDR5 memory for $109; the R9 370 with up to 4GB of GDDR5 memory for $149; and the R9 380, also with up to 4GB of GDDR5 memory for $199. At the top end is R9 390 for $329 and the R9 390X for $429, both of which come with 8GB of GDDR5 memory. We're trying to hunt down some UK prices and will update the post when we have them.
This post originated on Ars Technica UK
Expand full story
Mark Walton  / Mark is Gaming and Hardware Editor at Ars Technica UK by day, and keen musician by night. He hails from the UK, the home of ARM, heavy metal, and superior chocolate.
</article>
</item>
<item>
<title>Shock European court decision: Websites are liable for users’ comments</title>
<article>In a surprise decision, the European Court of Human Rights (ECHR) in Strasbourg has ruled that the Estonian news site Delfi may be held responsible for anonymous and allegedly defamatory comments from its readers. As the digital rights organization Access notes, this goes against the European Union’s e-commerce directive, which "guarantees liability protection for intermediaries that implement notice-and-takedown mechanisms on third-party comments." As such, Peter Micek, Senior Policy Counsel at Access, says the ECHR judgment has "dramatically shifted the internet away from the free expression and privacy protections that created the internet as we know it."
A post from the Media Legal Defence Initiative summarizes the reasons why the court came to this unexpected decision. The ECHR cited "the 'extreme' nature of the comments which the court considered to amount to hate speech, the fact that they were published on a professionally-run and commercial news website," as well as the "insufficient measures taken by Delfi to weed out the comments in question and the low likelihood of a prosecution of the users who posted the comments," and the moderate sanction imposed on Delfi.
In the wake of this judgment, the legal situation is complicated. In an e-mail to Ars, T J McIntyre, who is a lecturer in law and Chairman of Digital Rights Ireland, the lead organization that won an important victory against EU data retention in the Court of Justice of the European Union last year, explained where things now stand. "Today's decision doesn't have any direct legal effect. It simply finds that Estonia's laws on site liability aren't incompatible with the ECHR. It doesn't directly require any change in national or EU law. Indirectly, however, it may be influential in further development of the law in a way which undermines freedom of expression. As a decision of the Grand Chamber of the ECHR it will be given weight by other courts and by legislative bodies."
One of the worrying aspects of the ECHR decision is that it may encourage the idea that intermediaries are liable for "manifestly unlawful" content, without specifying what "manifestly unlawful" actually means. McIntyre points out that this is "something which may lead to a chilling effect where sites are over cautious in taking down material which might possibly be contentious."
As McIntyre notes, also troubling is that the judgment upholds a finding that "proactive monitoring" of Internet users can be required. That contradicts the important decision in the SABAM case of 2012, where the Court of Justice of the European Union (CJEU) ruled that forcing a hosting service to monitor and filter online content violated EU law. Copyright companies will doubtless try to use the Delfi decision to undermine that key CJEU judgement.
What's unfortunate is that Delfi would probably have won had it taken its case to the CJEU, given the e-commerce directive's clear guidelines, but this course of action was apparently not permitted by the Estonian courts. It therefore went to the ECHR, hoping for a ruling that the Estonian law was incompatible with the European Convention on Human Rights.
As Access's Micek told Ars: "The website argued that its 'freedom to impart information created and published by third parties'—the commenters—was at stake. Delfi invoked its Article 10 rights to freedom of expression under the European Convention on Human Rights and the [ECHR] accepted the case." Delfi's unexpected defeat there is likely to have important, if subtle consequences on not just the Web, but also freedom of speech and privacy, across the European Union.
This post originated on Ars Technica UK
Expand full story
Glyn Moody  / Glyn Moody is Contributing Policy Editor at Ars Technica.  He has been writing about the Internet, free software, copyright, patents and digital rights for over 20 years.
</article>
</item>
<item>
<title>Neil Gaiman’s American Gods to get a series on Starz</title>
<article>On Tuesday, cable and satellite network Starz announced that it was moving ahead with a series adaptation of Neil Gaiman's 2001 novel American Gods.
The show will be written and produced by Bryan Fuller and Michael Green. Fuller has worked on shows such as Hannibal, Pushing Daisies, and Heroes, and Green is known for his work on The River and Heroes. Gaiman will be an executive producer for the show.
The book, written in 2001, won Hugo and Nebula awards (which are given for science fiction or fantasy literature) and has been translated into dozens of languages. The story follows the main character, Shadow Moon, who leaves jail to be employed as a bodyguard for the mysterious Mr. Wednesday. The two journey around the US together in preparation for an epic battle between Old Gods from traditional mythology and New Gods representing the ideas and things that Americans worship today.
According to Variety, production of the show will begin as soon as a leading man is found to play Shadow. FremantleMedia North America is the company in charge of production, and it will distribute the series outside of Starz's TV and subscription video on demand (SOVD) rights.
In Starz' press release, Gaiman commented:
I am thrilled, ‎scared, delighted, nervous and a ball of glorious anticipation. The team that is going to bring the world of ‘American Gods’ to the screen has been assembled like the master criminals in a caper movie: I’m relieved and confident that my baby is in good hands. Now we finally move to the exciting business that fans have been doing for the last dozen years: casting our Shadow, our Wednesday, our Laura.
In 2011, HBO was set to get an American Gods adaptation, but the deal fell through, according to Deadline. For it's part, Starz isn't new to Sci-Fi novel adaptations. Most recently, the network has seen some success with its series adaptation of the novel Outlander. On American Gods, Fuller and Green have reported that two scripts have been written already. Neither Starz nor FremantleMedia returned our request for comment.
Expand full story
Megan Geuss  / Megan is a staff editor at Ars Technica. She writes breaking news and has a background in fact-checking and research.
</article>
</item>
<item>
<title>New exploit turns Samsung Galaxy phones into remote bugging devices</title>
<article>As many as 600 million Samsung phones may be vulnerable to attacks that allow hackers to surreptitiously monitor the camera and microphone, read incoming and outgoing text messages, and install malicious apps, a security researcher said.
The vulnerability is in the update mechanism for a Samsung-customized version of SwiftKey, available on the Samsung Galaxy S6, S5, and several other Galaxy models. When downloading updates, the Samsung devices don't encrypt the executable file, making it possible for attackers in a position to modify upstream traffic—such as those on the same Wi-Fi network—to replace the legitimate file with a malicious payload. The exploit was demonstrated Tuesday at the Blackhat security conference in London by Ryan Welton, a researcher with security firm NowSecure. A video of his exploit is here.
Phones that come pre-installed with the Samsung IME keyboard, as the Samsung markets its customized version of SwiftKey, periodically query an authorized server to see if updates are available for the keyboard app or any language packs that accompany it. Attackers in a man-in-the-middle position can impersonate the server and send a response that includes a malicious payload that's injected into a language pack update. Because Samsung phones grant extraordinarily elevated privileges to the updates, the malicious payload is able to bypass protections built into Google's Android operating system that normally limit the access third-party apps have over the device.
Surprisingly, the Zip archive file sent during the keyboard update isn't protected by transport layer security encryption and is therefore susceptible to man-in-the-middle tampering. The people designing the system do require the contents of that file to match a manifest file that gets sent to the phone earlier, but that requirement provided no meaningful security. To work around that measure Welton sent the vulnerable phone a spoofed manifest file that included the SHA1 hash of the malicious payload. He provided more details about the exploit and underlying vulnerability here and here.
Welton said the vulnerability exists regardless of what keyboard a susceptible phone is configured to use. Even when the Samsung IME keyboard isn't in use, the exploit is still possible. The attack is also possible whether or not a legitimate keyboard update is available. While SwiftKey is available as a third-party app for all Android phones, there's no immediate indication they are vulnerable, since those updates are handled through the normal Google Play update mechanism.
For the time being, there's little people with vulnerable phones can do to prevent attacks other than to avoid unsecured Wi-Fi networks. Even then, those users would be susceptible to attacks that use DNS hijacking, packet injection, or similar techniques to impersonate the update server. There is also no way to uninstall the underlying app, even when Galaxy owners use a different keyboard. In practical terms, the exploit requires patience on the part of attackers, since they must wait for the update mechanism to trigger, either when the phone starts, or during periodic intervals.
Further ReadingACLU asks feds to probe wireless carriers over Android security updates"Defective" phones from AT&T, Verizon, Sprint, T-Mobile pose risks, ACLU says.Welton said he has confirmed the vulnerability is active on the Samsung Galaxy S6 on Verizon and Sprint networks, the Galaxy S5 on T-Mobile, and the Galaxy S4 Mini on AT&amp;T. Welton has reported to bug to Samsung, Google, and the US CERT, which designated the vulnerability CVE-2015-2865. The bug has its origins in the software developer kit provided by SwiftKey, but it also involves the way Samsung implemented it in its Galaxy series of phones.
Update: In an e-mailed statement, SwiftKey officials wrote: "We’ve seen reports of a security issue related to the Samsung stock keyboard that uses the SwiftKey SDK. We can confirm that the SwiftKey Keyboard app available via Google Play or the Apple App Store is not affected by this vulnerability. We take reports of this manner very seriously and are currently investigating further."
The researcher said Samsung has provided a patch to mobile network operators, but he has been unable to learn if any of the major carriers have applied them. As Ars has reported in the past, carriers have consistently failed to offer security updates in a timely manner.
Post updated in the fourth paragraph to add details about transport layer security and to add comment from SwiftKey in the second-to-last paragraph.
Expand full story
Dan Goodin  / Dan is the Security Editor at Ars Technica, which he joined in 2012 after working for The Register, the Associated Press, Bloomberg News, and other publications.
</article>
</item>
<item>
<title>EFF, ACLU appeal license plate reader case to California Supreme Court</title>
<article>Further ReadingLA cops need not disclose license plate reader data, says appeals courtEFF, ACLU lose their California state appeal in LPR public records case.Two privacy activist groups formally appealed on Tuesday to the California Supreme Court, in their attempt to compel two Southern California law enforcement agencies to release one week’s worth of license plate reader data.
In May 2013, the Electronic Frontier Foundation (EFF) and the American Civil Liberties Union of Southern California (ACLU SoCal) had sued the Los Angeles Police Department and the Los Angeles Sheriff’s Department to gain access to the data as a way to better understand this surveillance technology. The groups lost in 2014 at the lower court level and last month at the appellate court.
Both agencies, like many others nationwide, use license plate readers (LPRs, or ALPRs) to scan cars and compare them at incredible speeds to a "hot list" of stolen or wanted vehicles. In some cases, that data is kept for weeks, months, or even years. Handing over such a large volume of records by a California law enforcement agency is not without precedent.
Further ReadingWe know where you’ve been: Ars acquires 4.6M license plate scans from the copsOne citizen demands: "Do you know why Oakland is spying on me and my wife?"Earlier this year, Ars obtained 4.6 million LPR records collected by the police in Oakland over four years and learned that just 0.16 percent of those reads were "hits." We discovered that such data is incredibly revelatory—we were even able to find the city block where a member of the city council lives, using nothing but the database, a related data visualization tool, and his license plate number.
The judge in the initial court ruling found that the law enforcement agencies could withhold LPR records—which include a plate number, date, time, and GPS location—through a particular exemption under the California Public Records Act that allows investigatory records to be kept private.
The EFF and the ACLU SoCal argue that interpreting the investigations provisions so broadly creates a ridiculous result.
By interpreting § 6254(f) to shield from public view this entire class of records, the Court of Appeal improperly expanded the scope of that exemption beyond prior precedent as established by this Court in Williams v. Superior Court, 5 Cal. 4th 337 (1993) and Haynie v. Superior Court, 26 Cal. 4th 1061 (2001), and so stretched the meaning of “investigation” as to force the absurd result that all cars in Los Angeles are constantly under police investigation.
…
The court also failed to address the fundamental differences between the mass surveillance technology in ALPRs and traditional human policing, and instead mechanically applied old caselaw addressing targeted investigations by human officers to ALPR technology. Indeed, the court’s opinion rests on the presumption that there is no difference between an officer manually checking a single license plate and high-tech surveillance equipment automatically cataloging the locations of millions of vehicles in Los Angeles every week.
Previously, as Ars has reported, I have run into similar walls when trying to access my own license plate reader data from various cities. Various agencies that we queried, including the LAPD and LASD, the San Mateo County Sheriff’s Office, and the Piedmont Police Department have all denied such records under § 6254(f) of the California Public Records Act, and a related provision, subsection (k).
In 2013, we outlined some previous California judicial findings as to why we didn’t think the government’s claim holds water. That included (Haynie v. Superior Court) from 2001, a California appellate court again found that law enforcement agencies could not summarily restrict access.
Yet, by including "routine" and "everyday" within the ambit of "investigations" in section 6254(f), we do not mean to shield everything law enforcement officers do from disclosure. (Cf. ACLU, supra, 32 Cal.3d at p. 449.) Often, officers make inquiries of citizens for purposes related to crime prevention and public safety that are unrelated to either civil or criminal investigations. The records of investigation exempted under section 6254(f) encompass only those investigations undertaken for the purpose of determining whether a violation of law may occur or has occurred.
The ACLU and EFF cite this case extensively in their Supreme Court petition:
In Haynie, this Court stated, “we do not mean to shield everything law enforcement officers do from disclosure.” 26 Cal. 4th at 1071. In doing so, the Court acknowledged the “records of . . . investigations” exemption has limiting principles, even if it did not define at the time what those were. The automated collection of data on millions of innocent drivers in Los Angeles is not an “investigation” within the meaning of Haynie or any of the cases to apply its rule. ALPRs do not involve a “decision” to investigate like the “decision to stop Haynie,” Haynie, 26 Cal. 4th at 1071; they also do not involve any specific allegations of wrongdoing or a connection to any particular crime. Instead, LPR cameras automatically photograph all plates within view without the driver’s knowledge, without the officer targeting any particular car, and without any level of suspicion. Under no prior cases is such data-gathering an “investigation” for purposes of § 6254(f).
The court’s review is not automatic: the California Supreme Court only agrees to hear a small fraction of the cases that are submitted to it.
Expand full story
Cyrus Farivar  / Cyrus is the Senior Business Editor at Ars Technica, and is also a radio producer and author. His first book, The Internet of Elsewhere, was published in April 2011.
</article>
</item>
<item>
<title>Google extends vulnerability bounties to Android; offers up to $30,000</title>
<article>Google's "Vulnerability Reward Program" has been incentivizing people to report security bugs to the tech giant for its Web services, apps, extensions, Chrome, and Chrome OS for some time now. Today the company announced that it's extending the cash-for-bugs program to its biggest operating system: Android.
Further ReadingAs 0days get meaner, Google defenses increasingly outpace MicrosoftIn today's attack climate, 90 days is an eternity. Unless you're Microsoft.The program doesn't cover any Android device, just new devices that Google is 100% responsible for: current, for sale, Nexus devices. For now, that means the Nexus 6 and Nexus 9. Google says that this "makes Nexus the first major line of mobile devices to offer an ongoing vulnerability rewards program."
Google will pay researchers not only for bug disclosures—it offers additional rewards tiers for test cases submitted with the bug, CTS tests that catch the bug, and AOSP patches that fix the bug. "CTS" is Android's "Compatibility Test Suite," the continually updated battery of tests all devices must pass in order to gain access to the Google Play Store. CTS tests ensure that a device and its software are Android-compatible and free of known vulnerabilities, ensure platform API correctness, and follow Google's mandatory (and minimal) UI practices for readability and consistency.
Google pays anywhere from $0 to $2,000 for a bug submission, depending on the severity level, and when combined with test cases, unit tests, and AOSP patches, offers a max payout of $8,000. In addition to the reward levels, there are also huge bonuses available if you compromise the kernel, TEE (TrustZone), or the Verified Boot process. For the worst possible scenario of remotely compromising TrustZone or Verified boot, Google is offering a $30,000 reward.
Google says it paid "more than 1.5 million dollars" to security researchers last year under the bug bounty program. The company is also a sponsor of the annual pwn2own hacking competition and plans to support other Android vulnerability contests in the future.
Expand full story
Ron Amadeo  / Ron is the Reviews Editor at Ars Technica, where he specializes in Android OS and Google products. He is always on the hunt for a new gadget and loves to rip things apart to see how they work.
</article>
</item>
<item>
<title>Everything old is new again at a nostalgia-filled E3 2015</title>
<article>The air around E3 this year is a bit more electric than it's been in the recent past. As the "watch trailers at a press conference" portion of the show gives way to the "wander the show floor and actually play games" portion, there's a palpable buzz of excitement about a variety of the big announcements from the major publishers and platform holders. For the most part, though, the most excitement seems to be coming from long-dormant franchises that have been excavated like time capsules for a public eager to remember their rose-tinted gaming past.
There were plenty of truly new announcements at these press events, of course. Sea of Thieves looks like the most substantial release from the Microsoft-owned Rare that we've seen in years. Media Molecule's Dreams is an intriguing experiment into guided content creation without the need for precise planning and skill. Ubisoft's For Honor is a promising new take on hand-to-hand medieval combat. Microsoft's Hololens seems poised to potentially create completely new ways to play games like Minecraft. Games like Horizon and ReCore look like fresh takes on an often stale action-adventure genre.
Overwhelmingly, though, those don't seem to be the announcements that are generating the most buzz at the show. The games that are really getting people here riled up are the ones from franchises that haven't been seen for years, or sometimes decades, bringing with them the promise and baggage of the kind of gaming we remember from our youths.
In a way, this is par for the course in an industry and a show that seems obsessed with mining the recent past for sequels, remakes, and re-releases of franchises with storied pasts. Regular releases in franchises like Assassin's Creed, Halo, Forza, Call of Duty, Lego, and many more are practically expected at this point. Games like Uncharted 4, Mirror's Edge Catalyst, Fallout 4, Deus Ex: Mankind Divided, Rock Band 4, and Guitar Hero Live are a little less expected after gaps of a few years, but they're still dipping into the relatively recent past for inspiration.
But the biggest announcements at this year's show seem to be the ones that are diving deeper, into a nostalgia for properties that have been long neglected and long requested by fans. Shenmue III has raised over $2.5 million in crowdfunding in less than 24 hours (and climbing quickly) off a property that hasn't seen a new game in 14 years. Sony got some of the biggest cheers of its show for a remake of Final Fantasy VII, a game that will be nearing 20 years old when the remake finally hits. Star Fox Zero, the linchpin of Nintendo's announcements, is part of a franchise that hasn't seen a major console release in 13 years. It's been ten years since we've last seen a game in the Star Wars Battlefront series. The Doom franchise is over 20 years old, and its last release was ten years ago.
Even some of the most popular "newer" announcements at this year's E3 seem focused on living in the gaming past. The Last Guardian garnered a huge reaction from the Sony conference crowd even though it's been trapped in a troubled, vaporware-esque development cycle since at least 2007 and is a spiritual successor to games that are over a decade old. Super Mario Maker mines nostalgia from 2D Mario console games that range from nine to 30 years old. The biggest reaction at Microsoft's showcase was for a new Xbox One feature that lets users play backward-compatible games from a console with hardware nearly a decade old.
The games that are generating the most excitement so far at this year's E3 are, by and large, the ones that are returning from the wilderness after a long absence. The series that return every year, or every few years, are getting polite attention and even some excitement from longtime fans. But those games suffer somewhat from a sense of over-familiarity; the jaded lens that comes with age and repetition and recent exposure. They offer more, but it often seems like more of the same after such a short time.
For the franchises returning from long sojourns away from the spotlight, though, absence seems to have made the generalized gaming heart grow fonder. Digging deeper for these hazily remembered classics seems to have revived memories of the feelings these games generated when they were first released, when the audience was younger and likely more carefree. Untouched for so many years, these properties mature in the mind like a fine wine, with all the anticipation that comes along with a coming uncorking.
It doesn't even matter what form the new reveal takes, really. The announcement of a Final Fantasy VII remake got people out of their chairs with a choppy, pre-rendered trailer that showed no gameplay or hint of a release date. There are practically no details on Shenmue III save for the presence of many of the creators behind the original game on the project. Merely mentioning these games is enough to get the memories and good feelings flowing, even if the original games can seem a little dated from a modern lens.
There's nothing inherently wrong with mining gaming's lost gems in this way—these games are getting attention because they're well-remembered, after all. But our memories of games this old aren't always as accurate as we'd like to think. Replaying these old games with a more experienced eye can make you wonder whether it was just youth and freshness that made them so interesting at the time. Revamping these games for a new gaming generation can be a good thing, but only if they don't hew too faithfully to dated designs.
Regardless, judging by this year's E3 so far, we're going to see a new wave of publishers dipping not just into the recent past, but digging deep into cult classic properties that have been languishing for too long. That's a welcome trend for gamers hoping to recapture the memories of their gaming youths, but one that could quickly become played out if it's not handled carefully.
Whatever. Bring on a new Battletoads already!
Expand full story
Kyle Orland  / Kyle is the Senior Gaming Editor at Ars Technica, specializing in video game hardware and software. He has journalism and computer science degrees from University of Maryland. He is based in Pittsburgh, PA. 
</article>
</item>
<item>
<title>United States Air Force needs a few hundred good drone pilots</title>
<article>Further ReadingPsssst! Wanna buy a lethal drone? US to export unmanned aircraftSale of deadly technology to be a financial bonanza for the US defense industry.The military brass in charge of America’s drones say that there’s a shortage of pilots.
According to The New York Times, a “significant number” of the 1,200 United States Air Force pilots are “coming up for re-enlistment and are opting to leave, while a training program is producing only about half of the new pilots that the service needs.”
Col. James Cluff, commander of the Air Force’s 432nd Wing, invited the Times along with a few other media onto the decade-old nerve center of drone operations outside of Las Vegas on Tuesday. He told them that the Air Force has pulled instructors from schools to the “flight line." The agency now conducts 65 drone flights a day, a number that is expected to drop to 60 by fall 2015.
With the rise of the Islamic State and other global hotspots, there is increasing pressure on the Air Force to provide more drone flights. But while drone operators get to see their families at night and are half a globe away from their targets, it still takes a toll.
“Having our folks make that mental shift every day, driving into the gate and thinking, ‘All right, I’ve got my war face on, and I’m going to the fight,’ and then driving out of the gate and stopping at Wal-Mart to pick up a carton of milk or going to the soccer game on the way home—and the fact that you can’t talk about most of what you do at home—all those stressors together are what is putting pressure on the family, putting pressure on the airman,” Col. Cluff said.
Expand full story
Cyrus Farivar  / Cyrus is the Senior Business Editor at Ars Technica, and is also a radio producer and author. His first book, The Internet of Elsewhere, was published in April 2011.
</article>
</item>
<item>
<title>Encryption “would not have helped” at OPM, says DHS official</title>
<article>During testimony today in a grueling two-hour hearing before the House Oversight and Government Reform Committee, Office of Personnel Management (OPM) Director Katherine Archuleta claimed that she had recognized huge problems with the agency's computer security when she assumed her post 18 months ago. But when pressed on why systems had not been protected with encryption prior to the recent discovery of an intrusion that gave attackers access to sensitive data on millions of government employees and government contractors, she said, "It is not feasible to implement on networks that are too old." She added that the agency is now working to encrypt data within its networks.
But even if the systems had been encrypted, it likely wouldn't have mattered. Department of Homeland Security Assistant Secretary for Cybersecurity Dr. Andy Ozment testified that encryption would "not have helped in this case" because the attackers had gained valid user credentials to the systems that they attacked—likely through social engineering. And because of the lack of multifactor authentication on these systems, the attackers would have been able to use those credentials at will to access systems from within and potentially even from outside the network.
House Oversight Chairman Jason Chaffetz (R-Utah) told Archuleta and OPM Chief Information Officer Donna Seymour, "You failed utterly and totally." He referred to OPM's own inspector general reports and hammered Seymour in particular for the 11 major systems out of 47 that had not been properly certified as secure—which were not contractor systems but systems operated by OPM's own IT department. "They were in your office, which is a horrible example to be setting," Chaffetz told Seymour. In total, 65 percent of OPM's data was stored on those uncertified systems.
Chaffetz pointed out in his opening statement that for the past eight years, according to OPM's own Inspector General reports, "OPM's data security posture was akin to leaving all your doors and windows unlocked and hoping nobody would walk in and take the information."
When Chaffetz asked Archuleta directly about the number of people who had been affected by the breach of OPM's systems and whether it included contractor information as well as that of federal employees, Archuleta replied repeatedly, "I would be glad to discuss that in a classified setting." That was Archuleta's response to nearly all of the committee members' questions over the course of the hearing this morning.
Archuleta told the committee that the breach was found only because she had been pushing forward with an aggressive plan to update OPM's security, centralizing the oversight of IT security under the chief information officer and implementing "numerous tools and capabilities." She claimed that it was during the process of updating tools that the breach was discovered. "But for the fact that OPM implemented new, more stringent security tools in its environment, we would have never known that malicious activity had previously existed on the network and would not have been able to share that information for the protection of the rest of the federal government," she read from her prepared statement.
Further ReadingWhy the “biggest government hack ever” got past the fedsInertia, a lack of internal expertise, and a decade of neglect at OPM led to breach.Dr. Ozment reiterated that when the malware activity behind the breach was discovered, "we loaded that information into Einstein (DHS' government-wide intrusion detection system) immediately. We also put it into Einstein 3 (the intrusion prevention system currently being rolled out) so that agencies protected by it would be protected from it going forward."
But nearly every question of substance about the breach—which systems were affected, how many individuals' data was exposed, what type of data was accessed, and the potential security implications of that data—was deferred by Archuleta on the grounds that the information was classified. What wasn't classified was OPM's horrible track record on security, which dates back at least to the George W. Bush administration—if not further.
During his opening statement, Chaffetz read verbatim from a 2009 OPM inspector general report that noted, "The continuing weakness in OPM information security program results directly from inadequate governance. Most if not all of the [information security] exceptions we noted this year result from a lack of leadership, policy, and guidance." Similar statements were read from 2010 and 2012 reports, each more dire than the last. The OPM Office of the Inspector General only began upgrading its assessment of the agency's security posture in its fiscal year 2014 report—filed just before news of a breach at a second OPM background investigation contractor surfaced.
Rep. Will Hurd (R-Texas), a freshman member of Congress, told the OPM executives and the other witnesses—DHS' Ozment, Interior Department CIO Sylvia Burns, the new US CIO Tony Scott, and OPM Assistant Inspector General Michael Esser— that "the execution on security has been horrific. Good intentions are not good enough." He asked Seymour pointedly about the legacy systems that had not been adequately protected or upgraded. Seymour replied that some of them were over 20 years old and written in COBOL, and they could not easily be upgraded or replaced. These systems would be difficult to update to include encryption or multi-factor authentication because of their aging code base, and they would require a full rewrite.
Personnel systems have often been treated with less sensitivity about security by government agencies. Even health systems have had issues, such as the Department of Veterans' Affairs national telehealth program, which was breached in December of 2014. And there have been two previous breaches of OPM background investigation data through contractors—first the now-defunct USIS in August of last year, and then KeyPoint Government Solutions less than four months later. Those breaches included data about both government employees and contractors working for the government.
But some of the security issues at OPM fall on Congress' shoulders—the breaches of contractors in particular. Until recently, federal agents carried out background investigations for OPM. Then Congress cut the budget for investigations, and they were outsourced to USIS, which, as one person familiar with OPM's investigation process told Ars, was essentially a company made up of "some OPM people who quit the agency and started up USIS on a shoestring." When USIS was breached and most of its data (if not all of it) was stolen, the company lost its government contracts and was replaced by KeyPoint—"a bunch of people on an even thinner shoestring. Now if you get investigated, it's by a person with a personal Gmail account because the company that does the investigation literally has no IT infrastructure. And this Gmail account is not one of those where a company contracts with Google for business services. It is a personal Gmail account."
Some of the contractors that have helped OPM with managing internal data have had security issues of their own—including potentially giving foreign governments direct access to data long before the recent reported breaches. A consultant who did some work with a company contracted by OPM to manage personnel records for a number of agencies told Ars that he found the Unix systems administrator for the project "was in Argentina and his co-worker was physically located in the [People's Republic of China]. Both had direct access to every row of data in every database: they were root. Another team that worked with these databases had at its head two team members with PRC passports. I know that because I challenged them personally and revoked their privileges. From my perspective, OPM compromised this information more than three years ago and my take on the current breach is 'so what's new?'"
Given the scope and duration of the data breaches, it may be impossible for the US government to get a handle on the exact extent of the damage done just by the latest attack on OPM's systems. If anything is clear, it is that the aging infrastructure of many civilian agencies in Washington magnify the problems the government faces in securing its networks, and OPM's data breach may just be the biggest one that the government knows about to date.
Expand full story
Sean Gallagher  / Sean is Ars Technica's IT Editor.  A former Navy officer, systems administrator, and network systems integrator with 20 years of IT journalism experience, he lives and works in Baltimore, Maryland.
</article>
</item>
<item>
<title>Retailers want to be able to scan your face without your permission</title>
<article>Further ReadingChicago man sues Facebook over facial recognition use in “Tag Suggestions”Plaintiff claims Facebook violates Illinois Biometric Information Privacy Act.After more than a year of discussions, all nine privacy advocates have stormed out of a government-organized “multi-stakeholder process” to sort out details around the best practices for facial recognition technology.
The sticking point was that corporations apparently refused to concede that there was any scenario during which a person’s consent to scan their face was needed.
“When we came in [last] Thursday, [we proposed] that in general, there will be exceptions, but the default for identifying unknown people is that you get permission before you identify them using facial recognition,” Alvaro Bedoya, one of the nine participating advocates and a law professor at Georgetown University, told Ars. “Not a single trade association or company would agree with that premise. That’s remarkable. Google is opt-in on facial recognition, Microsoft is opt-in on facial recognition, Facebook isn’t, but they’ve gotten sued and also had to turn it off in Europe. So not only does it go against state law, it goes against industry practice. Consumers deserve more.”
Bedoya is specifically referring to the Illinois Biometric Information Privacy Act (BIPA). Facebook was sued in April 2015 by a Chicago man, alleging that he and others in Illinois had their rights violated by Facebook as they did not give their express permission for facial recognition. Texas also has a similar law on the books.
As the privacy group wrote in a statement released late Monday evening:
At a base minimum, people should be able to walk down a public street without fear that companies they’ve never heard of are tracking their every movement—and identifying them by name—using facial recognition technology. Unfortunately, we have been unable to obtain agreement even with that basic, specific premise. The position that companies never need to ask permission to use biometric identification is at odds with consumer expectations, current industry practices, as well as existing state law.
Further ReadingGermany: Facebook must destroy facial recognition databaseSays opt-out mechanism violates European Union law.The process, which was organized by the National Telecommunications and Information Administration (NTIA), specifically aims “to develop a voluntary, enforceable code of conduct that specifies how the Consumer Privacy Bill of Rights applies to facial recognition technology in the commercial context.”
The talks began in February 2014 in Washington, DC and include representatives from tech industry and retail lobby groups, as well as Google, Facebook, and Microsoft. Ars contacted Facebook and a number of the trade groups, most of which did not immediately respond.
The Computer &amp; Communications Industry Association’s (CCIA) policy counsel, Bijan Madhani, told Ars that the group still believes discussions will “ultimately be beneficial to consumers.”
"CCIA is committed to continued participation in the NTIA multi-stakeholder process on facial recognition technologies,” he said in a statement. “Such tools are being employed in a wide range of environments for a variety of uses. Depending on the context and purpose, different implementations of facial recognition technologies should offer differing levels of control to individuals. There are circumstances where permission prior to participation in a facial recognition system makes a great deal of sense, and others where it does not."
When Ars asked Madhani to better articulate why, generally speaking, it does not make sense to seek consumer permission before conducting a facial scan, he outlined a scenario whereby a store might want to use facial recognition against a suspected shoplifter.
“Certain companies feel like their facial recognition provides more benefits to consumers if it provides a benign extension of their existing services,” he said.
Meanwhile, in a statement e-mailed to Ars, the NTIA said that the series of meetings, which remain open to the public, will continue.
“NTIA is disappointed that some stakeholders have chosen to stop participating in our multi-stakeholder engagement process regarding privacy and commercial facial recognition technology,” Juliana Gruenwald, an NTIA spokeswoman, wrote. “Up to this point, the process has made good progress as many stakeholders, including privacy advocates, have made substantial, constructive contributions to the group’s work. A substantial number of stakeholders want to continue the process and are establishing a working group that will tackle some of the thorniest privacy topics concerning facial recognition technology. The process is the strongest when all interested parties participate and are willing to engage on all issues. NTIA will continue to facilitate meetings on this topic for those stakeholders who want to participate.”
Expand full story
Cyrus Farivar  / Cyrus is the Senior Business Editor at Ars Technica, and is also a radio producer and author. His first book, The Internet of Elsewhere, was published in April 2011.
</article>
</item>
<item>
<title>Battling the AI in Rainbow Six Siege’s new wave-based Terrorhunt mode</title>
<article>OK, so maybe Rainbow Six Siege is a tad reminiscent of a certain Valve classic (read: Counter Strike: Global Offensive), but is that such a bad thing? After all, if you're going to use another video game as your inspiration, it might as well be one that's damn good. And it seems to have paid off: Rainbow Six Siege is pretty damn good too. Like Counter Strike, it's all about short, snappy rounds played in small teams, with a definitive objective and the threat of perma-death hanging over each and every shot you fire. This is a tense game, backed up by a clever scenery destruction system, a sharp array of weapons, and an even sharper visual style.
The big news from E3 is the announcement of a new mode named Terrorhunt, one of few that makes use of AI, rather than human meat-puppets, as opponents. The premise is simple: a team of five human players takes on a squadron of AI opponents. The aim (in my demo at least) is to defuse bombs inside a building while the AI does its best to make sure they go off. It's classic Rainbow Six wall rappelling, sneaking, and breaching, mixed with the wave-based AI survival modes made popular by the likes of Gears and War and Left 4 Dead, and it works very well indeed.
Naturally, teamwork is very important. With just five players squaring off against a much larger group of enemies, storming in and playing the hero doesn't get you particularly far. Case in point, during one of the rounds I played, a fellow teammate (and I use the word loosely here) stormed off in front of me and began climbing up the wall without letting the rest of the team scout it out first. Unfortunately for him, the wall was equipped with booby traps that he very quickly set off. Suffice to say, he wasn't able to play along for the rest of the round.
Rainbow Six Siege helps push teamwork by giving each member of the team a specific role to fill. Certain characters are equipped with a sledgehammer for busting in doors, while others can use shock drones (little remote control cars) that can be used for a spot of sneaky reconnaissance. Then there's the Thermite character who's equipped with an exothermic charge for breaching (yes, there's a lot of breaching in this game) through reinforced walls, and the Montagne character, who's equipped with a riot shield for fending off bullet fire.
If you get a team that knows and sticks to their roles, as I later did, Rainbow Six Siege comes alive. It plays to that classic SWAT team and Special Forces fantasy where everyone speaks in code words, moves in a constant crouch, and silently communicates by waving their fingers in the air and making fists in some sort of bizarre sign language. With my cooperative team in tow, scaling the consulate building where the bombs were located and hanging upside down to silently shoot tangos (see, you can't help it) by the windows meant we could get inside quickly to find the first bomb.
Once we did, it was a simple matter of putting down the bomb defuser—attached to a tense placement timer naturally—and then defending the site while the defuser went to work. The AI can make use of the exact same tactics as you can, so even if you back the team against a wall for cover, the AI can opt to breach those walls, and use the new opening to surround you. There are varying levels of AI difficulty to choose from, which changes the weapons they're hooked up with and how willing they are to breach walls.
Ubisoft means it when they say things get very tricky on the harder difficulty levels: in one particular round we survived for all of two minutes as the AI lay waste to the team with a barrage of bullets.
New footage of Rainbow Six: Siege and new IP For Honor.
So Rainbow Six Siege is lots of fun, even if I got my butt handed to me for a few rounds. And it's tremendous that each and every round can be very different. New to the game is what Ubisoft is calling a "Siege Generator," where the game randomly changes around each scenario. Not only does it change the building and setting, but it also changes the time of day (which naturally affects stealth), as well as the location of the bombs and enemies. Ubisoft says the AI is clever enough to adapt to its new surroundings and come up with intelligent ways to launch an attack. We don't know how well that will work in practice, particularly as other attempts at highly adaptive AI (Alien: Isolation, for example) haven't delivered.
Ubisoft, at least according to our early impressions, is onto a good thing with Rainbow Six Siege. Sure, there are a lot of nods to the speed and tactility of Counter Strike, but they're mixed with some classic Rainbow Six touches like rappelling down buildings and using neat gadgetry to outsmart the opposition. It's a great looking game too, a mix of realistic colours and bold cartoonish lines that make it stand out from your typical brown and grey shooter. Will it usurp the likes of Counter Strike in the e-sports world—a goal Ubisoft is certainly shooting for? I don't know, but I'm optimistic. It's been seven years since we last had a great Rainbow Six fix, time enough for Ubisoft to make this one very good indeed.
Rainbow Six Siege will be released on October 13 this year for PS4, Xbox One, and PC. There will be a beta available from September 24.
This post originated on Ars Technica UK
Expand full story
Mark Walton  / Mark is Gaming and Hardware Editor at Ars Technica UK by day, and keen musician by night. He hails from the UK, the home of ARM, heavy metal, and superior chocolate.
</article>
</item>
<item>
<title>Star Fox Zero is the highlight of Nintendo’s E3 2015 lineup</title>
<article>Nintendo doesn’t do amped-up stage shows at E3 anymore—its low-key “Digital Events” are smaller, quieter affairs. It's fitting then that today’s announcements were all mostly low-key.
The headliner was a Star Fox Zero game, a new Wii U title (not a remake) that strongly evokes Star Fox 64 down to the insistence that Fox perform barrel rolls. The in-game footage showed off several vehicles, including the classic Arwing, the Landmaster tank, a sort of ostrich-looking walker robot, and the “Gyro-wing,” a helicopter. You can attempt the same courses with several different vehicles to mix up the gameplay.
The footage shown was visually impressive and appropriately cinematic—seeing old Nintendo franchises in HD for the first time is still fun, even if the Wii U has less horsepower than other consoles. Star Fox Zero is due out this holiday season.
Nintendo didn’t mention the delayed open-world Wii U Zelda title that it announced at last year’s E3, but fans of the franchise still have a couple of things to look forward to. There was a new three-player Zelda title for the 3DS, dubbed Triforce Heroes and scheduled for release in 2015. This is a sort of spiritual follow up to the Four Swords franchise that uses what looks like the Link Between Worlds engine. While Four Swords gameplay was more competitive in nature, Nintendo says the focus this time around is on cooperative fighting and puzzle-solving.
Up to three players on three 3DSes can work together to fight enemies and solve puzzles. Single-player gameplay will be possible as well—players can switch between Link and a couple of “doll” characters to play the game.
A new 3DS version of Hyrule Warriors was also announced, which includes new characters and levels based on Wind Waker. Hyrule Warriors Legends is due out in the first quarter of 2016.
Two new titles from the Animal Crossing series were announced, too, though neither of them is a full Wii U version of the game. Animal Crossing Happy Home Designer is a game that appears to focus on the home-and-yard customization aspects of the series. The trailer shows a player scanning characters into the game with NFC cards, part of the Amiibo line. It will be released on September 25.
A second game, Animal Crossing Amiibo Festival, is a quiet-looking board game for the Wii U that looks like it will ruin fewer friendships than Mario Party. Animal Crossing Amiibo can be scanned into the game and used as game pieces—this game is due out this holiday season.
For fans of the Mario Tennis series, there's a Wii U entry in the franchise that’s also due out late this year. The Mario Tennis: Ultra Smash footage that was shown looks mostly familiar, except for the new mega mushroom mechanic that makes players gigantic.
Next, Super Mario RPG’s two spiritual sequel franchises are being brought together in a single game. The Mario &amp; Luigi franchise and the Paper Mario franchise are being mashed together into Mario &amp; Luigi: Paper Jam, and it will be out for the 3DS in spring of 2016. Gameplay-wise, it looks more like Mario &amp; Luigi with visual elements from Paper Mario rather than the other way around.
These were probably the biggest of the new (or new-ish) announcements, but Nintendo fired off a few other titles and release dates without showing us much of the games themselves. Xenoblade Chronicles X, already out in Japan, will be released for the Wii U in the US on December 4 this year. Metroid Prime: Federation Force, a game that promises “4-player co-op missions in the Metroid Prime universe,” will be out for the 3DS in 2016. And Fire Emblem Fates, another 3DS entry in the long-running tactical RPG series, is also due out in 2016.
Other announcements were rehashes of sorts from last year’s E3. The custom Mario level-generator Super Mario Maker was shown off again, albeit with considerably more kinds of enemies and obstacles and new Super Mario 3 and Super Mario World graphical themes (last year the only themes were classic Super Mario Bros. and New Super Mario Bros.). Of course, a new Nintendo game in 2015 means a new Amiibo, and Mario Maker will be released alongside an 8-bit Mario figure. Super Mario Maker is due out on September 11, 2015, and it will be playable at Best Buy stores in the US and Canada this week.
Ars Culture Editor Sam Machkovech has more on Super Mario Maker from the Nintendo World Championships competition earlier in the show:
“Super Mario Maker was previewed at Sunday's Nintendo World Championship in Los Angeles; it was the event's final game, and while it wasn't quite the major, new-game conclusion that would have echoed the 1989 film The Wizard, its drama and fun came close.
Super Mario Maker had been revealed at prior conferences—or, at least, its 3DS version—but we’d never seen a lengthy, intense demonstration of how crazy its level-creation tools really are. The Championship’s final two contestants had to face off in four levels created by staffers from Nintendo’s Treehouse testing group, and the results were on par with some of the insane custom Mario levels we’d seen made by hackers in the past—only juiced up with serious Nintendo magic.
Each one was based on a different 2D Mario universe, from the 1985 original to 2013’s Super Mario Bros. Wii U, and each original sprite and element was increased, tweaked, and expanded to create all matter of semi-familiar Mario chaos. Giant bridges made of fireballs, crazy run-and-jump speed ramps that had to be jumped while dodging super-sized piranha plants; a floating airship level with hundreds of cannonballs that had to be dodged and jumped upon to survive—it tested the competition's two final players to an extreme degree, and it got us more excited, frankly, than the Nintendo Direct presentation we saw on Tuesday morning.”
Yoshi’s Wooly World, due out on June 25, also made an appearance (along with its adorable yarn Yoshi Amiibo). This game combines elements from the Yoshi’s Island franchise with the visual aesthetic of Kirby’s Epic Yarn, and it was first shown off at E3 last year. Using Amiibo of different Nintendo characters with the game will make Yoshi take on visual characteristics of those characters.
And finally, third-party franchises got a tiny bit of love. An Atlus game that combines the Shin Megami Tensei franchise with Fire Emblem characters, Genei Ibun Roku #FE, is coming in 2016. A 3DS game from Level 5 called Yokai Watch, which is already a huge hit in Japan, combines exploration and monster collecting/battling with touchscreen-tapping attacks. It will be out this holiday season. And new Skylanders Superchargers characters and vehicles for Donkey Kong and Bowser were revealed, all exclusive to Nintendo consoles. These figures can work on the Skylanders portal or as Amiibo if you twist their bases.
Star Fox Zero appears to be the company’s flagship title for this holiday season—the rest of the games shown off mostly look to be smaller entries in established franchises, or they aren't coming until 2016. There’s nothing on the level of a Smash Bros. or Mario Kart on this list, but hopefully the games announced can help continue the momentum those titles helped Nintendo build last year.
Expand full story
Andrew Cunningham  / Andrew has a B.A. in Classics from Kenyon College and has over five years of experience in IT. His work has appeared on Charge Shot!!! and AnandTech, and he records a weekly book podcast called Overdue. 
</article>
</item>
<item>
<title>FDA follows through on its plan to ban trans fats</title>
<article>The FDA announced today that it has removed partially hydrogenated vegetable oils, the primary source of trans fats in human diets, from the list of items that fall into the category of "generally recognized as safe." The move, formally proposed last November, will mean that food makers must remove these fats from any products before three years is up.
Strong link to heart disease may mean the end of the fat's use as an additive.
Dietary intake of trans fats is strongly linked to coronary heart disease, and studies have linked these fats to tens of thousands of deaths each year in the US. Although the precise mechanism by which they influence the human body isn't clear, it is clear how they get into the human body in the first place. We put them there.
Fats are primarily composed of a long chain of linked carbon atoms. Many of the fats found in naturally produced oils (like vegetable oil) have a mixture of both double and single bonds between the carbons in this chain. These produce a kink in the molecule, which can take one of two forms: the cis form, where the two sides of the chain end up on the same side of the double bond, or the trans form, where they're on opposite sides. The vast majority of fats produced by living organisms are in the cis form.
Our industrial processes, however, are generally nowhere near as sensitive to these fine-scale structural details. So when we process various oils to change their chemical properties (turning a liquid into a solid at room temperature, for example), we often end up with a mixture of cis and trans fats. Their widespread use has led to their health effects becoming obvious.
The FDA announced its plan to remove them from the list of additives generally recognized as safe back in November; this allowed time for public comment. Any comments, however, did not change the original plan. As a result, food makers have three years to move on from including trans fats from their products.
Expand full story
John Timmer  / John became Ars Technica's science editor in 2007 after spending 15 years doing biology research at places like Berkeley and Cornell. 
</article>
</item>
